{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-evdFIC6J7wv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldjJu0OqMfv4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "9bwIsrr1NOmB",
        "outputId": "50b06f04-f4cf-4141-af47-0c5b6d31982f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title beds bath        area  \\\n",
              "0  Eminent Apartment Of 2200 Sq Ft Is Vacant For ...   3    4   2,200 sqft   \n",
              "1  Apartment Ready To Rent In South Khulshi, Near...   3    4   1,400 sqft   \n",
              "2  Smartly priced 1950 SQ FT apartment, that you ...   3    4   1,950 sqft   \n",
              "3  2000 Sq Ft Residential Apartment Is Up For Ren...   3    3   2,000 sqft   \n",
              "4  Strongly Structured This 1650 Sq. ft Apartment...   3    4   1,650 sqft   \n",
              "\n",
              "                               adress       type   purpose  \\\n",
              "0     Block A, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "1  South Khulshi, Khulshi, Chattogram  Apartment  For Rent   \n",
              "2     Block F, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "3             Sector 9, Uttara, Dhaka  Apartment  For Rent   \n",
              "4     Block I, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "\n",
              "                                            flooPlan  \\\n",
              "0  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "1  https://images-cdn.bproperty.com/thumbnails/44...   \n",
              "2  https://images-cdn.bproperty.com/thumbnails/11...   \n",
              "3  https://images-cdn.bproperty.com/thumbnails/14...   \n",
              "4  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "\n",
              "                                                 url        lastUpdated  \\\n",
              "0  https://www.bproperty.com/en/property/details-...    August 13, 2022   \n",
              "1  https://www.bproperty.com/en/property/details-...   January 25, 2022   \n",
              "2  https://www.bproperty.com/en/property/details-...  February 22, 2023   \n",
              "3  https://www.bproperty.com/en/property/details-...   October 28, 2021   \n",
              "4  https://www.bproperty.com/en/property/details-...  February 19, 2023   \n",
              "\n",
              "         price  \n",
              "0  50 Thousand  \n",
              "1  30 Thousand  \n",
              "2  30 Thousand  \n",
              "3  35 Thousand  \n",
              "4  25 Thousand  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffea59d7-4b5d-4462-a098-9a629da956b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>type</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eminent Apartment Of 2200 Sq Ft Is Vacant For ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>Block A, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>August 13, 2022</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apartment Ready To Rent In South Khulshi, Near...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>South Khulshi, Khulshi, Chattogram</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/44...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>January 25, 2022</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Smartly priced 1950 SQ FT apartment, that you ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>Block F, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/11...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 22, 2023</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000 Sq Ft Residential Apartment Is Up For Ren...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>Sector 9, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>October 28, 2021</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Strongly Structured This 1650 Sq. ft Apartment...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>Block I, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 19, 2023</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffea59d7-4b5d-4462-a098-9a629da956b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ffea59d7-4b5d-4462-a098-9a629da956b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ffea59d7-4b5d-4462-a098-9a629da956b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec2ec93e-9491-45c2-a6e1-afac94cf8c3f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec2ec93e-9491-45c2-a6e1-afac94cf8c3f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec2ec93e-9491-45c2-a6e1-afac94cf8c3f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('/content/property_listing_data_in_Bangladesh.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFc2d73eYbRq"
      },
      "outputs": [],
      "source": [
        "df = df.drop('title', axis = 1)\n",
        "df = df.drop('adress', axis = 1)\n",
        "df = df.drop('type', axis = 1)\n",
        "df = df.drop('purpose', axis = 1)\n",
        "df = df.drop('flooPlan', axis = 1)\n",
        "df = df.drop('url', axis = 1)\n",
        "df = df.drop('lastUpdated', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7EhbqrLaYg26",
        "outputId": "d2c48745-a704-485f-c14a-52cb7f77e5ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     beds bath        area        price\n",
              "0      3    4   2,200 sqft  50 Thousand\n",
              "1      3    4   1,400 sqft  30 Thousand\n",
              "2      3    4   1,950 sqft  30 Thousand\n",
              "3      3    3   2,000 sqft  35 Thousand\n",
              "4      3    4   1,650 sqft  25 Thousand\n",
              "...   ...  ...         ...          ...\n",
              "7552   4    4   3,600 sqft  80 Thousand\n",
              "7553   3    2     900 sqft  19 Thousand\n",
              "7554   2    2   1,000 sqft  22 Thousand\n",
              "7555   3    4   3,600 sqft    1.75 Lakh\n",
              "7556   4    4   2,600 sqft  90 Thousand\n",
              "\n",
              "[7557 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a1558d9-bd98-485c-86be-3d1e6272f4ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7552</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3,600 sqft</td>\n",
              "      <td>80 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7553</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>900 sqft</td>\n",
              "      <td>19 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7554</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1,000 sqft</td>\n",
              "      <td>22 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7555</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3,600 sqft</td>\n",
              "      <td>1.75 Lakh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7556</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2,600 sqft</td>\n",
              "      <td>90 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7557 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a1558d9-bd98-485c-86be-3d1e6272f4ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a1558d9-bd98-485c-86be-3d1e6272f4ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a1558d9-bd98-485c-86be-3d1e6272f4ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c313f190-60d1-42b5-9ae7-fae5ba5cb415\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c313f190-60d1-42b5-9ae7-fae5ba5cb415')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c313f190-60d1-42b5-9ae7-fae5ba5cb415 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epCQyQiwZPQZ"
      },
      "outputs": [],
      "source": [
        "df['bath'] = df['bath'].str.extract('(\\d+)')\n",
        "df['bath'] = pd.to_numeric(df['bath'])\n",
        "df['beds'] = df['beds'].str.extract('(\\d+)')\n",
        "df['beds'] = pd.to_numeric(df['beds'])\n",
        "df['area'] = df['area'].str.replace(',', '')\n",
        "df['area'] = df['area'].str.extract('(\\d+)')\n",
        "df['area'] = pd.to_numeric(df['area'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "df['price'] = df['price'].apply(lambda x: float(re.findall(r'\\d+\\.\\d+|\\d+', x)[0]) * 100000.0 if 'Lakh' in x else float(re.findall(r'\\d+\\.\\d+|\\d+', x)[0]) * 1000)\n"
      ],
      "metadata": {
        "id": "74mKZxikxpKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "C-UgfWG9xu8j",
        "outputId": "5b223bdc-3835-4cf9-bcbd-e6a2aa2f3e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   beds  bath  area     price\n",
              "0     3     4  2200   50000.0\n",
              "1     3     4  1400   30000.0\n",
              "2     3     4  1950   30000.0\n",
              "3     3     3  2000   35000.0\n",
              "4     3     4  1650   25000.0\n",
              "5     5     5  3400  110000.0\n",
              "6     3     3  1600   35000.0\n",
              "7     3     3  1250   23000.0\n",
              "8     3     4  2150   40000.0\n",
              "9     3     3  1250   23000.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-624a0288-0942-4d21-82dc-a37fd9cb5f74\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2200</td>\n",
              "      <td>50000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1400</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1950</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2000</td>\n",
              "      <td>35000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1650</td>\n",
              "      <td>25000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3400</td>\n",
              "      <td>110000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1600</td>\n",
              "      <td>35000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1250</td>\n",
              "      <td>23000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2150</td>\n",
              "      <td>40000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1250</td>\n",
              "      <td>23000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-624a0288-0942-4d21-82dc-a37fd9cb5f74')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-624a0288-0942-4d21-82dc-a37fd9cb5f74 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-624a0288-0942-4d21-82dc-a37fd9cb5f74');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b071dfd-ab7c-44b8-b805-11c0accf6b7a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b071dfd-ab7c-44b8-b805-11c0accf6b7a')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b071dfd-ab7c-44b8-b805-11c0accf6b7a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data=df.to_numpy()"
      ],
      "metadata": {
        "id": "BDKowR9yxzc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data[0][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqJhlp0Ex4jq",
        "outputId": "12ee9c10-ea26-426b-b894-bc2da9ccafe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2200.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Vo1zyOx7Hx",
        "outputId": "87422a03-53e1-41df-d66d-0355c9e99a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.00e+00, 4.00e+00, 2.20e+03, 5.00e+04],\n",
              "       [3.00e+00, 4.00e+00, 1.40e+03, 3.00e+04],\n",
              "       [3.00e+00, 4.00e+00, 1.95e+03, 3.00e+04],\n",
              "       ...,\n",
              "       [2.00e+00, 2.00e+00, 1.00e+03, 2.20e+04],\n",
              "       [3.00e+00, 4.00e+00, 3.60e+03, 1.75e+05],\n",
              "       [4.00e+00, 4.00e+00, 2.60e+03, 9.00e+04]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def z_score_scaling(dataset):\n",
        "    mean_vals = np.mean(dataset, axis=0)\n",
        "    stdev_vals = np.std(dataset, axis=0)\n",
        "    scaled_dataset = (dataset - mean_vals) / stdev_vals\n",
        "    return scaled_dataset\n",
        "modified_list=new_data\n",
        "X=modified_list[:,:-1]\n",
        "Y=modified_list[:,-1]\n",
        "X=z_score_scaling(X)\n",
        "X=np.array(X)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y, test_size=0.4, random_state=1)\n",
        "X_valid,X_test,y_valid,y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=1)\n",
        "print(\"Train data:\", len(X_train))\n",
        "print(\"Test data:\", len(X_valid))\n",
        "print(\"Validation data:\", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9jCdCFYx90C",
        "outputId": "fe914fd7-d4b5-46e4-a2e7-b28d0c8734b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 4534\n",
            "Test data: 1511\n",
            "Validation data: 1512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP-Y7pDNyEfx",
        "outputId": "aec2dbe6-0332-4eba-e4da-496ae37476e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.25164733,  0.34002476, -0.09619075],\n",
              "       [ 1.27626956,  0.34002476,  0.65640181],\n",
              "       [ 0.25164733, -0.76993944, -0.17981214],\n",
              "       ...,\n",
              "       [ 0.25164733,  1.44998896,  0.65640181],\n",
              "       [ 1.27626956,  1.44998896,  2.22430297],\n",
              "       [ 0.25164733,  0.34002476, -0.28433889]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_b = 10\n",
        "initial_w = np.array([-600., -9000.0, 5000.0])\n",
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b\n",
        "        cost = cost + (f_wb_i - y[i])**2\n",
        "    cost = cost / (2 * m)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "5KX4DuqWyHmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "    return dj_db, dj_dw\n",
        "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)"
      ],
      "metadata": {
        "id": "40XPotIhyK1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def gradient_descent(X, y, w_in, b_in, cost, gradient, alpha, num_iters):\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "    for i in range(num_iters):\n",
        "        dj_db,dj_dw = gradient(X, y, w, b)\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "        print(f\"Training Loss: {compute_cost(X_train,y_train,w,b)}\")\n",
        "        print(f\"Validation Loss: {compute_cost(X_valid,y_valid,w,b)}\")\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "c_LHOUtdyOWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_w = np.zeros_like(initial_w)\n",
        "print(init_w)\n",
        "init_b = 1\n",
        "iterations = 2000\n",
        "alpha = 0.01\n",
        "w_final, b_final = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
        "                                                    compute_cost, compute_gradient,\n",
        "                                                    alpha, iterations)\n",
        "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
        "m,_ = X_train.shape\n",
        "for i in range(20):\n",
        "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-RvvWBlyR6q",
        "outputId": "973b4f2e-14f6-4cdf-e4be-dda3284e9ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0.]\n",
            "Training Loss: 2813148056.5446415\n",
            "Validation Loss: 2087486073.0333261\n",
            "Training Loss: 2729427839.928401\n",
            "Validation Loss: 2030038340.1377027\n",
            "Training Loss: 2649670771.7505507\n",
            "Validation Loss: 1975291218.8384461\n",
            "Training Loss: 2573675640.33165\n",
            "Validation Loss: 1923107705.5126026\n",
            "Training Loss: 2501251691.5961933\n",
            "Validation Loss: 1873357929.4069047\n",
            "Training Loss: 2432218080.809965\n",
            "Validation Loss: 1825918778.0082371\n",
            "Training Loss: 2366403353.1560616\n",
            "Validation Loss: 1780673542.1464825\n",
            "Training Loss: 2303644951.6307116\n",
            "Validation Loss: 1737511579.789597\n",
            "Training Loss: 2243788750.8200307\n",
            "Validation Loss: 1696327997.5454257\n",
            "Training Loss: 2186688615.1949773\n",
            "Validation Loss: 1657023348.9370341\n",
            "Training Loss: 2132205980.6329844\n",
            "Validation Loss: 1619503348.5671492\n",
            "Training Loss: 2080209457.9436545\n",
            "Validation Loss: 1583678601.3343728\n",
            "Training Loss: 2030574457.2392905\n",
            "Validation Loss: 1549464345.9075441\n",
            "Training Loss: 1983182832.0531092\n",
            "Validation Loss: 1516780211.70678\n",
            "Training Loss: 1937922542.1649728\n",
            "Validation Loss: 1485549988.6792657\n",
            "Training Loss: 1894687334.1498504\n",
            "Validation Loss: 1455701409.1952546\n",
            "Training Loss: 1853376438.7158818\n",
            "Validation Loss: 1427165941.425599\n",
            "Training Loss: 1813894283.9478536\n",
            "Validation Loss: 1399878593.5954237\n",
            "Training Loss: 1776150223.6190455\n",
            "Validation Loss: 1373777728.5409195\n",
            "Training Loss: 1740058279.7779093\n",
            "Validation Loss: 1348804888.025949\n",
            "Training Loss: 1705536898.8581367\n",
            "Validation Loss: 1324904626.3043127\n",
            "Training Loss: 1672508720.6004086\n",
            "Validation Loss: 1302024352.440201\n",
            "Training Loss: 1640900359.1112444\n",
            "Validation Loss: 1280114180.9253087\n",
            "Training Loss: 1610642195.4200933\n",
            "Validation Loss: 1259126790.1553824\n",
            "Training Loss: 1581668180.9296312\n",
            "Validation Loss: 1239017288.351933\n",
            "Training Loss: 1553915651.1858003\n",
            "Validation Loss: 1219743086.536794\n",
            "Training Loss: 1527325149.4244323\n",
            "Validation Loss: 1201263778.187798\n",
            "Training Loss: 1501840259.3800974\n",
            "Validation Loss: 1183541025.223607\n",
            "Training Loss: 1477407446.8696256\n",
            "Validation Loss: 1166538449.984006\n",
            "Training Loss: 1453975909.688671\n",
            "Validation Loss: 1150221532.8898706\n",
            "Training Loss: 1431497435.3838427\n",
            "Validation Loss: 1134557515.4834368\n",
            "Training Loss: 1409926266.4858766\n",
            "Validation Loss: 1119515308.5654721\n",
            "Training Loss: 1389218972.8118157\n",
            "Validation Loss: 1105065405.1606655\n",
            "Training Loss: 1369334330.4635518\n",
            "Validation Loss: 1091179798.0570085\n",
            "Training Loss: 1350233207.1712093\n",
            "Validation Loss: 1077831901.678007\n",
            "Training Loss: 1331878453.6469915\n",
            "Validation Loss: 1064996478.0596063\n",
            "Training Loss: 1314234800.633867\n",
            "Validation Loss: 1052649566.7154728\n",
            "Training Loss: 1297268761.3491952\n",
            "Validation Loss: 1040768418.1859155\n",
            "Training Loss: 1280948539.0397682\n",
            "Validation Loss: 1029331431.0762523\n",
            "Training Loss: 1265243939.3793848\n",
            "Validation Loss: 1018318092.4009454\n",
            "Training Loss: 1250126287.4544306\n",
            "Validation Loss: 1007708921.0593268\n",
            "Training Loss: 1235568349.0960264\n",
            "Validation Loss: 997485414.2779825\n",
            "Training Loss: 1221544256.3304918\n",
            "Validation Loss: 987629996.8635628\n",
            "Training Loss: 1208029436.7313778\n",
            "Validation Loss: 978125973.1179657\n",
            "Training Loss: 1195000546.4681776\n",
            "Validation Loss: 968957481.2757806\n",
            "Training Loss: 1182435406.8572824\n",
            "Validation Loss: 960109450.331082\n",
            "Training Loss: 1170312944.2311532\n",
            "Validation Loss: 951567559.1278143\n",
            "Training Loss: 1158613132.9514112\n",
            "Validation Loss: 943318197.5945941\n",
            "Training Loss: 1147316941.4005244\n",
            "Validation Loss: 935348430.0109967\n",
            "Training Loss: 1136406280.7956944\n",
            "Validation Loss: 927645960.1984164\n",
            "Training Loss: 1125863956.6766446\n",
            "Validation Loss: 920199098.5341773\n",
            "Training Loss: 1115673622.9268374\n",
            "Validation Loss: 912996730.6929275\n",
            "Training Loss: 1105819738.1951587\n",
            "Validation Loss: 906028288.0244054\n",
            "Training Loss: 1096287524.5918067\n",
            "Validation Loss: 899283719.4814866\n",
            "Training Loss: 1087062928.53922\n",
            "Validation Loss: 892753465.0168979\n",
            "Training Loss: 1078132583.6646626\n",
            "Validation Loss: 886428430.3713177\n",
            "Training Loss: 1069483775.6274978\n",
            "Validation Loss: 880299963.1797363\n",
            "Training Loss: 1061104408.779491\n",
            "Validation Loss: 874359830.3266075\n",
            "Training Loss: 1052982974.561875\n",
            "Validation Loss: 868600196.4842044\n",
            "Training Loss: 1045108521.5482389\n",
            "Validation Loss: 863013603.7719036\n",
            "Training Loss: 1037470627.0467247\n",
            "Validation Loss: 857592952.4774327\n",
            "Training Loss: 1030059370.1798129\n",
            "Validation Loss: 852331482.7843232\n",
            "Training Loss: 1022865306.3641568\n",
            "Validation Loss: 847222757.4525071\n",
            "Training Loss: 1015879443.1171418\n",
            "Validation Loss: 842260645.4021541\n",
            "Training Loss: 1009093217.1204672\n",
            "Validation Loss: 837439306.1531144\n",
            "Training Loss: 1002498472.4749678\n",
            "Validation Loss: 832753175.0750338\n",
            "Training Loss: 996087440.0842062\n",
            "Validation Loss: 828196949.4056286\n",
            "Training Loss: 989852718.1077079\n",
            "Validation Loss: 823765574.9966197\n",
            "Training Loss: 983787253.4277079\n",
            "Validation Loss: 819454233.7492421\n",
            "Training Loss: 977884324.0765195\n",
            "Validation Loss: 815258331.7030041\n",
            "Training Loss: 972137522.5739573\n",
            "Validation Loss: 811173487.743436\n",
            "Training Loss: 966540740.1274002\n",
            "Validation Loss: 807195522.8963469\n",
            "Training Loss: 961088151.6492054\n",
            "Validation Loss: 803320450.1777725\n",
            "Training Loss: 955774201.5487645\n",
            "Validation Loss: 799544464.9704223\n",
            "Training Loss: 950593590.2586083\n",
            "Validation Loss: 795863935.8990912\n",
            "Training Loss: 945541261.4562545\n",
            "Validation Loss: 792275396.17872\n",
            "Training Loss: 940612389.9452943\n",
            "Validation Loss: 788775535.410483\n",
            "Training Loss: 935802370.1614406\n",
            "Validation Loss: 785361191.8022361\n",
            "Training Loss: 931106805.2706016\n",
            "Validation Loss: 782029344.791199\n",
            "Training Loss: 926521496.8283781\n",
            "Validation Loss: 778777108.0477294\n",
            "Training Loss: 922042434.9713428\n",
            "Validation Loss: 775601722.8402449\n",
            "Training Loss: 917665789.1125723\n",
            "Validation Loss: 772500551.742352\n",
            "Training Loss: 913387899.1149598\n",
            "Validation Loss: 769471072.664287\n",
            "Training Loss: 909205266.9174715\n",
            "Validation Loss: 766510873.1916494\n",
            "Training Loss: 905114548.5906461\n",
            "Validation Loss: 763617645.2153702\n",
            "Training Loss: 901112546.799\n",
            "Validation Loss: 760789179.8376884\n",
            "Training Loss: 897196203.6491338\n",
            "Validation Loss: 758023362.5396491\n",
            "Training Loss: 893362593.9033968\n",
            "Validation Loss: 755318168.5964956\n",
            "Training Loss: 889608918.5400708\n",
            "Validation Loss: 752671658.7279681\n",
            "Training Loss: 885932498.6421353\n",
            "Validation Loss: 750081974.9712187\n",
            "Training Loss: 882330769.5973343\n",
            "Validation Loss: 747547336.76481\n",
            "Training Loss: 878801275.5935122\n",
            "Validation Loss: 745066037.2325991\n",
            "Training Loss: 875341664.3937846\n",
            "Validation Loss: 742636439.6572505\n",
            "Training Loss: 871949682.3769895\n",
            "Validation Loss: 740256974.1333793\n",
            "Training Loss: 868623169.8296928\n",
            "Validation Loss: 737926134.390972\n",
            "Training Loss: 865360056.4766479\n",
            "Validation Loss: 735642474.780259\n",
            "Training Loss: 862158357.237321\n",
            "Validation Loss: 733404607.4095438\n",
            "Training Loss: 859016168.196818\n",
            "Validation Loss: 731211199.4280958\n",
            "Training Loss: 855931662.7799886\n",
            "Validation Loss: 729060970.4465109\n",
            "Training Loss: 852903088.1183379\n",
            "Validation Loss: 726952690.087397\n",
            "Training Loss: 849928761.5995729\n",
            "Validation Loss: 724885175.6596453\n",
            "Training Loss: 847007067.5904901\n",
            "Validation Loss: 722857289.9497758\n",
            "Training Loss: 844136454.3241714\n",
            "Validation Loss: 720867939.1243767\n",
            "Training Loss: 841315430.9429916\n",
            "Validation Loss: 718916070.7378103\n",
            "Training Loss: 838542564.6893674\n",
            "Validation Loss: 717000671.8397169\n",
            "Training Loss: 835816478.2367244\n",
            "Validation Loss: 715120767.1772054\n",
            "Training Loss: 833135847.1532944\n",
            "Validation Loss: 713275417.4867446\n",
            "Training Loss: 830499397.4920542\n",
            "Validation Loss: 711463717.8711604\n",
            "Training Loss: 827905903.500186\n",
            "Validation Loss: 709684796.2573391\n",
            "Training Loss: 825354185.4419721\n",
            "Validation Loss: 707937811.9304261\n",
            "Training Loss: 822843107.5292579\n",
            "Validation Loss: 706221954.140598\n",
            "Training Loss: 820371575.9540179\n",
            "Validation Loss: 704536440.778666\n",
            "Training Loss: 817938537.0176452\n",
            "Validation Loss: 702880517.1169327\n",
            "Training Loss: 815542975.3522238\n",
            "Validation Loss: 701253454.6119626\n",
            "Training Loss: 813183912.2288082\n",
            "Validation Loss: 699654549.7660688\n",
            "Training Loss: 810860403.9485073\n",
            "Validation Loss: 698083123.0444996\n",
            "Training Loss: 808571540.3119023\n",
            "Validation Loss: 696538517.8454607\n",
            "Training Loss: 806316443.163001\n",
            "Validation Loss: 695020099.5202601\n",
            "Training Loss: 804094265.0038142\n",
            "Validation Loss: 693527254.4409958\n",
            "Training Loss: 801904187.6759187\n",
            "Validation Loss: 692059389.1133701\n",
            "Training Loss: 799745421.1057398\n",
            "Validation Loss: 690615929.3323144\n",
            "Training Loss: 797617202.1101358\n",
            "Validation Loss: 689196319.3782141\n",
            "Training Loss: 795518793.2593619\n",
            "Validation Loss: 687800021.2516814\n",
            "Training Loss: 793449481.7944177\n",
            "Validation Loss: 686426513.9449215\n",
            "Training Loss: 791408578.5960472\n",
            "Validation Loss: 685075292.7478056\n",
            "Training Loss: 789395417.2028141\n",
            "Validation Loss: 683745868.5868808\n",
            "Training Loss: 787409352.875665\n",
            "Validation Loss: 682437767.3956639\n",
            "Training Loss: 785449761.7067597\n",
            "Validation Loss: 681150529.5146323\n",
            "Training Loss: 783516039.7702826\n",
            "Validation Loss: 679883709.1193769\n",
            "Training Loss: 781607602.3130686\n",
            "Validation Loss: 678636873.6755422\n",
            "Training Loss: 779723882.9831202\n",
            "Validation Loss: 677409603.4191403\n",
            "Training Loss: 777864333.0941349\n",
            "Validation Loss: 676201490.8610227\n",
            "Training Loss: 776028420.9240631\n",
            "Validation Loss: 675012140.3142393\n",
            "Training Loss: 774215631.0462871\n",
            "Validation Loss: 673841167.4431667\n",
            "Training Loss: 772425463.6914523\n",
            "Validation Loss: 672688198.8333037\n",
            "Training Loss: 770657434.1387438\n",
            "Validation Loss: 671552871.5806998\n",
            "Training Loss: 768911072.1349019\n",
            "Validation Loss: 670434832.9000301\n",
            "Training Loss: 767185921.3397071\n",
            "Validation Loss: 669333739.7504097\n",
            "Training Loss: 765481538.7966418\n",
            "Validation Loss: 668249258.4780086\n",
            "Training Loss: 763797494.4274147\n",
            "Validation Loss: 667181064.4747165\n",
            "Training Loss: 762133370.5491955\n",
            "Validation Loss: 666128841.8519729\n",
            "Training Loss: 760488761.413479\n",
            "Validation Loss: 665092283.1290857\n",
            "Training Loss: 758863272.7654661\n",
            "Validation Loss: 664071088.9352738\n",
            "Training Loss: 757256521.422959\n",
            "Validation Loss: 663064967.724775\n",
            "Training Loss: 755668134.8738446\n",
            "Validation Loss: 662073635.5043935\n",
            "Training Loss: 754097750.8912529\n",
            "Validation Loss: 661096815.572847\n",
            "Training Loss: 752545017.1654959\n",
            "Validation Loss: 660134238.2713566\n",
            "Training Loss: 751009590.9519663\n",
            "Validation Loss: 659185640.7449335\n",
            "Training Loss: 749491138.734373\n",
            "Validation Loss: 658250766.7138247\n",
            "Training Loss: 747989335.9022808\n",
            "Validation Loss: 657329366.2546217\n",
            "Training Loss: 746503866.4425297\n",
            "Validation Loss: 656421195.5906533\n",
            "Training Loss: 745034422.6438099\n",
            "Validation Loss: 655526016.8910115\n",
            "Training Loss: 743580704.8136064\n",
            "Validation Loss: 654643598.0780653\n",
            "Training Loss: 742142421.0071129\n",
            "Validation Loss: 653773712.6428161\n",
            "Training Loss: 740719286.7674704\n",
            "Validation Loss: 652916139.4678657\n",
            "Training Loss: 739311024.8767459\n",
            "Validation Loss: 652070662.6575869\n",
            "Training Loss: 737917365.1172316\n",
            "Validation Loss: 651237071.3751242\n",
            "Training Loss: 736538044.0425118\n",
            "Validation Loss: 650415159.6860046\n",
            "Training Loss: 735172804.7578685\n",
            "Validation Loss: 649604726.4079002\n",
            "Training Loss: 733821396.7095456\n",
            "Validation Loss: 648805574.9663818\n",
            "Training Loss: 732483575.4825557\n",
            "Validation Loss: 648017513.2563223\n",
            "Training Loss: 731159102.6064992\n",
            "Validation Loss: 647240353.508693\n",
            "Training Loss: 729847745.3691179\n",
            "Validation Loss: 646473912.1625125\n",
            "Training Loss: 728549276.6372218\n",
            "Validation Loss: 645718009.7417194\n",
            "Training Loss: 727263474.6845987\n",
            "Validation Loss: 644972470.7366985\n",
            "Training Loss: 725990123.0266092\n",
            "Validation Loss: 644237123.4903208\n",
            "Training Loss: 724729010.2611876\n",
            "Validation Loss: 643511800.0882168\n",
            "Training Loss: 723479929.9159366\n",
            "Validation Loss: 642796336.2531394\n",
            "Training Loss: 722242680.301017\n",
            "Validation Loss: 642090571.2432066\n",
            "Training Loss: 721017064.3676363\n",
            "Validation Loss: 641394347.7538882\n",
            "Training Loss: 719802889.5717882\n",
            "Validation Loss: 640707511.8235111\n",
            "Training Loss: 718599967.7431161\n",
            "Validation Loss: 640029912.7421623\n",
            "Training Loss: 717408114.9585912\n",
            "Validation Loss: 639361402.9638575\n",
            "Training Loss: 716227151.4208119\n",
            "Validation Loss: 638701838.0217804\n",
            "Training Loss: 715056901.3407786\n",
            "Validation Loss: 638051076.4465038\n",
            "Training Loss: 713897192.8248245\n",
            "Validation Loss: 637408979.6870446\n",
            "Training Loss: 712747857.7656627\n",
            "Validation Loss: 636775412.0346304\n",
            "Training Loss: 711608731.7372838\n",
            "Validation Loss: 636150240.5490446\n",
            "Training Loss: 710479653.8935307\n",
            "Validation Loss: 635533334.9875057\n",
            "Training Loss: 709360466.870263\n",
            "Validation Loss: 634924567.7358638\n",
            "Training Loss: 708251016.6908957\n",
            "Validation Loss: 634323813.7421285\n",
            "Training Loss: 707151152.6751702\n",
            "Validation Loss: 633730950.4521506\n",
            "Training Loss: 706060727.351079\n",
            "Validation Loss: 633145857.7474189\n",
            "Training Loss: 704979596.3697444\n",
            "Validation Loss: 632568417.8848315\n",
            "Training Loss: 703907618.423167\n",
            "Validation Loss: 631998515.4384389\n",
            "Training Loss: 702844655.1646885\n",
            "Validation Loss: 631436037.2429751\n",
            "Training Loss: 701790571.1321541\n",
            "Validation Loss: 630880872.3392278\n",
            "Training Loss: 700745233.6734886\n",
            "Validation Loss: 630332911.921039\n",
            "Training Loss: 699708512.874794\n",
            "Validation Loss: 629792049.2839766\n",
            "Training Loss: 698680281.4906737\n",
            "Validation Loss: 629258179.7755859\n",
            "Training Loss: 697660414.8768812\n",
            "Validation Loss: 628731200.7471066\n",
            "Training Loss: 696648790.9249913\n",
            "Validation Loss: 628211011.5066614\n",
            "Training Loss: 695645289.9992409\n",
            "Validation Loss: 627697513.2738311\n",
            "Training Loss: 694649794.875263\n",
            "Validation Loss: 627190609.1355804\n",
            "Training Loss: 693662190.6807169\n",
            "Validation Loss: 626690204.0034325\n",
            "Training Loss: 692682364.8378298\n",
            "Validation Loss: 626196204.5719237\n",
            "Training Loss: 691710207.0075464\n",
            "Validation Loss: 625708519.2782266\n",
            "Training Loss: 690745609.0354848\n",
            "Validation Loss: 625227058.262923\n",
            "Training Loss: 689788464.8994625\n",
            "Validation Loss: 624751733.3318698\n",
            "Training Loss: 688838670.6585507\n",
            "Validation Loss: 624282457.9191725\n",
            "Training Loss: 687896124.4037256\n",
            "Validation Loss: 623819147.0511259\n",
            "Training Loss: 686960726.2098575\n",
            "Validation Loss: 623361717.3111733\n",
            "Training Loss: 686032378.089145\n",
            "Validation Loss: 622910086.8058401\n",
            "Training Loss: 685110983.94589\n",
            "Validation Loss: 622464175.1315291\n",
            "Training Loss: 684196449.5325743\n",
            "Validation Loss: 622023903.3422662\n",
            "Training Loss: 683288682.407119\n",
            "Validation Loss: 621589193.9182613\n",
            "Training Loss: 682387591.8914504\n",
            "Validation Loss: 621159970.7353046\n",
            "Training Loss: 681493089.0311142\n",
            "Validation Loss: 620736159.0349784\n",
            "Training Loss: 680605086.5560893\n",
            "Validation Loss: 620317685.3956177\n",
            "Training Loss: 679723498.842636\n",
            "Validation Loss: 619904477.7040337\n",
            "Training Loss: 678848241.8762146\n",
            "Validation Loss: 619496465.1279453\n",
            "Training Loss: 677979233.2153645\n",
            "Validation Loss: 619093578.0891316\n",
            "Training Loss: 677116391.9566103\n",
            "Validation Loss: 618695748.2372122\n",
            "Training Loss: 676259638.7002791\n",
            "Validation Loss: 618302908.4241488\n",
            "Training Loss: 675408895.5172068\n",
            "Validation Loss: 617914992.6793039\n",
            "Training Loss: 674564085.9163576\n",
            "Validation Loss: 617531936.185173\n",
            "Training Loss: 673725134.8132666\n",
            "Validation Loss: 617153675.2536774\n",
            "Training Loss: 672891968.4993135\n",
            "Validation Loss: 616780147.3030391\n",
            "Training Loss: 672064514.6117814\n",
            "Validation Loss: 616411290.8352069\n",
            "Training Loss: 671242702.1046846\n",
            "Validation Loss: 616047045.413832\n",
            "Training Loss: 670426461.2203568\n",
            "Validation Loss: 615687351.6427612\n",
            "Training Loss: 669615723.4617317\n",
            "Validation Loss: 615332151.1450428\n",
            "Training Loss: 668810421.5653386\n",
            "Validation Loss: 614981386.5424123\n",
            "Training Loss: 668010489.474983\n",
            "Validation Loss: 614635001.4352573\n",
            "Training Loss: 667215862.316039\n",
            "Validation Loss: 614292940.3830816\n",
            "Training Loss: 666426476.3704466\n",
            "Validation Loss: 613955148.8853567\n",
            "Training Loss: 665642269.0522355\n",
            "Validation Loss: 613621573.362869\n",
            "Training Loss: 664863178.8837291\n",
            "Validation Loss: 613292161.1394662\n",
            "Training Loss: 664089145.4722856\n",
            "Validation Loss: 612966860.4242042\n",
            "Training Loss: 663320109.4875869\n",
            "Validation Loss: 612645620.2939239\n",
            "Training Loss: 662556012.6395307\n",
            "Validation Loss: 612328390.6762089\n",
            "Training Loss: 661796797.6565571\n",
            "Validation Loss: 612015122.332706\n",
            "Training Loss: 661042408.264605\n",
            "Validation Loss: 611705766.8428392\n",
            "Training Loss: 660292789.1664703\n",
            "Validation Loss: 611400276.5878712\n",
            "Training Loss: 659547886.0216962\n",
            "Validation Loss: 611098604.7353058\n",
            "Training Loss: 658807645.4269259\n",
            "Validation Loss: 610800705.223659\n",
            "Training Loss: 658072014.896705\n",
            "Validation Loss: 610506532.7475346\n",
            "Training Loss: 657340942.8447536\n",
            "Validation Loss: 610216042.7430234\n",
            "Training Loss: 656614378.565632\n",
            "Validation Loss: 609929191.3734423\n",
            "Training Loss: 655892272.216852\n",
            "Validation Loss: 609645935.5153552\n",
            "Training Loss: 655174574.8014168\n",
            "Validation Loss: 609366232.7448958\n",
            "Training Loss: 654461238.1506894\n",
            "Validation Loss: 609090041.3243979\n",
            "Training Loss: 653752214.9077519\n",
            "Validation Loss: 608817320.1892823\n",
            "Training Loss: 653047458.5110393\n",
            "Validation Loss: 608548028.9352608\n",
            "Training Loss: 652346923.1784031\n",
            "Validation Loss: 608282127.8057479\n",
            "Training Loss: 651650563.8915234\n",
            "Validation Loss: 608019577.6796162\n",
            "Training Loss: 650958336.3806436\n",
            "Validation Loss: 607760340.059132\n",
            "Training Loss: 650270197.1096703\n",
            "Validation Loss: 607504377.0581865\n",
            "Training Loss: 649586103.2615877\n",
            "Validation Loss: 607251651.390771\n",
            "Training Loss: 648906012.7242186\n",
            "Validation Loss: 607002126.3596591\n",
            "Training Loss: 648229884.0762619\n",
            "Validation Loss: 606755765.8453579\n",
            "Training Loss: 647557676.5736507\n",
            "Validation Loss: 606512534.2952662\n",
            "Training Loss: 646889350.1362332\n",
            "Validation Loss: 606272396.7130632\n",
            "Training Loss: 646224865.3347021\n",
            "Validation Loss: 606035318.6483026\n",
            "Training Loss: 645564183.3778329\n",
            "Validation Loss: 605801266.186242\n",
            "Training Loss: 644907266.100005\n",
            "Validation Loss: 605570205.9378483\n",
            "Training Loss: 644254075.9489666\n",
            "Validation Loss: 605342105.030034\n",
            "Training Loss: 643604575.9738634\n",
            "Validation Loss: 605116931.0960729\n",
            "Training Loss: 642958729.8135812\n",
            "Validation Loss: 604894652.2662203\n",
            "Training Loss: 642316501.6852391\n",
            "Validation Loss: 604675237.1585095\n",
            "Training Loss: 641677856.3730232\n",
            "Validation Loss: 604458654.8697437\n",
            "Training Loss: 641042759.217187\n",
            "Validation Loss: 604244874.9666722\n",
            "Training Loss: 640411176.1033353\n",
            "Validation Loss: 604033867.4773303\n",
            "Training Loss: 639783073.4519045\n",
            "Validation Loss: 603825602.8825476\n",
            "Training Loss: 639158418.2078675\n",
            "Validation Loss: 603620052.1076529\n",
            "Training Loss: 638537177.8306711\n",
            "Validation Loss: 603417186.5143214\n",
            "Training Loss: 637919320.2843833\n",
            "Validation Loss: 603216977.8925807\n",
            "Training Loss: 637304814.0280104\n",
            "Validation Loss: 603019398.4529908\n",
            "Training Loss: 636693628.0060658\n",
            "Validation Loss: 602824420.8189739\n",
            "Training Loss: 636085731.639324\n",
            "Validation Loss: 602632018.0192702\n",
            "Training Loss: 635481094.8157294\n",
            "Validation Loss: 602442163.480609\n",
            "Training Loss: 634879687.8815483\n",
            "Validation Loss: 602254831.0204222\n",
            "Training Loss: 634281481.6326458\n",
            "Validation Loss: 602069994.8398116\n",
            "Training Loss: 633686447.3060176\n",
            "Validation Loss: 601887629.516553\n",
            "Training Loss: 633094556.5714152\n",
            "Validation Loss: 601707709.998325\n",
            "Training Loss: 632505781.5232052\n",
            "Validation Loss: 601530211.5960065\n",
            "Training Loss: 631920094.6723696\n",
            "Validation Loss: 601355109.9771276\n",
            "Training Loss: 631337468.938654\n",
            "Validation Loss: 601182381.1594616\n",
            "Training Loss: 630757877.6429235\n",
            "Validation Loss: 601012001.5047126\n",
            "Training Loss: 630181294.4996321\n",
            "Validation Loss: 600843947.7123551\n",
            "Training Loss: 629607693.6094706\n",
            "Validation Loss: 600678196.8135703\n",
            "Training Loss: 629037049.452138\n",
            "Validation Loss: 600514726.1653149\n",
            "Training Loss: 628469336.8793011\n",
            "Validation Loss: 600353513.4444954\n",
            "Training Loss: 627904531.1076487\n",
            "Validation Loss: 600194536.6422601\n",
            "Training Loss: 627342607.7121309\n",
            "Validation Loss: 600037774.0583925\n",
            "Training Loss: 626783542.6192983\n",
            "Validation Loss: 599883204.2958401\n",
            "Training Loss: 626227312.100805\n",
            "Validation Loss: 599730806.2553133\n",
            "Training Loss: 625673892.7670381\n",
            "Validation Loss: 599580559.1299875\n",
            "Training Loss: 625123261.5608313\n",
            "Validation Loss: 599432442.4003661\n",
            "Training Loss: 624575395.7513982\n",
            "Validation Loss: 599286435.8291581\n",
            "Training Loss: 624030272.9282717\n",
            "Validation Loss: 599142519.4563204\n",
            "Training Loss: 623487870.9954869\n",
            "Validation Loss: 599000673.59415\n",
            "Training Loss: 622948168.1657467\n",
            "Validation Loss: 598860878.8225187\n",
            "Training Loss: 622411142.9548557\n",
            "Validation Loss: 598723115.9841342\n",
            "Training Loss: 621876774.1761118\n",
            "Validation Loss: 598587366.1799579\n",
            "Training Loss: 621345040.934932\n",
            "Validation Loss: 598453610.7646643\n",
            "Training Loss: 620815922.623526\n",
            "Validation Loss: 598321831.3422092\n",
            "Training Loss: 620289398.9156806\n",
            "Validation Loss: 598192009.761476\n",
            "Training Loss: 619765449.7616563\n",
            "Validation Loss: 598064128.1120045\n",
            "Training Loss: 619244055.3831751\n",
            "Validation Loss: 597938168.7197922\n",
            "Training Loss: 618725196.2685505\n",
            "Validation Loss: 597814114.1432122\n",
            "Training Loss: 618208853.1678358\n",
            "Validation Loss: 597691947.1689436\n",
            "Training Loss: 617695007.0881388\n",
            "Validation Loss: 597571650.808051\n",
            "Training Loss: 617183639.2890048\n",
            "Validation Loss: 597453208.2920947\n",
            "Training Loss: 616674731.2778754\n",
            "Validation Loss: 597336603.0693041\n",
            "Training Loss: 616168264.805652\n",
            "Validation Loss: 597221818.8008733\n",
            "Training Loss: 615664221.8623637\n",
            "Validation Loss: 597108839.3572868\n",
            "Training Loss: 615162584.6728778\n",
            "Validation Loss: 596997648.8147182\n",
            "Training Loss: 614663335.6927456\n",
            "Validation Loss: 596888231.4515152\n",
            "Training Loss: 614166457.6040877\n",
            "Validation Loss: 596780571.7447352\n",
            "Training Loss: 613671933.3115717\n",
            "Validation Loss: 596674654.3667632\n",
            "Training Loss: 613179745.9384809\n",
            "Validation Loss: 596570464.1819428\n",
            "Training Loss: 612689878.8228725\n",
            "Validation Loss: 596467986.2433767\n",
            "Training Loss: 612202315.5137547\n",
            "Validation Loss: 596367205.7896559\n",
            "Training Loss: 611717039.767402\n",
            "Validation Loss: 596268108.2417576\n",
            "Training Loss: 611234035.5437009\n",
            "Validation Loss: 596170679.199935\n",
            "Training Loss: 610753287.0025982\n",
            "Validation Loss: 596074904.4407051\n",
            "Training Loss: 610274778.5005877\n",
            "Validation Loss: 595980769.9138699\n",
            "Training Loss: 609798494.5872872\n",
            "Validation Loss: 595888261.7396108\n",
            "Training Loss: 609324420.0020828\n",
            "Validation Loss: 595797366.205612\n",
            "Training Loss: 608852539.6708131\n",
            "Validation Loss: 595708069.764277\n",
            "Training Loss: 608382838.7025534\n",
            "Validation Loss: 595620359.029951\n",
            "Training Loss: 607915302.3864291\n",
            "Validation Loss: 595534220.7762457\n",
            "Training Loss: 607449916.1885285\n",
            "Validation Loss: 595449641.9333627\n",
            "Training Loss: 606986665.7488321\n",
            "Validation Loss: 595366609.5855052\n",
            "Training Loss: 606525536.8782407\n",
            "Validation Loss: 595285110.968328\n",
            "Training Loss: 606066515.5556227\n",
            "Validation Loss: 595205133.4664216\n",
            "Training Loss: 605609587.9249786\n",
            "Validation Loss: 595126664.6108674\n",
            "Training Loss: 605154740.2925596\n",
            "Validation Loss: 595049692.0768168\n",
            "Training Loss: 604701959.1241735\n",
            "Validation Loss: 594974203.6811126\n",
            "Training Loss: 604251231.0424196\n",
            "Validation Loss: 594900187.3799998\n",
            "Training Loss: 603802542.8240534\n",
            "Validation Loss: 594827631.2668123\n",
            "Training Loss: 603355881.3973825\n",
            "Validation Loss: 594756523.5697504\n",
            "Training Loss: 602911233.8396888\n",
            "Validation Loss: 594686852.6496857\n",
            "Training Loss: 602468587.3747534\n",
            "Validation Loss: 594618606.9980078\n",
            "Training Loss: 602027929.3703452\n",
            "Validation Loss: 594551775.2345055\n",
            "Training Loss: 601589247.3358749\n",
            "Validation Loss: 594486346.1052876\n",
            "Training Loss: 601152528.9199606\n",
            "Validation Loss: 594422308.4807619\n",
            "Training Loss: 600717761.9081619\n",
            "Validation Loss: 594359651.3536175\n",
            "Training Loss: 600284934.2206736\n",
            "Validation Loss: 594298363.8368756\n",
            "Training Loss: 599854033.9101012\n",
            "Validation Loss: 594238435.1619632\n",
            "Training Loss: 599425049.1592611\n",
            "Validation Loss: 594179854.6768241\n",
            "Training Loss: 598997968.2790484\n",
            "Validation Loss: 594122611.8440545\n",
            "Training Loss: 598572779.7063123\n",
            "Validation Loss: 594066696.239101\n",
            "Training Loss: 598149472.001821\n",
            "Validation Loss: 594012097.5484477\n",
            "Training Loss: 597728033.8481745\n",
            "Validation Loss: 593958805.5679015\n",
            "Training Loss: 597308454.0478836\n",
            "Validation Loss: 593906810.2008287\n",
            "Training Loss: 596890721.5213751\n",
            "Validation Loss: 593856101.456493\n",
            "Training Loss: 596474825.305085\n",
            "Validation Loss: 593806669.4484009\n",
            "Training Loss: 596060754.549584\n",
            "Validation Loss: 593758504.3926412\n",
            "Training Loss: 595648498.5177479\n",
            "Validation Loss: 593711596.6063331\n",
            "Training Loss: 595238046.5829358\n",
            "Validation Loss: 593665936.5060189\n",
            "Training Loss: 594829388.2272052\n",
            "Validation Loss: 593621514.6061522\n",
            "Training Loss: 594422513.0396062\n",
            "Validation Loss: 593578321.51757\n",
            "Training Loss: 594017410.7144339\n",
            "Validation Loss: 593536347.9460218\n",
            "Training Loss: 593614071.0496092\n",
            "Validation Loss: 593495584.6907014\n",
            "Training Loss: 593212483.944965\n",
            "Validation Loss: 593456022.6428282\n",
            "Training Loss: 592812639.4007034\n",
            "Validation Loss: 593417652.784242\n",
            "Training Loss: 592414527.5157576\n",
            "Validation Loss: 593380466.1860192\n",
            "Training Loss: 592018138.486288\n",
            "Validation Loss: 593344454.0071306\n",
            "Training Loss: 591623462.6041203\n",
            "Validation Loss: 593309607.4931136\n",
            "Training Loss: 591230490.2552814\n",
            "Validation Loss: 593275917.9747524\n",
            "Training Loss: 590839211.9185094\n",
            "Validation Loss: 593243376.8668218\n",
            "Training Loss: 590449618.1638302\n",
            "Validation Loss: 593211975.6668229\n",
            "Training Loss: 590061699.6511526\n",
            "Validation Loss: 593181705.9537452\n",
            "Training Loss: 589675447.1288617\n",
            "Validation Loss: 593152559.3868549\n",
            "Training Loss: 589290851.4324689\n",
            "Validation Loss: 593124527.70452\n",
            "Training Loss: 588907903.48329\n",
            "Validation Loss: 593097602.7230281\n",
            "Training Loss: 588526594.2871152\n",
            "Validation Loss: 593071776.3354491\n",
            "Training Loss: 588146914.9329464\n",
            "Validation Loss: 593047040.5105164\n",
            "Training Loss: 587768856.5917202\n",
            "Validation Loss: 593023387.2914939\n",
            "Training Loss: 587392410.5150784\n",
            "Validation Loss: 593000808.7951411\n",
            "Training Loss: 587017568.0341452\n",
            "Validation Loss: 592979297.2105973\n",
            "Training Loss: 586644320.558349\n",
            "Validation Loss: 592958844.798363\n",
            "Training Loss: 586272659.5742543\n",
            "Validation Loss: 592939443.8892738\n",
            "Training Loss: 585902576.6443706\n",
            "Validation Loss: 592921086.8834841\n",
            "Training Loss: 585534063.4061002\n",
            "Validation Loss: 592903766.2494818\n",
            "Training Loss: 585167111.5705705\n",
            "Validation Loss: 592887474.5231133\n",
            "Training Loss: 584801712.9215671\n",
            "Validation Loss: 592872204.3066278\n",
            "Training Loss: 584437859.314488\n",
            "Validation Loss: 592857948.2677438\n",
            "Training Loss: 584075542.6752616\n",
            "Validation Loss: 592844699.1387272\n",
            "Training Loss: 583714754.9993423\n",
            "Validation Loss: 592832449.7154888\n",
            "Training Loss: 583355488.3507063\n",
            "Validation Loss: 592821192.8566893\n",
            "Training Loss: 582997734.8608531\n",
            "Validation Loss: 592810921.4828755\n",
            "Training Loss: 582641486.7278222\n",
            "Validation Loss: 592801628.5756161\n",
            "Training Loss: 582286736.2152735\n",
            "Validation Loss: 592793307.1766633\n",
            "Training Loss: 581933475.651508\n",
            "Validation Loss: 592785950.3871478\n",
            "Training Loss: 581581697.4285975\n",
            "Validation Loss: 592779551.3667215\n",
            "Training Loss: 581231394.0014194\n",
            "Validation Loss: 592774103.3327948\n",
            "Training Loss: 580882557.886843\n",
            "Validation Loss: 592769599.5597533\n",
            "Training Loss: 580535181.6627928\n",
            "Validation Loss: 592766033.3781725\n",
            "Training Loss: 580189257.9674475\n",
            "Validation Loss: 592763398.1740731\n",
            "Training Loss: 579844779.4983693\n",
            "Validation Loss: 592761687.3881756\n",
            "Training Loss: 579501739.0116858\n",
            "Validation Loss: 592760894.5151739\n",
            "Training Loss: 579160129.3212838\n",
            "Validation Loss: 592761013.1030172\n",
            "Training Loss: 578819943.2980279\n",
            "Validation Loss: 592762036.7522001\n",
            "Training Loss: 578481173.8689581\n",
            "Validation Loss: 592763959.115093\n",
            "Training Loss: 578143814.0165424\n",
            "Validation Loss: 592766773.8952315\n",
            "Training Loss: 577807856.7779039\n",
            "Validation Loss: 592770474.8466784\n",
            "Training Loss: 577473295.2441087\n",
            "Validation Loss: 592775055.7733508\n",
            "Training Loss: 577140122.5594336\n",
            "Validation Loss: 592780510.5283809\n",
            "Training Loss: 576808331.9206204\n",
            "Validation Loss: 592786833.0134872\n",
            "Training Loss: 576477916.5762298\n",
            "Validation Loss: 592794017.1783466\n",
            "Training Loss: 576148869.8259121\n",
            "Validation Loss: 592802057.0199878\n",
            "Training Loss: 575821185.0197545\n",
            "Validation Loss: 592810946.5821856\n",
            "Training Loss: 575494855.5576131\n",
            "Validation Loss: 592820679.9548831\n",
            "Training Loss: 575169874.8884528\n",
            "Validation Loss: 592831251.2736027\n",
            "Training Loss: 574846236.5097107\n",
            "Validation Loss: 592842654.7188731\n",
            "Training Loss: 574523933.9666876\n",
            "Validation Loss: 592854884.5156754\n",
            "Training Loss: 574202960.8518975\n",
            "Validation Loss: 592867934.9328989\n",
            "Training Loss: 573883310.8044928\n",
            "Validation Loss: 592881800.2827841\n",
            "Training Loss: 573564977.5096332\n",
            "Validation Loss: 592896474.920413\n",
            "Training Loss: 573247954.697939\n",
            "Validation Loss: 592911953.2431626\n",
            "Training Loss: 572932236.1448878\n",
            "Validation Loss: 592928229.6902124\n",
            "Training Loss: 572617815.6702638\n",
            "Validation Loss: 592945298.74203\n",
            "Training Loss: 572304687.1375936\n",
            "Validation Loss: 592963154.9198776\n",
            "Training Loss: 571992844.4536021\n",
            "Validation Loss: 592981792.7853202\n",
            "Training Loss: 571682281.5676894\n",
            "Validation Loss: 593001206.9397463\n",
            "Training Loss: 571372992.4713824\n",
            "Validation Loss: 593021392.0239146\n",
            "Training Loss: 571064971.1978285\n",
            "Validation Loss: 593042342.7174667\n",
            "Training Loss: 570758211.8212948\n",
            "Validation Loss: 593064053.7384784\n",
            "Training Loss: 570452708.4566538\n",
            "Validation Loss: 593086519.843025\n",
            "Training Loss: 570148455.2588978\n",
            "Validation Loss: 593109735.8247248\n",
            "Training Loss: 569845446.4226509\n",
            "Validation Loss: 593133696.5143194\n",
            "Training Loss: 569543676.1817089\n",
            "Validation Loss: 593158396.7792351\n",
            "Training Loss: 569243138.80856\n",
            "Validation Loss: 593183831.5231798\n",
            "Training Loss: 568943828.6139225\n",
            "Validation Loss: 593209995.6857152\n",
            "Training Loss: 568645739.9463092\n",
            "Validation Loss: 593236884.2418641\n",
            "Training Loss: 568348867.1915736\n",
            "Validation Loss: 593264492.2017134\n",
            "Training Loss: 568053204.772471\n",
            "Validation Loss: 593292814.6100144\n",
            "Training Loss: 567758747.1482346\n",
            "Validation Loss: 593321846.5458015\n",
            "Training Loss: 567465488.8141714\n",
            "Validation Loss: 593351583.1220163\n",
            "Training Loss: 567173424.301212\n",
            "Validation Loss: 593382019.4851309\n",
            "Training Loss: 566882548.1755363\n",
            "Validation Loss: 593413150.8147911\n",
            "Training Loss: 566592855.0381556\n",
            "Validation Loss: 593444972.3234413\n",
            "Training Loss: 566304339.5245283\n",
            "Validation Loss: 593477479.2559861\n",
            "Training Loss: 566016996.304154\n",
            "Validation Loss: 593510666.8894302\n",
            "Training Loss: 565730820.0802295\n",
            "Validation Loss: 593544530.5325433\n",
            "Training Loss: 565445805.5892465\n",
            "Validation Loss: 593579065.5255113\n",
            "Training Loss: 565161947.6006132\n",
            "Validation Loss: 593614267.2396274\n",
            "Training Loss: 564879240.916335\n",
            "Validation Loss: 593650131.0769325\n",
            "Training Loss: 564597680.3706124\n",
            "Validation Loss: 593686652.469932\n",
            "Training Loss: 564317260.8295219\n",
            "Validation Loss: 593723826.8812397\n",
            "Training Loss: 564037977.1906569\n",
            "Validation Loss: 593761649.8033067\n",
            "Training Loss: 563759824.3827888\n",
            "Validation Loss: 593800116.7580845\n",
            "Training Loss: 563482797.3655516\n",
            "Validation Loss: 593839223.296741\n",
            "Training Loss: 563206891.1290808\n",
            "Validation Loss: 593878964.9993472\n",
            "Training Loss: 562932100.6937335\n",
            "Validation Loss: 593919337.4746145\n",
            "Training Loss: 562658421.1097243\n",
            "Validation Loss: 593960336.359575\n",
            "Training Loss: 562385847.4568539\n",
            "Validation Loss: 594001957.3193226\n",
            "Training Loss: 562114374.8441826\n",
            "Validation Loss: 594044196.0467203\n",
            "Training Loss: 561843998.4097203\n",
            "Validation Loss: 594087048.2621353\n",
            "Training Loss: 561574713.3201474\n",
            "Validation Loss: 594130509.7131655\n",
            "Training Loss: 561306514.7705164\n",
            "Validation Loss: 594174576.1743752\n",
            "Training Loss: 561039397.9839547\n",
            "Validation Loss: 594219243.4470298\n",
            "Training Loss: 560773358.2113913\n",
            "Validation Loss: 594264507.3588485\n",
            "Training Loss: 560508390.7312772\n",
            "Validation Loss: 594310363.763744\n",
            "Training Loss: 560244490.8493041\n",
            "Validation Loss: 594356808.5415655\n",
            "Training Loss: 559981653.8981363\n",
            "Validation Loss: 594403837.5978725\n",
            "Training Loss: 559719875.2371575\n",
            "Validation Loss: 594451446.863673\n",
            "Training Loss: 559459150.2521791\n",
            "Validation Loss: 594499632.2952005\n",
            "Training Loss: 559199474.3552233\n",
            "Validation Loss: 594548389.8736753\n",
            "Training Loss: 558940842.984231\n",
            "Validation Loss: 594597715.6050602\n",
            "Training Loss: 558683251.6028371\n",
            "Validation Loss: 594647605.5198528\n",
            "Training Loss: 558426695.7001143\n",
            "Validation Loss: 594698055.6728535\n",
            "Training Loss: 558171170.7903249\n",
            "Validation Loss: 594749062.142942\n",
            "Training Loss: 557916672.4126954\n",
            "Validation Loss: 594800621.0328557\n",
            "Training Loss: 557663196.1311858\n",
            "Validation Loss: 594852728.4689938\n",
            "Training Loss: 557410737.5342311\n",
            "Validation Loss: 594905380.6011837\n",
            "Training Loss: 557159292.2345362\n",
            "Validation Loss: 594958573.6024894\n",
            "Training Loss: 556908855.8688587\n",
            "Validation Loss: 595012303.6689839\n",
            "Training Loss: 556659424.0977546\n",
            "Validation Loss: 595066567.0195928\n",
            "Training Loss: 556410992.605399\n",
            "Validation Loss: 595121359.8958447\n",
            "Training Loss: 556163557.0993468\n",
            "Validation Loss: 595176678.5617006\n",
            "Training Loss: 555917113.3103231\n",
            "Validation Loss: 595232519.3033489\n",
            "Training Loss: 555671656.9920365\n",
            "Validation Loss: 595288878.4290407\n",
            "Training Loss: 555427183.920942\n",
            "Validation Loss: 595345752.2688689\n",
            "Training Loss: 555183689.8960615\n",
            "Validation Loss: 595403137.174603\n",
            "Training Loss: 554941170.7387868\n",
            "Validation Loss: 595461029.5194968\n",
            "Training Loss: 554699622.2926598\n",
            "Validation Loss: 595519425.698119\n",
            "Training Loss: 554459040.4232147\n",
            "Validation Loss: 595578322.1261605\n",
            "Training Loss: 554219421.0177491\n",
            "Validation Loss: 595637715.2402846\n",
            "Training Loss: 553980759.9851576\n",
            "Validation Loss: 595697601.4979179\n",
            "Training Loss: 553743053.2557639\n",
            "Validation Loss: 595757977.3771176\n",
            "Training Loss: 553506296.7810926\n",
            "Validation Loss: 595818839.3763795\n",
            "Training Loss: 553270486.5337307\n",
            "Validation Loss: 595880184.0144819\n",
            "Training Loss: 553035618.5071375\n",
            "Validation Loss: 595942007.8303186\n",
            "Training Loss: 552801688.7154518\n",
            "Validation Loss: 596004307.3827438\n",
            "Training Loss: 552568693.1933554\n",
            "Validation Loss: 596067079.2504057\n",
            "Training Loss: 552336627.9958748\n",
            "Validation Loss: 596130320.0315973\n",
            "Training Loss: 552105489.1982158\n",
            "Validation Loss: 596194026.3441026\n",
            "Training Loss: 551875272.8956221\n",
            "Validation Loss: 596258194.825027\n",
            "Training Loss: 551645975.2031804\n",
            "Validation Loss: 596322822.1306725\n",
            "Training Loss: 551417592.2556905\n",
            "Validation Loss: 596387904.936374\n",
            "Training Loss: 551190120.2074711\n",
            "Validation Loss: 596453439.9363531\n",
            "Training Loss: 550963555.2322437\n",
            "Validation Loss: 596519423.8435819\n",
            "Training Loss: 550737893.5229537\n",
            "Validation Loss: 596585853.3896283\n",
            "Training Loss: 550513131.2916244\n",
            "Validation Loss: 596652725.3245255\n",
            "Training Loss: 550289264.7692019\n",
            "Validation Loss: 596720036.4166377\n",
            "Training Loss: 550066290.2054158\n",
            "Validation Loss: 596787783.4525009\n",
            "Training Loss: 549844203.8686376\n",
            "Validation Loss: 596855963.2367134\n",
            "Training Loss: 549623002.0457178\n",
            "Validation Loss: 596924572.5917792\n",
            "Training Loss: 549402681.0418626\n",
            "Validation Loss: 596993608.3580023\n",
            "Training Loss: 549183237.1804842\n",
            "Validation Loss: 597063067.3933349\n",
            "Training Loss: 548964666.8030651\n",
            "Validation Loss: 597132946.5732505\n",
            "Training Loss: 548746966.2690209\n",
            "Validation Loss: 597203242.7906309\n",
            "Training Loss: 548530131.955574\n",
            "Validation Loss: 597273952.9556297\n",
            "Training Loss: 548314160.2576033\n",
            "Validation Loss: 597345073.9955488\n",
            "Training Loss: 548099047.5875275\n",
            "Validation Loss: 597416602.8547239\n",
            "Training Loss: 547884790.3751743\n",
            "Validation Loss: 597488536.4943901\n",
            "Training Loss: 547671385.0676326\n",
            "Validation Loss: 597560871.8925796\n",
            "Training Loss: 547458828.1291726\n",
            "Validation Loss: 597633606.0439911\n",
            "Training Loss: 547247116.0410637\n",
            "Validation Loss: 597706735.9598801\n",
            "Training Loss: 547036245.3014985\n",
            "Validation Loss: 597780258.6679413\n",
            "Training Loss: 546826212.4254422\n",
            "Validation Loss: 597854171.2121813\n",
            "Training Loss: 546617013.9445287\n",
            "Validation Loss: 597928470.6528493\n",
            "Training Loss: 546408646.4069324\n",
            "Validation Loss: 598003154.0662662\n",
            "Training Loss: 546201106.3772521\n",
            "Validation Loss: 598078218.5447711\n",
            "Training Loss: 545994390.4364128\n",
            "Validation Loss: 598153661.1965706\n",
            "Training Loss: 545788495.1815232\n",
            "Validation Loss: 598229479.145653\n",
            "Training Loss: 545583417.2257805\n",
            "Validation Loss: 598305669.5316873\n",
            "Training Loss: 545379153.1983619\n",
            "Validation Loss: 598382229.5098985\n",
            "Training Loss: 545175699.7443029\n",
            "Validation Loss: 598459156.2509779\n",
            "Training Loss: 544973053.5243943\n",
            "Validation Loss: 598536446.9409864\n",
            "Training Loss: 544771211.215074\n",
            "Validation Loss: 598614098.7812322\n",
            "Training Loss: 544570169.508333\n",
            "Validation Loss: 598692108.9881988\n",
            "Training Loss: 544369925.1115822\n",
            "Validation Loss: 598770474.7934167\n",
            "Training Loss: 544170474.7475775\n",
            "Validation Loss: 598849193.443393\n",
            "Training Loss: 543971815.154301\n",
            "Validation Loss: 598928262.1994998\n",
            "Training Loss: 543773943.0848724\n",
            "Validation Loss: 599007678.3378717\n",
            "Training Loss: 543576855.3074272\n",
            "Validation Loss: 599087439.1493319\n",
            "Training Loss: 543380548.6050446\n",
            "Validation Loss: 599167541.9392843\n",
            "Training Loss: 543185019.7756337\n",
            "Validation Loss: 599247984.0276262\n",
            "Training Loss: 542990265.6318399\n",
            "Validation Loss: 599328762.7486472\n",
            "Training Loss: 542796283.000948\n",
            "Validation Loss: 599409875.4509542\n",
            "Training Loss: 542603068.7247899\n",
            "Validation Loss: 599491319.4973732\n",
            "Training Loss: 542410619.6596622\n",
            "Validation Loss: 599573092.2648611\n",
            "Training Loss: 542218932.6761972\n",
            "Validation Loss: 599655191.1444216\n",
            "Training Loss: 542028004.6593277\n",
            "Validation Loss: 599737613.5410051\n",
            "Training Loss: 541837832.5081428\n",
            "Validation Loss: 599820356.8734488\n",
            "Training Loss: 541648413.1358336\n",
            "Validation Loss: 599903418.5743649\n",
            "Training Loss: 541459743.469595\n",
            "Validation Loss: 599986796.090073\n",
            "Training Loss: 541271820.4505261\n",
            "Validation Loss: 600070486.8805076\n",
            "Training Loss: 541084641.0335599\n",
            "Validation Loss: 600154488.4191543\n",
            "Training Loss: 540898202.1873811\n",
            "Validation Loss: 600238798.1929287\n",
            "Training Loss: 540712500.8943137\n",
            "Validation Loss: 600323413.7021456\n",
            "Training Loss: 540527534.1502793\n",
            "Validation Loss: 600408332.4604038\n",
            "Training Loss: 540343298.9646673\n",
            "Validation Loss: 600493551.9945134\n",
            "Training Loss: 540159792.3602983\n",
            "Validation Loss: 600579069.8444306\n",
            "Training Loss: 539977011.3733093\n",
            "Validation Loss: 600664883.5631727\n",
            "Training Loss: 539794953.053094\n",
            "Validation Loss: 600750990.7167281\n",
            "Training Loss: 539613614.4622054\n",
            "Validation Loss: 600837388.8840019\n",
            "Training Loss: 539432992.6762943\n",
            "Validation Loss: 600924075.6567286\n",
            "Training Loss: 539253084.7840253\n",
            "Validation Loss: 601011048.639398\n",
            "Training Loss: 539073887.8869984\n",
            "Validation Loss: 601098305.4491887\n",
            "Training Loss: 538895399.0996776\n",
            "Validation Loss: 601185843.7158781\n",
            "Training Loss: 538717615.5492914\n",
            "Validation Loss: 601273661.0817933\n",
            "Training Loss: 538540534.3758103\n",
            "Validation Loss: 601361755.2017183\n",
            "Training Loss: 538364152.7318153\n",
            "Validation Loss: 601450123.7428334\n",
            "Training Loss: 538188467.7824599\n",
            "Validation Loss: 601538764.3846458\n",
            "Training Loss: 538013476.7053881\n",
            "Validation Loss: 601627674.8189137\n",
            "Training Loss: 537839176.6906648\n",
            "Validation Loss: 601716852.7495766\n",
            "Training Loss: 537665564.9406945\n",
            "Validation Loss: 601806295.8927001\n",
            "Training Loss: 537492638.6701696\n",
            "Validation Loss: 601896001.9763869\n",
            "Training Loss: 537320395.1059794\n",
            "Validation Loss: 601985968.7407303\n",
            "Training Loss: 537148831.4871585\n",
            "Validation Loss: 602076193.9377207\n",
            "Training Loss: 536977945.0648129\n",
            "Validation Loss: 602166675.3312191\n",
            "Training Loss: 536807733.1020416\n",
            "Validation Loss: 602257410.6968516\n",
            "Training Loss: 536638192.8738869\n",
            "Validation Loss: 602348397.8219664\n",
            "Training Loss: 536469321.6672653\n",
            "Validation Loss: 602439634.5055635\n",
            "Training Loss: 536301116.78088236\n",
            "Validation Loss: 602531118.55823\n",
            "Training Loss: 536133575.52519584\n",
            "Validation Loss: 602622847.8020821\n",
            "Training Loss: 535966695.22231776\n",
            "Validation Loss: 602714820.0706934\n",
            "Training Loss: 535800473.20598954\n",
            "Validation Loss: 602807033.2090393\n",
            "Training Loss: 535634906.8214901\n",
            "Validation Loss: 602899485.0734341\n",
            "Training Loss: 535469993.4255776\n",
            "Validation Loss: 602992173.531468\n",
            "Training Loss: 535305730.3864323\n",
            "Validation Loss: 603085096.4619433\n",
            "Training Loss: 535142115.0835995\n",
            "Validation Loss: 603178251.7548265\n",
            "Training Loss: 534979144.9079112\n",
            "Validation Loss: 603271637.3111664\n",
            "Training Loss: 534816817.26144737\n",
            "Validation Loss: 603365251.0430632\n",
            "Training Loss: 534655129.55745584\n",
            "Validation Loss: 603459090.8735859\n",
            "Training Loss: 534494079.22030616\n",
            "Validation Loss: 603553154.7367254\n",
            "Training Loss: 534333663.68541896\n",
            "Validation Loss: 603647440.577329\n",
            "Training Loss: 534173880.39922506\n",
            "Validation Loss: 603741946.351055\n",
            "Training Loss: 534014726.81908715\n",
            "Validation Loss: 603836670.024306\n",
            "Training Loss: 533856200.41326016\n",
            "Validation Loss: 603931609.5741715\n",
            "Training Loss: 533698298.66081405\n",
            "Validation Loss: 604026762.9883798\n",
            "Training Loss: 533541019.05159485\n",
            "Validation Loss: 604122128.2652359\n",
            "Training Loss: 533384359.0861647\n",
            "Validation Loss: 604217703.4135656\n",
            "Training Loss: 533228316.27574074\n",
            "Validation Loss: 604313486.4526703\n",
            "Training Loss: 533072888.1421427\n",
            "Validation Loss: 604409475.4122581\n",
            "Training Loss: 532918072.2177325\n",
            "Validation Loss: 604505668.3323956\n",
            "Training Loss: 532763866.04538035\n",
            "Validation Loss: 604602063.2634681\n",
            "Training Loss: 532610267.1783795\n",
            "Validation Loss: 604698658.2661009\n",
            "Training Loss: 532457273.18042076\n",
            "Validation Loss: 604795451.4111279\n",
            "Training Loss: 532304881.6255221\n",
            "Validation Loss: 604892440.7795234\n",
            "Training Loss: 532153090.0979872\n",
            "Validation Loss: 604989624.4623688\n",
            "Training Loss: 532001896.1923404\n",
            "Validation Loss: 605087000.5607811\n",
            "Training Loss: 531851297.51328945\n",
            "Validation Loss: 605184567.185871\n",
            "Training Loss: 531701291.6756629\n",
            "Validation Loss: 605282322.4586972\n",
            "Training Loss: 531551876.3043667\n",
            "Validation Loss: 605380264.5102026\n",
            "Training Loss: 531403049.0343312\n",
            "Validation Loss: 605478391.4811807\n",
            "Training Loss: 531254807.5104571\n",
            "Validation Loss: 605576701.5222069\n",
            "Training Loss: 531107149.3875661\n",
            "Validation Loss: 605675192.7936076\n",
            "Training Loss: 530960072.33036214\n",
            "Validation Loss: 605773863.4653926\n",
            "Training Loss: 530813574.01337194\n",
            "Validation Loss: 605872711.7172279\n",
            "Training Loss: 530667652.1208899\n",
            "Validation Loss: 605971735.7383653\n",
            "Training Loss: 530522304.3469548\n",
            "Validation Loss: 606070933.7276075\n",
            "Training Loss: 530377528.3952697\n",
            "Validation Loss: 606170303.89326\n",
            "Training Loss: 530233321.97917813\n",
            "Validation Loss: 606269844.4530813\n",
            "Training Loss: 530089682.82161367\n",
            "Validation Loss: 606369553.6342319\n",
            "Training Loss: 529946608.65503234\n",
            "Validation Loss: 606469429.6732265\n",
            "Training Loss: 529804097.2214018\n",
            "Validation Loss: 606569470.8159075\n",
            "Training Loss: 529662146.27213347\n",
            "Validation Loss: 606669675.3173637\n",
            "Training Loss: 529520753.56801736\n",
            "Validation Loss: 606770041.4419178\n",
            "Training Loss: 529379916.87922704\n",
            "Validation Loss: 606870567.463061\n",
            "Training Loss: 529239633.9852218\n",
            "Validation Loss: 606971251.6634198\n",
            "Training Loss: 529099902.67473435\n",
            "Validation Loss: 607072092.3346952\n",
            "Training Loss: 528960720.74572504\n",
            "Validation Loss: 607173087.7776328\n",
            "Training Loss: 528822086.00532067\n",
            "Validation Loss: 607274236.3019727\n",
            "Training Loss: 528683996.26977515\n",
            "Validation Loss: 607375536.2264115\n",
            "Training Loss: 528546449.36443925\n",
            "Validation Loss: 607476985.8785483\n",
            "Training Loss: 528409443.1237103\n",
            "Validation Loss: 607578583.5948358\n",
            "Training Loss: 528272975.39098114\n",
            "Validation Loss: 607680327.7205678\n",
            "Training Loss: 528137044.0186062\n",
            "Validation Loss: 607782216.609797\n",
            "Training Loss: 528001646.86785346\n",
            "Validation Loss: 607884248.6253217\n",
            "Training Loss: 527866781.8088687\n",
            "Validation Loss: 607986422.1386247\n",
            "Training Loss: 527732446.72062707\n",
            "Validation Loss: 608088735.5298445\n",
            "Training Loss: 527598639.4909022\n",
            "Validation Loss: 608191187.187724\n",
            "Training Loss: 527465358.0162006\n",
            "Validation Loss: 608293775.5095708\n",
            "Training Loss: 527332600.2017572\n",
            "Validation Loss: 608396498.9012215\n",
            "Training Loss: 527200363.96146303\n",
            "Validation Loss: 608499355.7769966\n",
            "Training Loss: 527068647.2178402\n",
            "Validation Loss: 608602344.5596551\n",
            "Training Loss: 526937447.9019944\n",
            "Validation Loss: 608705463.6803569\n",
            "Training Loss: 526806763.9535839\n",
            "Validation Loss: 608808711.5786266\n",
            "Training Loss: 526676593.32076675\n",
            "Validation Loss: 608912086.7023097\n",
            "Training Loss: 526546933.96018064\n",
            "Validation Loss: 609015587.5075347\n",
            "Training Loss: 526417783.83687896\n",
            "Validation Loss: 609119212.4586641\n",
            "Training Loss: 526289140.92431545\n",
            "Validation Loss: 609222960.0282738\n",
            "Training Loss: 526161003.2042847\n",
            "Validation Loss: 609326828.6970901\n",
            "Training Loss: 526033368.66690713\n",
            "Validation Loss: 609430816.9539683\n",
            "Training Loss: 525906235.31056625\n",
            "Validation Loss: 609534923.2958544\n",
            "Training Loss: 525779601.1418891\n",
            "Validation Loss: 609639146.2277285\n",
            "Training Loss: 525653464.17569304\n",
            "Validation Loss: 609743484.2625904\n",
            "Training Loss: 525527822.43496716\n",
            "Validation Loss: 609847935.9214017\n",
            "Training Loss: 525402673.950824\n",
            "Validation Loss: 609952499.7330607\n",
            "Training Loss: 525278016.7624545\n",
            "Validation Loss: 610057174.2343513\n",
            "Training Loss: 525153848.91710335\n",
            "Validation Loss: 610161957.9699259\n",
            "Training Loss: 525030168.4700369\n",
            "Validation Loss: 610266849.492246\n",
            "Training Loss: 524906973.48448616\n",
            "Validation Loss: 610371847.3615602\n",
            "Training Loss: 524784262.0316374\n",
            "Validation Loss: 610476950.1458638\n",
            "Training Loss: 524662032.1905727\n",
            "Validation Loss: 610582156.420854\n",
            "Training Loss: 524540282.0482388\n",
            "Validation Loss: 610687464.7699063\n",
            "Training Loss: 524419009.69943154\n",
            "Validation Loss: 610792873.7840317\n",
            "Training Loss: 524298213.24674153\n",
            "Validation Loss: 610898382.0618374\n",
            "Training Loss: 524177890.8005107\n",
            "Validation Loss: 611003988.2094941\n",
            "Training Loss: 524058040.4788265\n",
            "Validation Loss: 611109690.8407081\n",
            "Training Loss: 523938660.407462\n",
            "Validation Loss: 611215488.5766655\n",
            "Training Loss: 523819748.7198572\n",
            "Validation Loss: 611321380.0460267\n",
            "Training Loss: 523701303.5570707\n",
            "Validation Loss: 611427363.8848575\n",
            "Training Loss: 523583323.06774974\n",
            "Validation Loss: 611533438.736621\n",
            "Training Loss: 523465805.40811086\n",
            "Validation Loss: 611639603.2521307\n",
            "Training Loss: 523348748.7418866\n",
            "Validation Loss: 611745856.0895215\n",
            "Training Loss: 523232151.2403107\n",
            "Validation Loss: 611852195.9142041\n",
            "Training Loss: 523116011.0820645\n",
            "Validation Loss: 611958621.3988487\n",
            "Training Loss: 523000326.45325315\n",
            "Validation Loss: 612065131.2233295\n",
            "Training Loss: 522885095.547375\n",
            "Validation Loss: 612171724.074712\n",
            "Training Loss: 522770316.5653009\n",
            "Validation Loss: 612278398.6472102\n",
            "Training Loss: 522655987.7152063\n",
            "Validation Loss: 612385153.6421443\n",
            "Training Loss: 522542107.2125827\n",
            "Validation Loss: 612491987.7679244\n",
            "Training Loss: 522428673.28016305\n",
            "Validation Loss: 612598899.740004\n",
            "Training Loss: 522315684.1479277\n",
            "Validation Loss: 612705888.2808548\n",
            "Training Loss: 522203138.0530439\n",
            "Validation Loss: 612812952.1199298\n",
            "Training Loss: 522091033.239854\n",
            "Validation Loss: 612920089.9936295\n",
            "Training Loss: 521979367.95983386\n",
            "Validation Loss: 613027300.6452739\n",
            "Training Loss: 521868140.471558\n",
            "Validation Loss: 613134582.8250743\n",
            "Training Loss: 521757349.0406783\n",
            "Validation Loss: 613241935.2900876\n",
            "Training Loss: 521646991.93989027\n",
            "Validation Loss: 613349356.8041886\n",
            "Training Loss: 521537067.44890136\n",
            "Validation Loss: 613456846.1380545\n",
            "Training Loss: 521427573.8543945\n",
            "Validation Loss: 613564402.069112\n",
            "Training Loss: 521318509.4500076\n",
            "Validation Loss: 613672023.3815098\n",
            "Training Loss: 521209872.536303\n",
            "Validation Loss: 613779708.8661022\n",
            "Training Loss: 521101661.4207189\n",
            "Validation Loss: 613887457.3203988\n",
            "Training Loss: 520993874.4175711\n",
            "Validation Loss: 613995267.5485494\n",
            "Training Loss: 520886509.84799594\n",
            "Validation Loss: 614103138.3612996\n",
            "Training Loss: 520779566.03993213\n",
            "Validation Loss: 614211068.5759679\n",
            "Training Loss: 520673041.3280944\n",
            "Validation Loss: 614319057.016416\n",
            "Training Loss: 520566934.0539301\n",
            "Validation Loss: 614427102.5130146\n",
            "Training Loss: 520461242.565612\n",
            "Validation Loss: 614535203.9026144\n",
            "Training Loss: 520355965.21798915\n",
            "Validation Loss: 614643360.028518\n",
            "Training Loss: 520251100.37256485\n",
            "Validation Loss: 614751569.7404463\n",
            "Training Loss: 520146646.39748085\n",
            "Validation Loss: 614859831.8945143\n",
            "Training Loss: 520042601.66745496\n",
            "Validation Loss: 614968145.3531904\n",
            "Training Loss: 519938964.56380576\n",
            "Validation Loss: 615076508.9852779\n",
            "Training Loss: 519835733.47435886\n",
            "Validation Loss: 615184921.6658942\n",
            "Training Loss: 519732906.7934734\n",
            "Validation Loss: 615293382.2764093\n",
            "Training Loss: 519630482.9220082\n",
            "Validation Loss: 615401889.7044482\n",
            "Training Loss: 519528460.2672455\n",
            "Validation Loss: 615510442.8438495\n",
            "Training Loss: 519426837.2429182\n",
            "Validation Loss: 615619040.5946363\n",
            "Training Loss: 519325612.2691652\n",
            "Validation Loss: 615727681.8629876\n",
            "Training Loss: 519224783.77248603\n",
            "Validation Loss: 615836365.561218\n",
            "Training Loss: 519124350.1857462\n",
            "Validation Loss: 615945090.6077393\n",
            "Training Loss: 519024309.9481187\n",
            "Validation Loss: 616053855.9270287\n",
            "Training Loss: 518924661.5050729\n",
            "Validation Loss: 616162660.4496182\n",
            "Training Loss: 518825403.3083509\n",
            "Validation Loss: 616271503.1120486\n",
            "Training Loss: 518726533.8159309\n",
            "Validation Loss: 616380382.8568498\n",
            "Training Loss: 518628051.4920049\n",
            "Validation Loss: 616489298.6325169\n",
            "Training Loss: 518529954.8069531\n",
            "Validation Loss: 616598249.3934782\n",
            "Training Loss: 518432242.2373216\n",
            "Validation Loss: 616707234.1000541\n",
            "Training Loss: 518334912.2657849\n",
            "Validation Loss: 616816251.7184659\n",
            "Training Loss: 518237963.3811284\n",
            "Validation Loss: 616925301.2207664\n",
            "Training Loss: 518141394.078231\n",
            "Validation Loss: 617034381.5848398\n",
            "Training Loss: 518045202.8580103\n",
            "Validation Loss: 617143491.7943728\n",
            "Training Loss: 517949388.227432\n",
            "Validation Loss: 617252630.838811\n",
            "Training Loss: 517853948.69945645\n",
            "Validation Loss: 617361797.7133538\n",
            "Training Loss: 517758882.7930366\n",
            "Validation Loss: 617470991.4189123\n",
            "Training Loss: 517664189.0330754\n",
            "Validation Loss: 617580210.9620911\n",
            "Training Loss: 517569865.9503986\n",
            "Validation Loss: 617689455.3551538\n",
            "Training Loss: 517475912.08175844\n",
            "Validation Loss: 617798723.6160088\n",
            "Training Loss: 517382325.9697646\n",
            "Validation Loss: 617908014.768171\n",
            "Training Loss: 517289106.1629004\n",
            "Validation Loss: 618017327.8407482\n",
            "Training Loss: 517196251.2154683\n",
            "Validation Loss: 618126661.8683997\n",
            "Training Loss: 517103759.68759036\n",
            "Validation Loss: 618236015.8913254\n",
            "Training Loss: 517011630.1451562\n",
            "Validation Loss: 618345388.9552315\n",
            "Training Loss: 516919861.15982175\n",
            "Validation Loss: 618454780.1113143\n",
            "Training Loss: 516828451.3089831\n",
            "Validation Loss: 618564188.4162135\n",
            "Training Loss: 516737399.175728\n",
            "Validation Loss: 618673612.9320195\n",
            "Training Loss: 516646703.3488523\n",
            "Validation Loss: 618783052.7262201\n",
            "Training Loss: 516556362.422799\n",
            "Validation Loss: 618892506.8716801\n",
            "Training Loss: 516466374.99765736\n",
            "Validation Loss: 619001974.4466381\n",
            "Training Loss: 516376739.67912406\n",
            "Validation Loss: 619111454.5346578\n",
            "Training Loss: 516287455.07849085\n",
            "Validation Loss: 619220946.2246066\n",
            "Training Loss: 516198519.81262094\n",
            "Validation Loss: 619330448.610639\n",
            "Training Loss: 516109932.50391585\n",
            "Validation Loss: 619439960.7921765\n",
            "Training Loss: 516021691.7803083\n",
            "Validation Loss: 619549481.8738649\n",
            "Training Loss: 515933796.27521765\n",
            "Validation Loss: 619659010.9655625\n",
            "Training Loss: 515846244.6275474\n",
            "Validation Loss: 619768547.1823195\n",
            "Training Loss: 515759035.48165464\n",
            "Validation Loss: 619878089.644345\n",
            "Training Loss: 515672167.48731107\n",
            "Validation Loss: 619987637.4769927\n",
            "Training Loss: 515585639.29971915\n",
            "Validation Loss: 620097189.8107258\n",
            "Training Loss: 515499449.57945144\n",
            "Validation Loss: 620206745.781091\n",
            "Training Loss: 515413596.99244326\n",
            "Validation Loss: 620316304.5287215\n",
            "Training Loss: 515328080.20997894\n",
            "Validation Loss: 620425865.199278\n",
            "Training Loss: 515242897.90865284\n",
            "Validation Loss: 620535426.9434557\n",
            "Training Loss: 515158048.77035564\n",
            "Validation Loss: 620644988.9169359\n",
            "Training Loss: 515073531.4822544\n",
            "Validation Loss: 620754550.2803793\n",
            "Training Loss: 514989344.73676825\n",
            "Validation Loss: 620864110.1993991\n",
            "Training Loss: 514905487.23154986\n",
            "Validation Loss: 620973667.8445367\n",
            "Training Loss: 514821957.6694534\n",
            "Validation Loss: 621083222.3912364\n",
            "Training Loss: 514738754.758521\n",
            "Validation Loss: 621192773.0198251\n",
            "Training Loss: 514655877.211965\n",
            "Validation Loss: 621302318.9154994\n",
            "Training Loss: 514573323.7481419\n",
            "Validation Loss: 621411859.2682773\n",
            "Training Loss: 514491093.0905233\n",
            "Validation Loss: 621521393.2730107\n",
            "Training Loss: 514409183.96768963\n",
            "Validation Loss: 621630920.1293275\n",
            "Training Loss: 514327595.11330104\n",
            "Validation Loss: 621740439.041632\n",
            "Training Loss: 514246325.2660763\n",
            "Validation Loss: 621849949.219088\n",
            "Training Loss: 514165373.16977024\n",
            "Validation Loss: 621959449.8755648\n",
            "Training Loss: 514084737.57316154\n",
            "Validation Loss: 622068940.2296549\n",
            "Training Loss: 514004417.23002106\n",
            "Validation Loss: 622178419.5046251\n",
            "Training Loss: 513924410.8990898\n",
            "Validation Loss: 622287886.9284023\n",
            "Training Loss: 513844717.34408474\n",
            "Validation Loss: 622397341.7335583\n",
            "Training Loss: 513765335.33363503\n",
            "Validation Loss: 622506783.1572738\n",
            "Training Loss: 513686263.6413\n",
            "Validation Loss: 622616210.4413352\n",
            "Training Loss: 513607501.04552746\n",
            "Validation Loss: 622725622.8320988\n",
            "Training Loss: 513529046.3296468\n",
            "Validation Loss: 622835019.5804743\n",
            "Training Loss: 513450898.2818312\n",
            "Validation Loss: 622944399.9419004\n",
            "Training Loss: 513373055.69509494\n",
            "Validation Loss: 623053763.1763389\n",
            "Training Loss: 513295517.36727077\n",
            "Validation Loss: 623163108.5482279\n",
            "Training Loss: 513218282.1009804\n",
            "Validation Loss: 623272435.326485\n",
            "Training Loss: 513141348.70361984\n",
            "Validation Loss: 623381742.7844683\n",
            "Training Loss: 513064715.9873482\n",
            "Validation Loss: 623491030.1999687\n",
            "Training Loss: 512988382.7690505\n",
            "Validation Loss: 623600296.8551847\n",
            "Training Loss: 512912347.8703414\n",
            "Validation Loss: 623709542.0367024\n",
            "Training Loss: 512836610.1175181\n",
            "Validation Loss: 623818765.0354667\n",
            "Training Loss: 512761168.34156084\n",
            "Validation Loss: 623927965.1467772\n",
            "Training Loss: 512686021.37812227\n",
            "Validation Loss: 624037141.6702546\n",
            "Training Loss: 512611168.0674706\n",
            "Validation Loss: 624146293.9098234\n",
            "Training Loss: 512536607.254512\n",
            "Validation Loss: 624255421.1736956\n",
            "Training Loss: 512462337.78875643\n",
            "Validation Loss: 624364522.7743495\n",
            "Training Loss: 512388358.5242767\n",
            "Validation Loss: 624473598.0285114\n",
            "Training Loss: 512314668.3197441\n",
            "Validation Loss: 624582646.2571218\n",
            "Training Loss: 512241266.0383294\n",
            "Validation Loss: 624691666.7853353\n",
            "Training Loss: 512168150.5477719\n",
            "Validation Loss: 624800658.942499\n",
            "Training Loss: 512095320.7202948\n",
            "Validation Loss: 624909622.0621119\n",
            "Training Loss: 512022775.4326196\n",
            "Validation Loss: 625018555.4818294\n",
            "Training Loss: 511950513.5659384\n",
            "Validation Loss: 625127458.5434306\n",
            "Training Loss: 511878534.0059006\n",
            "Validation Loss: 625236330.5928067\n",
            "Training Loss: 511806835.64258283\n",
            "Validation Loss: 625345170.979936\n",
            "Training Loss: 511735417.3704853\n",
            "Validation Loss: 625453979.0588614\n",
            "Training Loss: 511664278.0884996\n",
            "Validation Loss: 625562754.187687\n",
            "Training Loss: 511593416.6999014\n",
            "Validation Loss: 625671495.7285436\n",
            "Training Loss: 511522832.112339\n",
            "Validation Loss: 625780203.047568\n",
            "Training Loss: 511452523.23779047\n",
            "Validation Loss: 625888875.5149043\n",
            "Training Loss: 511382488.99256593\n",
            "Validation Loss: 625997512.5046595\n",
            "Training Loss: 511312728.2973015\n",
            "Validation Loss: 626106113.3949051\n",
            "Training Loss: 511243240.0768927\n",
            "Validation Loss: 626214677.5676512\n",
            "Training Loss: 511174023.2605417\n",
            "Validation Loss: 626323204.4088254\n",
            "Training Loss: 511105076.78169006\n",
            "Validation Loss: 626431693.308251\n",
            "Training Loss: 511036399.57802594\n",
            "Validation Loss: 626540143.6596428\n",
            "Training Loss: 510967990.5914585\n",
            "Validation Loss: 626648554.8605742\n",
            "Training Loss: 510899848.76810056\n",
            "Validation Loss: 626756926.3124613\n",
            "Training Loss: 510831973.0582595\n",
            "Validation Loss: 626865257.4205655\n",
            "Training Loss: 510764362.4164045\n",
            "Validation Loss: 626973547.5939401\n",
            "Training Loss: 510697015.80116504\n",
            "Validation Loss: 627081796.2454407\n",
            "Training Loss: 510629932.1753119\n",
            "Validation Loss: 627190002.7916938\n",
            "Training Loss: 510563110.50572586\n",
            "Validation Loss: 627298166.6530819\n",
            "Training Loss: 510496549.7634045\n",
            "Validation Loss: 627406287.253727\n",
            "Training Loss: 510430248.92341685\n",
            "Validation Loss: 627514364.021475\n",
            "Training Loss: 510364206.96491873\n",
            "Validation Loss: 627622396.3878752\n",
            "Training Loss: 510298422.87110925\n",
            "Validation Loss: 627730383.7881563\n",
            "Training Loss: 510232895.6292294\n",
            "Validation Loss: 627838325.6612262\n",
            "Training Loss: 510167624.2305408\n",
            "Validation Loss: 627946221.4496384\n",
            "Training Loss: 510102607.67030495\n",
            "Validation Loss: 628054070.5995799\n",
            "Training Loss: 510037844.9477834\n",
            "Validation Loss: 628161872.5608563\n",
            "Training Loss: 509973335.0661996\n",
            "Validation Loss: 628269626.7868718\n",
            "Training Loss: 509909077.03273433\n",
            "Validation Loss: 628377332.7346191\n",
            "Training Loss: 509845069.85851336\n",
            "Validation Loss: 628484989.8646516\n",
            "Training Loss: 509781312.5585863\n",
            "Validation Loss: 628592597.6410677\n",
            "Training Loss: 509717804.1519063\n",
            "Validation Loss: 628700155.5315086\n",
            "Training Loss: 509654543.66132396\n",
            "Validation Loss: 628807663.0071259\n",
            "Training Loss: 509591530.1135581\n",
            "Validation Loss: 628915119.5425649\n",
            "Training Loss: 509528762.53921014\n",
            "Validation Loss: 629022524.6159642\n",
            "Training Loss: 509466239.9726974\n",
            "Validation Loss: 629129877.7089165\n",
            "Training Loss: 509403961.45228046\n",
            "Validation Loss: 629237178.3064733\n",
            "Training Loss: 509341926.02004623\n",
            "Validation Loss: 629344425.8971115\n",
            "Training Loss: 509280132.7218583\n",
            "Validation Loss: 629451619.97273\n",
            "Training Loss: 509218580.60738075\n",
            "Validation Loss: 629558760.0286227\n",
            "Training Loss: 509157268.7300367\n",
            "Validation Loss: 629665845.5634764\n",
            "Training Loss: 509096196.1470059\n",
            "Validation Loss: 629772876.079338\n",
            "Training Loss: 509035361.9192072\n",
            "Validation Loss: 629879851.081607\n",
            "Training Loss: 508974765.11127913\n",
            "Validation Loss: 629986770.0790223\n",
            "Training Loss: 508914404.79156595\n",
            "Validation Loss: 630093632.5836403\n",
            "Training Loss: 508854280.0321051\n",
            "Validation Loss: 630200438.1108234\n",
            "Training Loss: 508794389.9086176\n",
            "Validation Loss: 630307186.1792245\n",
            "Training Loss: 508734733.5004839\n",
            "Validation Loss: 630413876.3107618\n",
            "Training Loss: 508675309.89072764\n",
            "Validation Loss: 630520508.0306225\n",
            "Training Loss: 508616118.1660092\n",
            "Validation Loss: 630627080.8672174\n",
            "Training Loss: 508557157.4166147\n",
            "Validation Loss: 630733594.3522115\n",
            "Training Loss: 508498426.7364198\n",
            "Validation Loss: 630840048.020451\n",
            "Training Loss: 508439925.22289824\n",
            "Validation Loss: 630946441.4099935\n",
            "Training Loss: 508381651.9771023\n",
            "Validation Loss: 631052774.0620773\n",
            "Training Loss: 508323606.1036382\n",
            "Validation Loss: 631159045.5210997\n",
            "Training Loss: 508265786.71065414\n",
            "Validation Loss: 631265255.3346077\n",
            "Training Loss: 508208192.90984213\n",
            "Validation Loss: 631371403.0532874\n",
            "Training Loss: 508150823.8164014\n",
            "Validation Loss: 631477488.2309451\n",
            "Training Loss: 508093678.54904586\n",
            "Validation Loss: 631583510.4244902\n",
            "Training Loss: 508036756.2299626\n",
            "Validation Loss: 631689469.1939136\n",
            "Training Loss: 507980055.98482513\n",
            "Validation Loss: 631795364.1022933\n",
            "Training Loss: 507923576.9427638\n",
            "Validation Loss: 631901194.7157648\n",
            "Training Loss: 507867318.23636734\n",
            "Validation Loss: 632006960.6035018\n",
            "Training Loss: 507811279.00163865\n",
            "Validation Loss: 632112661.3377172\n",
            "Training Loss: 507755458.3780066\n",
            "Validation Loss: 632218296.4936372\n",
            "Training Loss: 507699855.5083224\n",
            "Validation Loss: 632323865.6494915\n",
            "Training Loss: 507644469.53880286\n",
            "Validation Loss: 632429368.3864915\n",
            "Training Loss: 507589299.619064\n",
            "Validation Loss: 632534804.2888258\n",
            "Training Loss: 507534344.9020728\n",
            "Validation Loss: 632640172.9436442\n",
            "Training Loss: 507479604.54415405\n",
            "Validation Loss: 632745473.9410282\n",
            "Training Loss: 507425077.70497334\n",
            "Validation Loss: 632850706.8740115\n",
            "Training Loss: 507370763.54750866\n",
            "Validation Loss: 632955871.3385231\n",
            "Training Loss: 507316661.2380633\n",
            "Validation Loss: 633060966.9334056\n",
            "Training Loss: 507262769.9462251\n",
            "Validation Loss: 633165993.2603811\n",
            "Training Loss: 507209088.84487844\n",
            "Validation Loss: 633270949.9240566\n",
            "Training Loss: 507155617.1101656\n",
            "Validation Loss: 633375836.5318875\n",
            "Training Loss: 507102353.9215012\n",
            "Validation Loss: 633480652.694183\n",
            "Training Loss: 507049298.4615292\n",
            "Validation Loss: 633585398.0240803\n",
            "Training Loss: 506996449.9161398\n",
            "Validation Loss: 633690072.1375414\n",
            "Training Loss: 506943807.4744329\n",
            "Validation Loss: 633794674.6533221\n",
            "Training Loss: 506891370.3287126\n",
            "Validation Loss: 633899205.1929855\n",
            "Training Loss: 506839137.67448205\n",
            "Validation Loss: 634003663.3808588\n",
            "Training Loss: 506787108.71042854\n",
            "Validation Loss: 634108048.8440367\n",
            "Training Loss: 506735282.63838464\n",
            "Validation Loss: 634212361.2123694\n",
            "Training Loss: 506683658.66337305\n",
            "Validation Loss: 634316600.1184449\n",
            "Training Loss: 506632235.99352556\n",
            "Validation Loss: 634420765.197566\n",
            "Training Loss: 506581013.8401215\n",
            "Validation Loss: 634524856.0877569\n",
            "Training Loss: 506529991.41754454\n",
            "Validation Loss: 634628872.4297372\n",
            "Training Loss: 506479167.94329625\n",
            "Validation Loss: 634732813.8669056\n",
            "Training Loss: 506428542.6379604\n",
            "Validation Loss: 634836680.0453397\n",
            "Training Loss: 506378114.7252075\n",
            "Validation Loss: 634940470.6137698\n",
            "Training Loss: 506327883.43176514\n",
            "Validation Loss: 635044185.2235814\n",
            "Training Loss: 506277847.9874268\n",
            "Validation Loss: 635147823.5287784\n",
            "Training Loss: 506228007.6250126\n",
            "Validation Loss: 635251385.1859941\n",
            "Training Loss: 506178361.5803954\n",
            "Validation Loss: 635354869.8544681\n",
            "Training Loss: 506128909.0924443\n",
            "Validation Loss: 635458277.1960306\n",
            "Training Loss: 506079649.40304524\n",
            "Validation Loss: 635561606.8750994\n",
            "Training Loss: 506030581.7570798\n",
            "Validation Loss: 635664858.5586542\n",
            "Training Loss: 505981705.4024046\n",
            "Validation Loss: 635768031.9162396\n",
            "Training Loss: 505933019.58985525\n",
            "Validation Loss: 635871126.6199318\n",
            "Training Loss: 505884523.5732083\n",
            "Validation Loss: 635974142.3443539\n",
            "Training Loss: 505836216.60921025\n",
            "Validation Loss: 636077078.7666324\n",
            "Training Loss: 505788097.9575219\n",
            "Validation Loss: 636179935.5664124\n",
            "Training Loss: 505740166.8807398\n",
            "Validation Loss: 636282712.4258246\n",
            "Training Loss: 505692422.64436024\n",
            "Validation Loss: 636385409.0294906\n",
            "Training Loss: 505644864.5167827\n",
            "Validation Loss: 636488025.064495\n",
            "Training Loss: 505597491.76930755\n",
            "Validation Loss: 636590560.2203819\n",
            "Training Loss: 505550303.67609227\n",
            "Validation Loss: 636693014.1891403\n",
            "Training Loss: 505503299.5141647\n",
            "Validation Loss: 636795386.6651969\n",
            "Training Loss: 505456478.56341195\n",
            "Validation Loss: 636897677.3453953\n",
            "Training Loss: 505409840.10655314\n",
            "Validation Loss: 636999885.9289898\n",
            "Training Loss: 505363383.42914546\n",
            "Validation Loss: 637102012.117631\n",
            "Training Loss: 505317107.81956166\n",
            "Validation Loss: 637204055.6153615\n",
            "Training Loss: 505271012.5689862\n",
            "Validation Loss: 637306016.128588\n",
            "Training Loss: 505225096.9713887\n",
            "Validation Loss: 637407893.3660978\n",
            "Training Loss: 505179360.32353866\n",
            "Validation Loss: 637509687.039\n",
            "Training Loss: 505133801.92497104\n",
            "Validation Loss: 637611396.8607717\n",
            "Training Loss: 505088421.07798404\n",
            "Validation Loss: 637713022.5471984\n",
            "Training Loss: 505043217.0876287\n",
            "Validation Loss: 637814563.8163897\n",
            "Training Loss: 504998189.2617011\n",
            "Validation Loss: 637916020.3887609\n",
            "Training Loss: 504953336.9107237\n",
            "Validation Loss: 638017391.9870136\n",
            "Training Loss: 504908659.3479308\n",
            "Validation Loss: 638118678.3361441\n",
            "Training Loss: 504864155.8892783\n",
            "Validation Loss: 638219879.1633971\n",
            "Training Loss: 504819825.8534147\n",
            "Validation Loss: 638320994.1982993\n",
            "Training Loss: 504775668.5616743\n",
            "Validation Loss: 638422023.1726096\n",
            "Training Loss: 504731683.3380582\n",
            "Validation Loss: 638522965.8203301\n",
            "Training Loss: 504687869.50924546\n",
            "Validation Loss: 638623821.8776908\n",
            "Training Loss: 504644226.40456814\n",
            "Validation Loss: 638724591.0831302\n",
            "Training Loss: 504600753.3559952\n",
            "Validation Loss: 638825273.1772913\n",
            "Training Loss: 504557449.6981389\n",
            "Validation Loss: 638925867.9030179\n",
            "Training Loss: 504514314.7682186\n",
            "Validation Loss: 639026375.0053161\n",
            "Training Loss: 504471347.906082\n",
            "Validation Loss: 639126794.2313864\n",
            "Training Loss: 504428548.4541722\n",
            "Validation Loss: 639227125.3305714\n",
            "Training Loss: 504385915.75752324\n",
            "Validation Loss: 639327368.0543683\n",
            "Training Loss: 504343449.1637508\n",
            "Validation Loss: 639427522.156415\n",
            "Training Loss: 504301148.02303743\n",
            "Validation Loss: 639527587.3924727\n",
            "Training Loss: 504259011.6881394\n",
            "Validation Loss: 639627563.5204252\n",
            "Training Loss: 504217039.51434183\n",
            "Validation Loss: 639727450.3002499\n",
            "Training Loss: 504175230.8594965\n",
            "Validation Loss: 639827247.4940382\n",
            "Training Loss: 504133585.08396435\n",
            "Validation Loss: 639926954.8659477\n",
            "Training Loss: 504092101.5506283\n",
            "Validation Loss: 640026572.1822286\n",
            "Training Loss: 504050779.6248955\n",
            "Validation Loss: 640126099.2111788\n",
            "Training Loss: 504009618.6746539\n",
            "Validation Loss: 640225535.7231591\n",
            "Training Loss: 503968618.07030034\n",
            "Validation Loss: 640324881.4905752\n",
            "Training Loss: 503927777.1846935\n",
            "Validation Loss: 640424136.2878621\n",
            "Training Loss: 503887095.39317703\n",
            "Validation Loss: 640523299.891473\n",
            "Training Loss: 503846572.07354546\n",
            "Validation Loss: 640622372.079886\n",
            "Training Loss: 503806206.60604817\n",
            "Validation Loss: 640721352.6335726\n",
            "Training Loss: 503765998.373372\n",
            "Validation Loss: 640820241.3349977\n",
            "Training Loss: 503725946.7606338\n",
            "Validation Loss: 640919037.9686141\n",
            "Training Loss: 503686051.15538186\n",
            "Validation Loss: 641017742.32084\n",
            "Training Loss: 503646310.94756544\n",
            "Validation Loss: 641116354.1800557\n",
            "Training Loss: 503606725.52953565\n",
            "Validation Loss: 641214873.336601\n",
            "Training Loss: 503567294.29603875\n",
            "Validation Loss: 641313299.5827553\n",
            "Training Loss: 503528016.6442087\n",
            "Validation Loss: 641411632.7127273\n",
            "Training Loss: 503488891.9735487\n",
            "Validation Loss: 641509872.5226527\n",
            "Training Loss: 503449919.68591917\n",
            "Validation Loss: 641608018.810577\n",
            "Training Loss: 503411099.1855433\n",
            "Validation Loss: 641706071.3764462\n",
            "Training Loss: 503372429.8789881\n",
            "Validation Loss: 641804030.022116\n",
            "Training Loss: 503333911.17515856\n",
            "Validation Loss: 641901894.5513048\n",
            "Training Loss: 503295542.48527884\n",
            "Validation Loss: 641999664.7696207\n",
            "Training Loss: 503257323.22289836\n",
            "Validation Loss: 642097340.4845275\n",
            "Training Loss: 503219252.8038632\n",
            "Validation Loss: 642194921.5053471\n",
            "Training Loss: 503181330.6463326\n",
            "Validation Loss: 642292407.6432557\n",
            "Training Loss: 503143556.1707439\n",
            "Validation Loss: 642389798.7112511\n",
            "Training Loss: 503105928.7998255\n",
            "Validation Loss: 642487094.5241718\n",
            "Training Loss: 503068447.95856494\n",
            "Validation Loss: 642584294.898666\n",
            "Training Loss: 503031113.074218\n",
            "Validation Loss: 642681399.6531913\n",
            "Training Loss: 502993923.57629657\n",
            "Validation Loss: 642778408.6080068\n",
            "Training Loss: 502956878.8965579\n",
            "Validation Loss: 642875321.5851609\n",
            "Training Loss: 502919978.4689839\n",
            "Validation Loss: 642972138.4084814\n",
            "Training Loss: 502883221.7297947\n",
            "Validation Loss: 643068858.90357\n",
            "Training Loss: 502846608.11742365\n",
            "Validation Loss: 643165482.8977922\n",
            "Training Loss: 502810137.0725092\n",
            "Validation Loss: 643262010.2202622\n",
            "Training Loss: 502773808.0378993\n",
            "Validation Loss: 643358440.7018416\n",
            "Training Loss: 502737620.4586241\n",
            "Validation Loss: 643454774.175129\n",
            "Training Loss: 502701573.78189915\n",
            "Validation Loss: 643551010.474444\n",
            "Training Loss: 502665667.4571211\n",
            "Validation Loss: 643647149.4358326\n",
            "Training Loss: 502629900.935834\n",
            "Validation Loss: 643743190.8970417\n",
            "Training Loss: 502594273.6717627\n",
            "Validation Loss: 643839134.6975204\n",
            "Training Loss: 502558785.1207607\n",
            "Validation Loss: 643934980.6784133\n",
            "Training Loss: 502523434.740827\n",
            "Validation Loss: 644030728.6825386\n",
            "Training Loss: 502488221.99209404\n",
            "Validation Loss: 644126378.5544032\n",
            "Training Loss: 502453146.33681715\n",
            "Validation Loss: 644221930.1401604\n",
            "Training Loss: 502418207.23936564\n",
            "Validation Loss: 644317383.2876316\n",
            "Training Loss: 502383404.1662049\n",
            "Validation Loss: 644412737.8462869\n",
            "Training Loss: 502348736.58591586\n",
            "Validation Loss: 644507993.6672304\n",
            "Training Loss: 502314203.96914923\n",
            "Validation Loss: 644603150.603198\n",
            "Training Loss: 502279805.78865254\n",
            "Validation Loss: 644698208.5085489\n",
            "Training Loss: 502245541.5192372\n",
            "Validation Loss: 644793167.23926\n",
            "Training Loss: 502211410.6377786\n",
            "Validation Loss: 644888026.6529096\n",
            "Training Loss: 502177412.6232054\n",
            "Validation Loss: 644982786.6086701\n",
            "Training Loss: 502143546.95651096\n",
            "Validation Loss: 645077446.9673065\n",
            "Training Loss: 502109813.12070715\n",
            "Validation Loss: 645172007.5911641\n",
            "Training Loss: 502076210.6008526\n",
            "Validation Loss: 645266468.344164\n",
            "Training Loss: 502042738.8840121\n",
            "Validation Loss: 645360829.0917836\n",
            "Training Loss: 502009397.45929086\n",
            "Validation Loss: 645455089.7010612\n",
            "Training Loss: 501976185.81777966\n",
            "Validation Loss: 645549250.0405859\n",
            "Training Loss: 501943103.45258236\n",
            "Validation Loss: 645643309.9804782\n",
            "Training Loss: 501910149.8587857\n",
            "Validation Loss: 645737269.3923925\n",
            "Training Loss: 501877324.53346246\n",
            "Validation Loss: 645831128.1495115\n",
            "Training Loss: 501844626.97566617\n",
            "Validation Loss: 645924886.1265295\n",
            "Training Loss: 501812056.68641466\n",
            "Validation Loss: 646018543.1996493\n",
            "Training Loss: 501779613.1686825\n",
            "Validation Loss: 646112099.2465742\n",
            "Training Loss: 501747295.92739975\n",
            "Validation Loss: 646205554.1464934\n",
            "Training Loss: 501715104.46944815\n",
            "Validation Loss: 646298907.7800925\n",
            "Training Loss: 501683038.3036333\n",
            "Validation Loss: 646392160.029517\n",
            "Training Loss: 501651096.9406987\n",
            "Validation Loss: 646485310.778399\n",
            "Training Loss: 501619279.8933075\n",
            "Validation Loss: 646578359.911806\n",
            "Training Loss: 501587586.6760335\n",
            "Validation Loss: 646671307.316285\n",
            "Training Loss: 501556016.8053649\n",
            "Validation Loss: 646764152.8798132\n",
            "Training Loss: 501524569.7996796\n",
            "Validation Loss: 646856896.4918098\n",
            "Training Loss: 501493245.1792521\n",
            "Validation Loss: 646949538.0431153\n",
            "Training Loss: 501462042.46623987\n",
            "Validation Loss: 647042077.4260036\n",
            "Training Loss: 501430961.18467444\n",
            "Validation Loss: 647134514.5341623\n",
            "Training Loss: 501400000.8604594\n",
            "Validation Loss: 647226849.2626767\n",
            "Training Loss: 501369161.0213577\n",
            "Validation Loss: 647319081.5080379\n",
            "Training Loss: 501338441.19699025\n",
            "Validation Loss: 647411211.1681346\n",
            "Training Loss: 501307840.9188194\n",
            "Validation Loss: 647503238.1422288\n",
            "Training Loss: 501277359.7201405\n",
            "Validation Loss: 647595162.3309699\n",
            "Training Loss: 501246997.1361059\n",
            "Validation Loss: 647686983.6363688\n",
            "Training Loss: 501216752.70366174\n",
            "Validation Loss: 647778701.9618046\n",
            "Training Loss: 501186625.96159744\n",
            "Validation Loss: 647870317.21201\n",
            "Training Loss: 501156616.45049435\n",
            "Validation Loss: 647961829.2930717\n",
            "Training Loss: 501126723.7127442\n",
            "Validation Loss: 648053238.1124034\n",
            "Training Loss: 501096947.29254216\n",
            "Validation Loss: 648144543.5787679\n",
            "Training Loss: 501067286.73585856\n",
            "Validation Loss: 648235745.6022475\n",
            "Training Loss: 501037741.59045255\n",
            "Validation Loss: 648326844.0942398\n",
            "Training Loss: 501008311.4058597\n",
            "Validation Loss: 648417838.9674643\n",
            "Training Loss: 500978995.73338145\n",
            "Validation Loss: 648508730.1359365\n",
            "Training Loss: 500949794.1260855\n",
            "Validation Loss: 648599517.5149764\n",
            "Training Loss: 500920706.13877696\n",
            "Validation Loss: 648690201.0211941\n",
            "Training Loss: 500891731.3280255\n",
            "Validation Loss: 648780780.572484\n",
            "Training Loss: 500862869.25213265\n",
            "Validation Loss: 648871256.0880163\n",
            "Training Loss: 500834119.471138\n",
            "Validation Loss: 648961627.4882282\n",
            "Training Loss: 500805481.5468009\n",
            "Validation Loss: 649051894.6948322\n",
            "Training Loss: 500776955.04259497\n",
            "Validation Loss: 649142057.6307847\n",
            "Training Loss: 500748539.52372956\n",
            "Validation Loss: 649232116.2203029\n",
            "Training Loss: 500720234.5570926\n",
            "Validation Loss: 649322070.3888353\n",
            "Training Loss: 500692039.71128386\n",
            "Validation Loss: 649411920.0630779\n",
            "Training Loss: 500663954.55660295\n",
            "Validation Loss: 649501665.170951\n",
            "Training Loss: 500635978.6650148\n",
            "Validation Loss: 649591305.6415985\n",
            "Training Loss: 500608111.6101838\n",
            "Validation Loss: 649680841.4053801\n",
            "Training Loss: 500580352.96743315\n",
            "Validation Loss: 649770272.3938683\n",
            "Training Loss: 500552702.31376487\n",
            "Validation Loss: 649859598.5398324\n",
            "Training Loss: 500525159.2278187\n",
            "Validation Loss: 649948819.7772472\n",
            "Training Loss: 500497723.28990704\n",
            "Validation Loss: 650037936.0412687\n",
            "Training Loss: 500470394.08197945\n",
            "Validation Loss: 650126947.2682389\n",
            "Training Loss: 500443171.1876243\n",
            "Validation Loss: 650215853.3956834\n",
            "Training Loss: 500416054.1920705\n",
            "Validation Loss: 650304654.3622928\n",
            "Training Loss: 500389042.6821604\n",
            "Validation Loss: 650393350.107916\n",
            "Training Loss: 500362136.24636716\n",
            "Validation Loss: 650481940.5735704\n",
            "Training Loss: 500335334.4747813\n",
            "Validation Loss: 650570425.7014185\n",
            "Training Loss: 500308636.95908165\n",
            "Validation Loss: 650658805.434769\n",
            "Training Loss: 500282043.29256433\n",
            "Validation Loss: 650747079.7180732\n",
            "Training Loss: 500255553.0701199\n",
            "Validation Loss: 650835248.4969081\n",
            "Training Loss: 500229165.8882174\n",
            "Validation Loss: 650923311.717976\n",
            "Training Loss: 500202881.34491545\n",
            "Validation Loss: 651011269.329113\n",
            "Training Loss: 500176699.0398465\n",
            "Validation Loss: 651099121.2792559\n",
            "Training Loss: 500150618.5742118\n",
            "Validation Loss: 651186867.5184463\n",
            "Training Loss: 500124639.5507782\n",
            "Validation Loss: 651274507.9978412\n",
            "Training Loss: 500098761.57385916\n",
            "Validation Loss: 651362042.6696817\n",
            "Training Loss: 500072984.2493407\n",
            "Validation Loss: 651449471.4873012\n",
            "Training Loss: 500047307.1846344\n",
            "Validation Loss: 651536794.4051167\n",
            "Training Loss: 500021729.9886936\n",
            "Validation Loss: 651624011.3786215\n",
            "Training Loss: 499996252.27201337\n",
            "Validation Loss: 651711122.3643843\n",
            "Training Loss: 499970873.64660466\n",
            "Validation Loss: 651798127.3200316\n",
            "Training Loss: 499945593.72601265\n",
            "Validation Loss: 651885026.2042556\n",
            "Training Loss: 499920412.12527084\n",
            "Validation Loss: 651971818.9768012\n",
            "Training Loss: 499895328.4609546\n",
            "Validation Loss: 652058505.5984472\n",
            "Training Loss: 499870342.35112065\n",
            "Validation Loss: 652145086.0310429\n",
            "Training Loss: 499845453.4153203\n",
            "Validation Loss: 652231560.2374452\n",
            "Training Loss: 499820661.274623\n",
            "Validation Loss: 652317928.1815544\n",
            "Training Loss: 499795965.55154127\n",
            "Validation Loss: 652404189.828289\n",
            "Training Loss: 499771365.87009627\n",
            "Validation Loss: 652490345.1435943\n",
            "Training Loss: 499746861.85577667\n",
            "Validation Loss: 652576394.0944195\n",
            "Training Loss: 499722453.135535\n",
            "Validation Loss: 652662336.6487256\n",
            "Training Loss: 499698139.33778346\n",
            "Validation Loss: 652748172.7754699\n",
            "Training Loss: 499673920.0923949\n",
            "Validation Loss: 652833902.4446125\n",
            "Training Loss: 499649795.0306914\n",
            "Validation Loss: 652919525.6271003\n",
            "Training Loss: 499625763.7854371\n",
            "Validation Loss: 653005042.2948593\n",
            "Training Loss: 499601825.990832\n",
            "Validation Loss: 653090452.420802\n",
            "Training Loss: 499577981.2825168\n",
            "Validation Loss: 653175755.9788086\n",
            "Training Loss: 499554229.2975587\n",
            "Validation Loss: 653260952.9437323\n",
            "Training Loss: 499530569.6744339\n",
            "Validation Loss: 653346043.2913841\n",
            "Training Loss: 499507002.0530499\n",
            "Validation Loss: 653431026.9985322\n",
            "Training Loss: 499483526.0747191\n",
            "Validation Loss: 653515904.0428944\n",
            "Training Loss: 499460141.3821574\n",
            "Validation Loss: 653600674.4031384\n",
            "Training Loss: 499436847.61947125\n",
            "Validation Loss: 653685338.0588732\n",
            "Training Loss: 499413644.4321834\n",
            "Validation Loss: 653769894.9906349\n",
            "Training Loss: 499390531.46718544\n",
            "Validation Loss: 653854345.1798977\n",
            "Training Loss: 499367508.3727564\n",
            "Validation Loss: 653938688.6090539\n",
            "Training Loss: 499344574.7985578\n",
            "Validation Loss: 654022925.2614204\n",
            "Training Loss: 499321730.3956198\n",
            "Validation Loss: 654107055.1212249\n",
            "Training Loss: 499298974.8163358\n",
            "Validation Loss: 654191078.173599\n",
            "Training Loss: 499276307.7144607\n",
            "Validation Loss: 654274994.4045895\n",
            "Training Loss: 499253728.7451157\n",
            "Validation Loss: 654358803.8011253\n",
            "Training Loss: 499231237.5647564\n",
            "Validation Loss: 654442506.3510455\n",
            "Training Loss: 499208833.83119285\n",
            "Validation Loss: 654526102.0430593\n",
            "Training Loss: 499186517.20356977\n",
            "Validation Loss: 654609590.8667738\n",
            "Training Loss: 499164287.3423798\n",
            "Validation Loss: 654692972.812663\n",
            "Training Loss: 499142143.909426\n",
            "Validation Loss: 654776247.8720784\n",
            "Training Loss: 499120086.56784534\n",
            "Validation Loss: 654859416.037237\n",
            "Training Loss: 499098114.9820915\n",
            "Validation Loss: 654942477.3012139\n",
            "Training Loss: 499076228.8179382\n",
            "Validation Loss: 655025431.6579531\n",
            "Training Loss: 499054427.74245393\n",
            "Validation Loss: 655108279.1022382\n",
            "Training Loss: 499032711.4240206\n",
            "Validation Loss: 655191019.6297094\n",
            "Training Loss: 499011079.53231347\n",
            "Validation Loss: 655273653.2368376\n",
            "Training Loss: 498989531.73830557\n",
            "Validation Loss: 655356179.9209459\n",
            "Training Loss: 498968067.71426123\n",
            "Validation Loss: 655438599.6801765\n",
            "Training Loss: 498946687.133704\n",
            "Validation Loss: 655520912.5135068\n",
            "Training Loss: 498925389.67146415\n",
            "Validation Loss: 655603118.4207352\n",
            "Training Loss: 498904175.00362873\n",
            "Validation Loss: 655685217.4024754\n",
            "Training Loss: 498883042.80756205\n",
            "Validation Loss: 655767209.4601579\n",
            "Training Loss: 498861992.7618791\n",
            "Validation Loss: 655849094.5960196\n",
            "Training Loss: 498841024.546458\n",
            "Validation Loss: 655930872.813099\n",
            "Training Loss: 498820137.84243065\n",
            "Validation Loss: 656012544.1152365\n",
            "Training Loss: 498799332.33217955\n",
            "Validation Loss: 656094108.5070621\n",
            "Training Loss: 498778607.69932353\n",
            "Validation Loss: 656175565.9940003\n",
            "Training Loss: 498757963.62872815\n",
            "Validation Loss: 656256916.5822564\n",
            "Training Loss: 498737399.8064798\n",
            "Validation Loss: 656338160.2788182\n",
            "Training Loss: 498716915.9198984\n",
            "Validation Loss: 656419297.0914464\n",
            "Training Loss: 498696511.65753007\n",
            "Validation Loss: 656500327.0286719\n",
            "Training Loss: 498676186.70914495\n",
            "Validation Loss: 656581250.0997916\n",
            "Training Loss: 498655940.7657155\n",
            "Validation Loss: 656662066.3148686\n",
            "Training Loss: 498635773.5194195\n",
            "Validation Loss: 656742775.6847175\n",
            "Training Loss: 498615684.66365594\n",
            "Validation Loss: 656823378.2209026\n",
            "Training Loss: 498595673.89301366\n",
            "Validation Loss: 656903873.9357463\n",
            "Training Loss: 498575740.9032725\n",
            "Validation Loss: 656984262.8423073\n",
            "Training Loss: 498555885.3914024\n",
            "Validation Loss: 657064544.9543797\n",
            "Training Loss: 498536107.05557114\n",
            "Validation Loss: 657144720.2865006\n",
            "Training Loss: 498516405.5951205\n",
            "Validation Loss: 657224788.8539311\n",
            "Training Loss: 498496780.71055627\n",
            "Validation Loss: 657304750.6726605\n",
            "Training Loss: 498477232.10357064\n",
            "Validation Loss: 657384605.7593946\n",
            "Training Loss: 498457759.47701997\n",
            "Validation Loss: 657464354.1315618\n",
            "Training Loss: 498438362.53491825\n",
            "Validation Loss: 657543995.8072981\n",
            "Training Loss: 498419040.9824375\n",
            "Validation Loss: 657623530.8054566\n",
            "Training Loss: 498399794.52590656\n",
            "Validation Loss: 657702959.1455786\n",
            "Training Loss: 498380622.8727997\n",
            "Validation Loss: 657782280.8479192\n",
            "Training Loss: 498361525.73173845\n",
            "Validation Loss: 657861495.9334197\n",
            "Training Loss: 498342502.8124734\n",
            "Validation Loss: 657940604.4237202\n",
            "Training Loss: 498323553.82590663\n",
            "Validation Loss: 658019606.3411402\n",
            "Training Loss: 498304678.48406315\n",
            "Validation Loss: 658098501.7086856\n",
            "Training Loss: 498285876.5000826\n",
            "Validation Loss: 658177290.5500377\n",
            "Training Loss: 498267147.5882502\n",
            "Validation Loss: 658255972.8895543\n",
            "Training Loss: 498248491.46394247\n",
            "Validation Loss: 658334548.7522635\n",
            "Training Loss: 498229907.84366924\n",
            "Validation Loss: 658413018.163861\n",
            "Training Loss: 498211396.4450383\n",
            "Validation Loss: 658491381.1506956\n",
            "Training Loss: 498192956.98676366\n",
            "Validation Loss: 658569637.7397832\n",
            "Training Loss: 498174589.1886588\n",
            "Validation Loss: 658647787.9587855\n",
            "Training Loss: 498156292.77163124\n",
            "Validation Loss: 658725831.8360236\n",
            "Training Loss: 498138067.4576893\n",
            "Validation Loss: 658803769.4004582\n",
            "Training Loss: 498119912.969909\n",
            "Validation Loss: 658881600.6816869\n",
            "Training Loss: 498101829.0324662\n",
            "Validation Loss: 658959325.7099522\n",
            "Training Loss: 498083815.3706064\n",
            "Validation Loss: 659036944.5161234\n",
            "Training Loss: 498065871.71065545\n",
            "Validation Loss: 659114457.1317087\n",
            "Training Loss: 498047997.78000116\n",
            "Validation Loss: 659191863.5888306\n",
            "Training Loss: 498030193.3071014\n",
            "Validation Loss: 659269163.9202446\n",
            "Training Loss: 498012458.02147466\n",
            "Validation Loss: 659346358.1593091\n",
            "Training Loss: 497994791.6536982\n",
            "Validation Loss: 659423446.3400118\n",
            "Training Loss: 497977193.93540096\n",
            "Validation Loss: 659500428.4969444\n",
            "Training Loss: 497959664.59926087\n",
            "Validation Loss: 659577304.6653014\n",
            "Training Loss: 497942203.378997\n",
            "Validation Loss: 659654074.8808825\n",
            "Training Loss: 497924810.00937575\n",
            "Validation Loss: 659730739.1800842\n",
            "Training Loss: 497907484.2261974\n",
            "Validation Loss: 659807297.5998985\n",
            "Training Loss: 497890225.7662888\n",
            "Validation Loss: 659883750.1779112\n",
            "Training Loss: 497873034.3675168\n",
            "Validation Loss: 659960096.9522909\n",
            "Training Loss: 497855909.76876295\n",
            "Validation Loss: 660036337.9617901\n",
            "Training Loss: 497838851.70993507\n",
            "Validation Loss: 660112473.2457383\n",
            "Training Loss: 497821859.93195087\n",
            "Validation Loss: 660188502.8440521\n",
            "Training Loss: 497804934.17674804\n",
            "Validation Loss: 660264426.7972108\n",
            "Training Loss: 497788074.187266\n",
            "Validation Loss: 660340245.1462623\n",
            "Training Loss: 497771279.70746225\n",
            "Validation Loss: 660415957.9328158\n",
            "Training Loss: 497754550.48227304\n",
            "Validation Loss: 660491565.1990557\n",
            "Training Loss: 497737886.2576445\n",
            "Validation Loss: 660567066.9877096\n",
            "Training Loss: 497721286.7805095\n",
            "Validation Loss: 660642463.3420697\n",
            "Training Loss: 497704751.7987997\n",
            "Validation Loss: 660717754.3059639\n",
            "Training Loss: 497688281.06142426\n",
            "Validation Loss: 660792939.9237853\n",
            "Training Loss: 497671874.3182682\n",
            "Validation Loss: 660868020.2404623\n",
            "Training Loss: 497655531.3202065\n",
            "Validation Loss: 660942995.3014531\n",
            "Training Loss: 497639251.81907845\n",
            "Validation Loss: 661017865.1527686\n",
            "Training Loss: 497623035.5676925\n",
            "Validation Loss: 661092629.840942\n",
            "Training Loss: 497606882.3198305\n",
            "Validation Loss: 661167289.4130427\n",
            "Training Loss: 497590791.8302206\n",
            "Validation Loss: 661241843.9166566\n",
            "Training Loss: 497574763.8545691\n",
            "Validation Loss: 661316293.399895\n",
            "Training Loss: 497558798.14952415\n",
            "Validation Loss: 661390637.9113971\n",
            "Training Loss: 497542894.472682\n",
            "Validation Loss: 661464877.5003079\n",
            "Training Loss: 497527052.58259505\n",
            "Validation Loss: 661539012.2162826\n",
            "Training Loss: 497511272.2387536\n",
            "Validation Loss: 661613042.1094962\n",
            "Training Loss: 497495553.2015811\n",
            "Validation Loss: 661686967.230614\n",
            "Training Loss: 497479895.23245764\n",
            "Validation Loss: 661760787.6308188\n",
            "Training Loss: 497464298.09366417\n",
            "Validation Loss: 661834503.3617799\n",
            "Training Loss: 497448761.548443\n",
            "Validation Loss: 661908114.4756646\n",
            "Training Loss: 497433285.36093396\n",
            "Validation Loss: 661981621.0251328\n",
            "Training Loss: 497417869.2962148\n",
            "Validation Loss: 662055023.0633385\n",
            "Training Loss: 497402513.1202695\n",
            "Validation Loss: 662128320.6439148\n",
            "Training Loss: 497387216.60000706\n",
            "Validation Loss: 662201513.8209766\n",
            "Training Loss: 497371979.50323963\n",
            "Validation Loss: 662274602.6491199\n",
            "Training Loss: 497356801.5986854\n",
            "Validation Loss: 662347587.1834127\n",
            "Training Loss: 497341682.65596825\n",
            "Validation Loss: 662420467.4794065\n",
            "Training Loss: 497326622.4456147\n",
            "Validation Loss: 662493243.5931014\n",
            "Training Loss: 497311620.73903847\n",
            "Validation Loss: 662565915.5809852\n",
            "Training Loss: 497296677.3085512\n",
            "Validation Loss: 662638483.4999923\n",
            "Training Loss: 497281791.927358\n",
            "Validation Loss: 662710947.4075285\n",
            "Training Loss: 497266964.3695411\n",
            "Validation Loss: 662783307.361449\n",
            "Training Loss: 497252194.41006964\n",
            "Validation Loss: 662855563.4200636\n",
            "Training Loss: 497237481.8247905\n",
            "Validation Loss: 662927715.6421295\n",
            "Training Loss: 497222826.3904233\n",
            "Validation Loss: 662999764.0868576\n",
            "Training Loss: 497208227.8845658\n",
            "Validation Loss: 663071708.8139012\n",
            "Training Loss: 497193686.0856743\n",
            "Validation Loss: 663143549.8833505\n",
            "Training Loss: 497179200.7730777\n",
            "Validation Loss: 663215287.3557376\n",
            "Training Loss: 497164771.7269696\n",
            "Validation Loss: 663286921.2920225\n",
            "Training Loss: 497150398.7283876\n",
            "Validation Loss: 663358451.7536076\n",
            "Training Loss: 497136081.5592353\n",
            "Validation Loss: 663429878.8023193\n",
            "Training Loss: 497121820.00226825\n",
            "Validation Loss: 663501202.5004067\n",
            "Training Loss: 497107613.84108144\n",
            "Validation Loss: 663572422.910548\n",
            "Training Loss: 497093462.8601217\n",
            "Validation Loss: 663643540.0958335\n",
            "Training Loss: 497079366.84467834\n",
            "Validation Loss: 663714554.1197759\n",
            "Training Loss: 497065325.5808713\n",
            "Validation Loss: 663785465.0463022\n",
            "Training Loss: 497051338.85566205\n",
            "Validation Loss: 663856272.9397503\n",
            "Training Loss: 497037406.45684105\n",
            "Validation Loss: 663926977.8648616\n",
            "Training Loss: 497023528.1730287\n",
            "Validation Loss: 663997579.8867896\n",
            "Training Loss: 497009703.7936638\n",
            "Validation Loss: 664068079.0710826\n",
            "Training Loss: 496995933.10901767\n",
            "Validation Loss: 664138475.4836966\n",
            "Training Loss: 496982215.91017467\n",
            "Validation Loss: 664208769.190978\n",
            "Training Loss: 496968551.98902625\n",
            "Validation Loss: 664278960.2596725\n",
            "Training Loss: 496954941.1382932\n",
            "Validation Loss: 664349048.7569102\n",
            "Training Loss: 496941383.1514964\n",
            "Validation Loss: 664419034.7502104\n",
            "Training Loss: 496927877.82295614\n",
            "Validation Loss: 664488918.3074877\n",
            "Training Loss: 496914424.9478045\n",
            "Validation Loss: 664558699.4970227\n",
            "Training Loss: 496901024.321967\n",
            "Validation Loss: 664628378.3874947\n",
            "Training Loss: 496887675.7421744\n",
            "Validation Loss: 664697955.0479422\n",
            "Training Loss: 496874379.0059408\n",
            "Validation Loss: 664767429.5477945\n",
            "Training Loss: 496861133.9115733\n",
            "Validation Loss: 664836801.9568341\n",
            "Training Loss: 496847940.2581629\n",
            "Validation Loss: 664906072.3452288\n",
            "Training Loss: 496834797.84559953\n",
            "Validation Loss: 664975240.783507\n",
            "Training Loss: 496821706.4745381\n",
            "Validation Loss: 665044307.3425545\n",
            "Training Loss: 496808665.94641614\n",
            "Validation Loss: 665113272.0936255\n",
            "Training Loss: 496795676.0634397\n",
            "Validation Loss: 665182135.1083344\n",
            "Training Loss: 496782736.62860537\n",
            "Validation Loss: 665250896.4586415\n",
            "Training Loss: 496769847.44565344\n",
            "Validation Loss: 665319556.2168659\n",
            "Training Loss: 496757008.3191069\n",
            "Validation Loss: 665388114.4556804\n",
            "Training Loss: 496744219.0542477\n",
            "Validation Loss: 665456571.2480932\n",
            "Training Loss: 496731479.45711535\n",
            "Validation Loss: 665524926.6674732\n",
            "Training Loss: 496718789.3345108\n",
            "Validation Loss: 665593180.7875175\n",
            "Training Loss: 496706148.4939803\n",
            "Validation Loss: 665661333.6822774\n",
            "Training Loss: 496693556.7438224\n",
            "Validation Loss: 665729385.426126\n",
            "Training Loss: 496681013.8930923\n",
            "Validation Loss: 665797336.0937822\n",
            "Training Loss: 496668519.751586\n",
            "Validation Loss: 665865185.7602905\n",
            "Training Loss: 496656074.12983626\n",
            "Validation Loss: 665932934.5010309\n",
            "Training Loss: 496643676.8391123\n",
            "Validation Loss: 666000582.3917097\n",
            "Training Loss: 496631327.6914363\n",
            "Validation Loss: 666068129.508354\n",
            "Training Loss: 496619026.4995467\n",
            "Validation Loss: 666135575.9273132\n",
            "Training Loss: 496606773.07692176\n",
            "Validation Loss: 666202921.7252576\n",
            "Training Loss: 496594567.23775786\n",
            "Validation Loss: 666270166.9791801\n",
            "Training Loss: 496582408.79699165\n",
            "Validation Loss: 666337311.7663764\n",
            "Training Loss: 496570297.57026833\n",
            "Validation Loss: 666404356.1644672\n",
            "Training Loss: 496558233.3739516\n",
            "Validation Loss: 666471300.251374\n",
            "Training Loss: 496546216.02513206\n",
            "Validation Loss: 666538144.1053336\n",
            "Training Loss: 496534245.3416037\n",
            "Validation Loss: 666604887.8048764\n",
            "Training Loss: 496522321.1418736\n",
            "Validation Loss: 666671531.4288484\n",
            "Training Loss: 496510443.2451592\n",
            "Validation Loss: 666738075.0563898\n",
            "Training Loss: 496498611.47138643\n",
            "Validation Loss: 666804518.7669344\n",
            "Training Loss: 496486825.64116776\n",
            "Validation Loss: 666870862.6402204\n",
            "Training Loss: 496475085.57583493\n",
            "Validation Loss: 666937106.7562736\n",
            "Training Loss: 496463391.09740597\n",
            "Validation Loss: 667003251.1954074\n",
            "Training Loss: 496451742.02859384\n",
            "Validation Loss: 667069296.0382366\n",
            "Training Loss: 496440138.1928002\n",
            "Validation Loss: 667135241.3656484\n",
            "Training Loss: 496428579.41412467\n",
            "Validation Loss: 667201087.2588171\n",
            "Training Loss: 496417065.5173448\n",
            "Validation Loss: 667266833.7992092\n",
            "Training Loss: 496405596.32791865\n",
            "Validation Loss: 667332481.0685537\n",
            "Training Loss: 496394171.6719929\n",
            "Validation Loss: 667398029.1488692\n",
            "Training Loss: 496382791.3763917\n",
            "Validation Loss: 667463478.1224475\n",
            "Training Loss: 496371455.26860476\n",
            "Validation Loss: 667528828.0718465\n",
            "Training Loss: 496360163.1768037\n",
            "Validation Loss: 667594079.0799011\n",
            "Training Loss: 496348914.92982763\n",
            "Validation Loss: 667659231.2297099\n",
            "Training Loss: 496337710.3571856\n",
            "Validation Loss: 667724284.6046414\n",
            "Training Loss: 496326549.2890483\n",
            "Validation Loss: 667789239.2883252\n",
            "Training Loss: 496315431.55624497\n",
            "Validation Loss: 667854095.3646522\n",
            "Training Loss: 496304356.9902657\n",
            "Validation Loss: 667918852.9177784\n",
            "Training Loss: 496293325.423268\n",
            "Validation Loss: 667983512.0321048\n",
            "Training Loss: 496282336.6880516\n",
            "Validation Loss: 668048072.7922994\n",
            "Training Loss: 496271390.6180681\n",
            "Validation Loss: 668112535.2832787\n",
            "Training Loss: 496260487.0474296\n",
            "Validation Loss: 668176899.5902071\n",
            "Training Loss: 496249625.8108874\n",
            "Validation Loss: 668241165.7985009\n",
            "Training Loss: 496238806.743831\n",
            "Validation Loss: 668305333.993827\n",
            "Training Loss: 496228029.68229896\n",
            "Validation Loss: 668369404.2620848\n",
            "Training Loss: 496217294.46297044\n",
            "Validation Loss: 668433376.6894288\n",
            "Training Loss: 496206600.9231575\n",
            "Validation Loss: 668497251.3622478\n",
            "Training Loss: 496195948.9008104\n",
            "Validation Loss: 668561028.3671635\n",
            "Training Loss: 496185338.2345046\n",
            "Validation Loss: 668624707.7910458\n",
            "Training Loss: 496174768.7634508\n",
            "Validation Loss: 668688289.7209907\n",
            "Training Loss: 496164240.32748145\n",
            "Validation Loss: 668751774.2443293\n",
            "Training Loss: 496153752.7670501\n",
            "Validation Loss: 668815161.448616\n",
            "Training Loss: 496143305.9232511\n",
            "Validation Loss: 668878451.4216458\n",
            "Training Loss: 496132899.6377753\n",
            "Validation Loss: 668941644.2514251\n",
            "Training Loss: 496122533.7529428\n",
            "Validation Loss: 669004740.0261943\n",
            "Training Loss: 496112208.11168396\n",
            "Validation Loss: 669067738.8344115\n",
            "Training Loss: 496101922.5575447\n",
            "Validation Loss: 669130640.7647601\n",
            "Training Loss: 496091676.93467855\n",
            "Validation Loss: 669193445.9061327\n",
            "Training Loss: 496081471.0878447\n",
            "Validation Loss: 669256154.3476455\n",
            "Training Loss: 496071304.8624044\n",
            "Validation Loss: 669318766.1786274\n",
            "Training Loss: 496061178.10433626\n",
            "Validation Loss: 669381281.4886137\n",
            "Training Loss: 496051090.6602101\n",
            "Validation Loss: 669443700.367359\n",
            "Training Loss: 496041042.37718314\n",
            "Validation Loss: 669506022.9048228\n",
            "Training Loss: 496031033.10302204\n",
            "Validation Loss: 669568249.191162\n",
            "Training Loss: 496021062.68608385\n",
            "Validation Loss: 669630379.3167518\n",
            "Training Loss: 496011130.97531337\n",
            "Validation Loss: 669692413.372164\n",
            "Training Loss: 496001237.8202491\n",
            "Validation Loss: 669754351.4481715\n",
            "Training Loss: 495991383.0710065\n",
            "Validation Loss: 669816193.6357416\n",
            "Training Loss: 495981566.57829934\n",
            "Validation Loss: 669877940.026044\n",
            "Training Loss: 495971788.19340545\n",
            "Validation Loss: 669939590.7104471\n",
            "Training Loss: 495962047.76819825\n",
            "Validation Loss: 670001145.7805047\n",
            "Training Loss: 495952345.155119\n",
            "Validation Loss: 670062605.3279632\n",
            "Training Loss: 495942680.2071846\n",
            "Validation Loss: 670123969.4447607\n",
            "Training Loss: 495933052.777994\n",
            "Validation Loss: 670185238.2230276\n",
            "Training Loss: 495923462.72169733\n",
            "Validation Loss: 670246411.7550721\n",
            "Training Loss: 495913909.89303464\n",
            "Validation Loss: 670307490.1333905\n",
            "Training Loss: 495904394.1472973\n",
            "Validation Loss: 670368473.4506629\n",
            "Training Loss: 495894915.340347\n",
            "Validation Loss: 670429361.7997439\n",
            "Training Loss: 495885473.3286049\n",
            "Validation Loss: 670490155.2736754\n",
            "Training Loss: 495876067.9690514\n",
            "Validation Loss: 670550853.9656731\n",
            "Training Loss: 495866699.1192188\n",
            "Validation Loss: 670611457.9691236\n",
            "Training Loss: 495857366.63721466\n",
            "Validation Loss: 670671967.3775982\n",
            "Training Loss: 495848070.38167065\n",
            "Validation Loss: 670732382.2848238\n",
            "Training Loss: 495838810.211787\n",
            "Validation Loss: 670792702.784712\n",
            "Training Loss: 495829585.98730844\n",
            "Validation Loss: 670852928.9713352\n",
            "Training Loss: 495820397.56853133\n",
            "Validation Loss: 670913060.9389412\n",
            "Training Loss: 495811244.81628454\n",
            "Validation Loss: 670973098.7819251\n",
            "Training Loss: 495802127.59194946\n",
            "Validation Loss: 671033042.5948673\n",
            "Training Loss: 495793045.7574429\n",
            "Validation Loss: 671092892.4724895\n",
            "Training Loss: 495783999.1752207\n",
            "Validation Loss: 671152648.50969\n",
            "Training Loss: 495774987.70826733\n",
            "Validation Loss: 671212310.8015143\n",
            "Training Loss: 495766011.2201187\n",
            "Validation Loss: 671271879.4431735\n",
            "Training Loss: 495757069.5748214\n",
            "Validation Loss: 671331354.5300269\n",
            "Training Loss: 495748162.63697314\n",
            "Validation Loss: 671390736.1575869\n",
            "Training Loss: 495739290.2716737\n",
            "Validation Loss: 671450024.4215233\n",
            "Training Loss: 495730452.3445701\n",
            "Validation Loss: 671509219.4176503\n",
            "Training Loss: 495721648.7218256\n",
            "Validation Loss: 671568321.241934\n",
            "Training Loss: 495712879.2701183\n",
            "Validation Loss: 671627329.9904878\n",
            "Training Loss: 495704143.85665035\n",
            "Validation Loss: 671686245.7595705\n",
            "Training Loss: 495695442.34914714\n",
            "Validation Loss: 671745068.6455759\n",
            "Training Loss: 495686774.61584336\n",
            "Validation Loss: 671803798.7450525\n",
            "Training Loss: 495678140.52547675\n",
            "Validation Loss: 671862436.1546896\n",
            "Training Loss: 495669539.9473205\n",
            "Validation Loss: 671920980.9712968\n",
            "Training Loss: 495660972.75112957\n",
            "Validation Loss: 671979433.2918464\n",
            "Training Loss: 495652438.80719554\n",
            "Validation Loss: 672037793.2134317\n",
            "Training Loss: 495643937.98628503\n",
            "Validation Loss: 672096060.8332797\n",
            "Training Loss: 495635470.1596849\n",
            "Validation Loss: 672154236.2487582\n",
            "Training Loss: 495627035.1991901\n",
            "Validation Loss: 672212319.557363\n",
            "Training Loss: 495618632.97706836\n",
            "Validation Loss: 672270310.8567137\n",
            "Training Loss: 495610263.36611974\n",
            "Validation Loss: 672328210.2445656\n",
            "Training Loss: 495601926.2396081\n",
            "Validation Loss: 672386017.8187976\n",
            "Training Loss: 495593621.47131157\n",
            "Validation Loss: 672443733.6774198\n",
            "Training Loss: 495585348.9354885\n",
            "Validation Loss: 672501357.9185567\n",
            "Training Loss: 495577108.5068936\n",
            "Validation Loss: 672558890.6404574\n",
            "Training Loss: 495568900.0607611\n",
            "Validation Loss: 672616331.9414976\n",
            "Training Loss: 495560723.47282124\n",
            "Validation Loss: 672673681.9201719\n",
            "Training Loss: 495552578.6192829\n",
            "Validation Loss: 672730940.6750821\n",
            "Training Loss: 495544465.3768393\n",
            "Validation Loss: 672788108.30496\n",
            "Training Loss: 495536383.6226522\n",
            "Validation Loss: 672845184.9086442\n",
            "Training Loss: 495528333.23438674\n",
            "Validation Loss: 672902170.5850893\n",
            "Training Loss: 495520314.0901509\n",
            "Validation Loss: 672959065.433366\n",
            "Training Loss: 495512326.06856346\n",
            "Validation Loss: 673015869.5526493\n",
            "Training Loss: 495504369.0486782\n",
            "Validation Loss: 673072583.042227\n",
            "Training Loss: 495496442.91005784\n",
            "Validation Loss: 673129206.0014951\n",
            "Training Loss: 495488547.5327007\n",
            "Validation Loss: 673185738.5299542\n",
            "Training Loss: 495480682.7970966\n",
            "Validation Loss: 673242180.7272108\n",
            "Training Loss: 495472848.584181\n",
            "Validation Loss: 673298532.6929774\n",
            "Training Loss: 495465044.77537644\n",
            "Validation Loss: 673354794.5270674\n",
            "Training Loss: 495457271.2525395\n",
            "Validation Loss: 673410966.3293973\n",
            "Training Loss: 495449527.89801306\n",
            "Validation Loss: 673467048.1999762\n",
            "Training Loss: 495441814.5945749\n",
            "Validation Loss: 673523040.2389235\n",
            "Training Loss: 495434131.22547513\n",
            "Validation Loss: 673578942.5464436\n",
            "Training Loss: 495426477.674414\n",
            "Validation Loss: 673634755.2228414\n",
            "Training Loss: 495418853.8255422\n",
            "Validation Loss: 673690478.3685256\n",
            "Training Loss: 495411259.5634617\n",
            "Validation Loss: 673746112.083978\n",
            "Training Loss: 495403694.7732217\n",
            "Validation Loss: 673801656.4697949\n",
            "Training Loss: 495396159.3403262\n",
            "Validation Loss: 673857111.6266448\n",
            "Training Loss: 495388653.15071964\n",
            "Validation Loss: 673912477.6552924\n",
            "Training Loss: 495381176.0907936\n",
            "Validation Loss: 673967754.656592\n",
            "Training Loss: 495373728.04737663\n",
            "Validation Loss: 674022942.7314811\n",
            "Training Loss: 495366308.9077423\n",
            "Validation Loss: 674078041.9809875\n",
            "Training Loss: 495358918.5595997\n",
            "Validation Loss: 674133052.5062128\n",
            "Training Loss: 495351556.89109755\n",
            "Validation Loss: 674187974.4083538\n",
            "Training Loss: 495344223.7908209\n",
            "Validation Loss: 674242807.788683\n",
            "Training Loss: 495336919.1477826\n",
            "Validation Loss: 674297552.748554\n",
            "Training Loss: 495329642.85143304\n",
            "Validation Loss: 674352209.3893965\n",
            "Training Loss: 495322394.79165584\n",
            "Validation Loss: 674406777.812717\n",
            "Training Loss: 495315174.8587543\n",
            "Validation Loss: 674461258.1201123\n",
            "Training Loss: 495307982.94346297\n",
            "Validation Loss: 674515650.4132315\n",
            "Training Loss: 495300818.9369434\n",
            "Validation Loss: 674569954.7938231\n",
            "Training Loss: 495293682.7307778\n",
            "Validation Loss: 674624171.3636838\n",
            "Training Loss: 495286574.2169721\n",
            "Validation Loss: 674678300.2247046\n",
            "Training Loss: 495279493.28795445\n",
            "Validation Loss: 674732341.4788321\n",
            "Training Loss: 495272439.8365654\n",
            "Validation Loss: 674786295.2280843\n",
            "Training Loss: 495265413.7560687\n",
            "Validation Loss: 674840161.5745531\n",
            "Training Loss: 495258414.94014\n",
            "Validation Loss: 674893940.6203933\n",
            "Training Loss: 495251443.2828737\n",
            "Validation Loss: 674947632.4678245\n",
            "Training Loss: 495244498.6787661\n",
            "Validation Loss: 675001237.2191341\n",
            "Training Loss: 495237581.0227332\n",
            "Validation Loss: 675054754.9766699\n",
            "Training Loss: 495230690.21009773\n",
            "Validation Loss: 675108185.842846\n",
            "Training Loss: 495223826.13659304\n",
            "Validation Loss: 675161529.9201329\n",
            "Training Loss: 495216988.69834894\n",
            "Validation Loss: 675214787.311063\n",
            "Training Loss: 495210177.79190624\n",
            "Validation Loss: 675267958.1182321\n",
            "Training Loss: 495203393.31420994\n",
            "Validation Loss: 675321042.4442852\n",
            "Training Loss: 495196635.16260076\n",
            "Validation Loss: 675374040.3919283\n",
            "Training Loss: 495189903.23482066\n",
            "Validation Loss: 675426952.0639279\n",
            "Training Loss: 495183197.42901313\n",
            "Validation Loss: 675479777.5630926\n",
            "Training Loss: 495176517.6437122\n",
            "Validation Loss: 675532516.9922956\n",
            "Training Loss: 495169863.7778498\n",
            "Validation Loss: 675585170.4544607\n",
            "Training Loss: 495163235.7307521\n",
            "Validation Loss: 675637738.0525582\n",
            "Training Loss: 495156633.40213364\n",
            "Validation Loss: 675690219.8896106\n",
            "Training Loss: 495150056.69210356\n",
            "Validation Loss: 675742616.0686886\n",
            "Training Loss: 495143505.50115746\n",
            "Validation Loss: 675794926.6929157\n",
            "Training Loss: 495136979.73017615\n",
            "Validation Loss: 675847151.8654536\n",
            "Training Loss: 495130479.2804328\n",
            "Validation Loss: 675899291.6895107\n",
            "Training Loss: 495124004.0535718\n",
            "Validation Loss: 675951346.2683563\n",
            "Training Loss: 495117553.9516352\n",
            "Validation Loss: 676003315.7052813\n",
            "Training Loss: 495111128.87703574\n",
            "Validation Loss: 676055200.1036311\n",
            "Training Loss: 495104728.7325682\n",
            "Validation Loss: 676106999.5667889\n",
            "Training Loss: 495098353.42140824\n",
            "Validation Loss: 676158714.1981821\n",
            "Training Loss: 495092002.84710896\n",
            "Validation Loss: 676210344.1012763\n",
            "Training Loss: 495085676.91359466\n",
            "Validation Loss: 676261889.3795719\n",
            "Training Loss: 495079375.5251657\n",
            "Validation Loss: 676313350.1366059\n",
            "Training Loss: 495073098.5864918\n",
            "Validation Loss: 676364726.475965\n",
            "Training Loss: 495066846.0026148\n",
            "Validation Loss: 676416018.5012549\n",
            "Training Loss: 495060617.6789576\n",
            "Validation Loss: 676467226.3161244\n",
            "Training Loss: 495054413.5212856\n",
            "Validation Loss: 676518350.0242503\n",
            "Training Loss: 495048233.4357506\n",
            "Validation Loss: 676569389.7293532\n",
            "Training Loss: 495042077.32887596\n",
            "Validation Loss: 676620345.5351709\n",
            "Training Loss: 495035945.1075228\n",
            "Validation Loss: 676671217.54548\n",
            "Training Loss: 495029836.6789316\n",
            "Validation Loss: 676722005.8640865\n",
            "Training Loss: 495023751.9507063\n",
            "Validation Loss: 676772710.5948251\n",
            "Training Loss: 495017690.83080465\n",
            "Validation Loss: 676823331.8415538\n",
            "Training Loss: 495011653.22754043\n",
            "Validation Loss: 676873869.7081566\n",
            "Training Loss: 495005639.04958314\n",
            "Validation Loss: 676924324.2985528\n",
            "Training Loss: 494999648.20596766\n",
            "Validation Loss: 676974695.7166781\n",
            "Training Loss: 494993680.60607076\n",
            "Validation Loss: 677024984.0664942\n",
            "Training Loss: 494987736.159631\n",
            "Validation Loss: 677075189.4519864\n",
            "Training Loss: 494981814.7767343\n",
            "Validation Loss: 677125311.9771605\n",
            "Training Loss: 494975916.3678101\n",
            "Validation Loss: 677175351.7460436\n",
            "Training Loss: 494970040.8436475\n",
            "Validation Loss: 677225308.862686\n",
            "Training Loss: 494964188.1153728\n",
            "Validation Loss: 677275183.4311546\n",
            "Training Loss: 494958358.0944651\n",
            "Validation Loss: 677324975.5555311\n",
            "Training Loss: 494952550.69274706\n",
            "Validation Loss: 677374685.3399233\n",
            "Training Loss: 494946765.8223724\n",
            "Validation Loss: 677424312.8884499\n",
            "Training Loss: 494941003.3958675\n",
            "Validation Loss: 677473858.3052436\n",
            "Training Loss: 494935263.3260559\n",
            "Validation Loss: 677523321.6944543\n",
            "Training Loss: 494929545.52613276\n",
            "Validation Loss: 677572703.1602484\n",
            "Training Loss: 494923849.90961766\n",
            "Validation Loss: 677622002.8067992\n",
            "Training Loss: 494918176.39036775\n",
            "Validation Loss: 677671220.7383007\n",
            "Training Loss: 494912524.88258266\n",
            "Validation Loss: 677720357.0589515\n",
            "Training Loss: 494906895.3007826\n",
            "Validation Loss: 677769411.8729556\n",
            "Training Loss: 494901287.5598319\n",
            "Validation Loss: 677818385.2845438\n",
            "Training Loss: 494895701.5749165\n",
            "Validation Loss: 677867277.3979381\n",
            "Training Loss: 494890137.2615615\n",
            "Validation Loss: 677916088.3173782\n",
            "Training Loss: 494884594.53561383\n",
            "Validation Loss: 677964818.1471083\n",
            "Training Loss: 494879073.3132531\n",
            "Validation Loss: 678013466.9913768\n",
            "Training Loss: 494873573.51097816\n",
            "Validation Loss: 678062034.9544363\n",
            "Training Loss: 494868095.04561615\n",
            "Validation Loss: 678110522.1405483\n",
            "Training Loss: 494862637.83432186\n",
            "Validation Loss: 678158928.6539822\n",
            "Training Loss: 494857201.7945637\n",
            "Validation Loss: 678207254.5989966\n",
            "Training Loss: 494851786.8441338\n",
            "Validation Loss: 678255500.0798647\n",
            "Training Loss: 494846392.9011501\n",
            "Validation Loss: 678303665.200855\n",
            "Training Loss: 494841019.8840428\n",
            "Validation Loss: 678351750.0662384\n",
            "Training Loss: 494835667.7115594\n",
            "Validation Loss: 678399754.7802858\n",
            "Training Loss: 494830336.30276495\n",
            "Validation Loss: 678447679.4472659\n",
            "Training Loss: 494825025.5770415\n",
            "Validation Loss: 678495524.1714411\n",
            "Training Loss: 494819735.45408195\n",
            "Validation Loss: 678543289.0570807\n",
            "Training Loss: 494814465.8538917\n",
            "Validation Loss: 678590974.2084432\n",
            "Training Loss: 494809216.6967804\n",
            "Validation Loss: 678638579.7297859\n",
            "Training Loss: 494803987.9033828\n",
            "Validation Loss: 678686105.7253622\n",
            "Training Loss: 494798779.39463013\n",
            "Validation Loss: 678733552.2994173\n",
            "Training Loss: 494793591.0917678\n",
            "Validation Loss: 678780919.5561904\n",
            "Training Loss: 494788422.91633946\n",
            "Validation Loss: 678828207.5999116\n",
            "Training Loss: 494783274.79020554\n",
            "Validation Loss: 678875416.5348045\n",
            "Training Loss: 494778146.6355143\n",
            "Validation Loss: 678922546.4650862\n",
            "Training Loss: 494773038.37473625\n",
            "Validation Loss: 678969597.4949641\n",
            "Training Loss: 494767949.93062586\n",
            "Validation Loss: 679016569.7286335\n",
            "Training Loss: 494762881.2262443\n",
            "Validation Loss: 679063463.2702715\n",
            "Training Loss: 494757832.1849606\n",
            "Validation Loss: 679110278.2240586\n",
            "Training Loss: 494752802.7304231\n",
            "Validation Loss: 679157014.694155\n",
            "Training Loss: 494747792.78659654\n",
            "Validation Loss: 679203672.7847023\n",
            "Training Loss: 494742802.277724\n",
            "Validation Loss: 679250252.5998381\n",
            "Training Loss: 494737831.1283603\n",
            "Validation Loss: 679296754.2436811\n",
            "Training Loss: 494732879.2633439\n",
            "Validation Loss: 679343177.8203294\n",
            "Training Loss: 494727946.60779566\n",
            "Validation Loss: 679389523.4338799\n",
            "Training Loss: 494723033.0871566\n",
            "Validation Loss: 679435791.188392\n",
            "Training Loss: 494718138.6271229\n",
            "Validation Loss: 679481981.1879259\n",
            "Training Loss: 494713263.15370166\n",
            "Validation Loss: 679528093.5365168\n",
            "Training Loss: 494708406.5931806\n",
            "Validation Loss: 679574128.3381773\n",
            "Training Loss: 494703568.87213665\n",
            "Validation Loss: 679620085.6969068\n",
            "Training Loss: 494698749.91743577\n",
            "Validation Loss: 679665965.7166812\n",
            "Training Loss: 494693949.65621513\n",
            "Validation Loss: 679711768.5014565\n",
            "Training Loss: 494689168.0159061\n",
            "Validation Loss: 679757494.1551683\n",
            "Training Loss: 494684404.9242251\n",
            "Validation Loss: 679803142.7817252\n",
            "Training Loss: 494679660.3091556\n",
            "Validation Loss: 679848714.485023\n",
            "Training Loss: 494674934.0989713\n",
            "Validation Loss: 679894209.3689172\n",
            "Training Loss: 494670226.2222267\n",
            "Validation Loss: 679939627.5372615\n",
            "Training Loss: 494665536.6077445\n",
            "Validation Loss: 679984969.0938662\n",
            "Training Loss: 494660865.1846364\n",
            "Validation Loss: 680030234.142526\n",
            "Training Loss: 494656211.88228166\n",
            "Validation Loss: 680075422.7870009\n",
            "Training Loss: 494651576.630328\n",
            "Validation Loss: 680120535.1310372\n",
            "Training Loss: 494646959.3587104\n",
            "Validation Loss: 680165571.278342\n",
            "Training Loss: 494642359.9976263\n",
            "Validation Loss: 680210531.3326019\n",
            "Training Loss: 494637778.4775523\n",
            "Validation Loss: 680255415.3974656\n",
            "Training Loss: 494633214.7292276\n",
            "Validation Loss: 680300223.576568\n",
            "Training Loss: 494628668.68366677\n",
            "Validation Loss: 680344955.9734981\n",
            "Training Loss: 494624140.2721441\n",
            "Validation Loss: 680389612.6918262\n",
            "Training Loss: 494619629.42621374\n",
            "Validation Loss: 680434193.8350827\n",
            "Training Loss: 494615136.0776841\n",
            "Validation Loss: 680478699.5067736\n",
            "Training Loss: 494610660.15863216\n",
            "Validation Loss: 680523129.8103667\n",
            "Training Loss: 494606201.6014075\n",
            "Validation Loss: 680567484.8493026\n",
            "Training Loss: 494601760.3386038\n",
            "Validation Loss: 680611764.726982\n",
            "Training Loss: 494597336.30309504\n",
            "Validation Loss: 680655969.5467801\n",
            "Training Loss: 494592929.4280059\n",
            "Validation Loss: 680700099.4120296\n",
            "Training Loss: 494588539.64673346\n",
            "Validation Loss: 680744154.4260358\n",
            "Training Loss: 494584166.89291024\n",
            "Validation Loss: 680788134.6920515\n",
            "Training Loss: 494579811.1004478\n",
            "Validation Loss: 680832040.3133191\n",
            "Training Loss: 494575472.20351076\n",
            "Validation Loss: 680875871.3930199\n",
            "Training Loss: 494571150.1365145\n",
            "Validation Loss: 680919628.0343131\n",
            "Training Loss: 494566844.83412695\n",
            "Validation Loss: 680963310.3403125\n",
            "Training Loss: 494562556.23128116\n",
            "Validation Loss: 681006918.4140912\n",
            "Training Loss: 494558284.2631497\n",
            "Validation Loss: 681050452.3586974\n",
            "Training Loss: 494554028.86516976\n",
            "Validation Loss: 681093912.27712\n",
            "Training Loss: 494549789.97301775\n",
            "Validation Loss: 681137298.2723229\n",
            "Training Loss: 494545567.5226297\n",
            "Validation Loss: 681180610.4472181\n",
            "Training Loss: 494541361.45018035\n",
            "Validation Loss: 681223848.9046861\n",
            "Training Loss: 494537171.6921105\n",
            "Validation Loss: 681267013.7475568\n",
            "Training Loss: 494532998.1850869\n",
            "Validation Loss: 681310105.0786233\n",
            "Training Loss: 494528840.8660314\n",
            "Validation Loss: 681353123.0006351\n",
            "Training Loss: 494524699.67211366\n",
            "Validation Loss: 681396067.6162935\n",
            "Training Loss: 494520574.5407461\n",
            "Validation Loss: 681438939.0282634\n",
            "Training Loss: 494516465.4095859\n",
            "Validation Loss: 681481737.3391566\n",
            "Training Loss: 494512372.2165184\n",
            "Validation Loss: 681524462.6515476\n",
            "Training Loss: 494508294.8996941\n",
            "Validation Loss: 681567115.0679605\n",
            "Training Loss: 494504233.3974915\n",
            "Validation Loss: 681609694.6908733\n",
            "Training Loss: 494500187.6485169\n",
            "Validation Loss: 681652201.6227201\n",
            "Training Loss: 494496157.5916383\n",
            "Validation Loss: 681694635.9658871\n",
            "Training Loss: 494492143.1659498\n",
            "Validation Loss: 681736997.8227112\n",
            "Training Loss: 494488144.31077445\n",
            "Validation Loss: 681779287.2954808\n",
            "Training Loss: 494484160.965683\n",
            "Validation Loss: 681821504.4864383\n",
            "Training Loss: 494480193.07048124\n",
            "Validation Loss: 681863649.4977788\n",
            "Training Loss: 494476240.5652003\n",
            "Validation Loss: 681905722.431637\n",
            "Training Loss: 494472303.39010644\n",
            "Validation Loss: 681947723.3901113\n",
            "Training Loss: 494468381.48570234\n",
            "Validation Loss: 681989652.4752415\n",
            "Training Loss: 494464474.792719\n",
            "Validation Loss: 682031509.7890196\n",
            "Training Loss: 494460583.25212675\n",
            "Validation Loss: 682073295.4333823\n",
            "Training Loss: 494456706.805106\n",
            "Validation Loss: 682115009.5102153\n",
            "Training Loss: 494452845.3930803\n",
            "Validation Loss: 682156652.1213568\n",
            "Training Loss: 494448998.9577043\n",
            "Validation Loss: 682198223.3685852\n",
            "Training Loss: 494445167.44083935\n",
            "Validation Loss: 682239723.353631\n",
            "Training Loss: 494441350.7845932\n",
            "Validation Loss: 682281152.1781654\n",
            "Training Loss: 494437548.9312904\n",
            "Validation Loss: 682322509.9438113\n",
            "Training Loss: 494433761.82347953\n",
            "Validation Loss: 682363796.7521319\n",
            "Training Loss: 494429989.40393287\n",
            "Validation Loss: 682405012.7046381\n",
            "Training Loss: 494426231.6156535\n",
            "Validation Loss: 682446157.9027853\n",
            "Training Loss: 494422488.4018434\n",
            "Validation Loss: 682487232.4479659\n",
            "Training Loss: 494418759.7059474\n",
            "Validation Loss: 682528236.4415239\n",
            "Training Loss: 494415045.47162086\n",
            "Validation Loss: 682569169.9847479\n",
            "Training Loss: 494411345.6427404\n",
            "Validation Loss: 682610033.1788588\n",
            "Training Loss: 494407660.1633972\n",
            "Validation Loss: 682650826.1250272\n",
            "Training Loss: 494403988.97790694\n",
            "Validation Loss: 682691548.9243647\n",
            "Training Loss: 494400332.03079283\n",
            "Validation Loss: 682732201.6779244\n",
            "Training Loss: 494396689.266798\n",
            "Validation Loss: 682772784.4866954\n",
            "Training Loss: 494393060.6308819\n",
            "Validation Loss: 682813297.451613\n",
            "Training Loss: 494389446.0682108\n",
            "Validation Loss: 682853740.6735519\n",
            "Training Loss: 494385845.5241766\n",
            "Validation Loss: 682894114.2533208\n",
            "Training Loss: 494382258.9443736\n",
            "Validation Loss: 682934418.2916713\n",
            "Training Loss: 494378686.2746068\n",
            "Validation Loss: 682974652.8892987\n",
            "Training Loss: 494375127.46089464\n",
            "Validation Loss: 683014818.1468254\n",
            "Training Loss: 494371582.44946915\n",
            "Validation Loss: 683054914.164826\n",
            "Training Loss: 494368051.1867637\n",
            "Validation Loss: 683094941.043799\n",
            "Training Loss: 494364533.6194244\n",
            "Validation Loss: 683134898.8841883\n",
            "Training Loss: 494361029.6943016\n",
            "Validation Loss: 683174787.7863683\n",
            "Training Loss: 494357539.3584624\n",
            "Validation Loss: 683214607.8506548\n",
            "Training Loss: 494354062.55916303\n",
            "Validation Loss: 683254359.1773037\n",
            "Training Loss: 494350599.2438743\n",
            "Validation Loss: 683294041.8664916\n",
            "Training Loss: 494347149.3602744\n",
            "Validation Loss: 683333656.0183502\n",
            "Training Loss: 494343712.8562321\n",
            "Validation Loss: 683373201.732926\n",
            "Training Loss: 494340289.6798385\n",
            "Validation Loss: 683412679.1102123\n",
            "Training Loss: 494336879.77935857\n",
            "Validation Loss: 683452088.2501378\n",
            "Training Loss: 494333483.10328525\n",
            "Validation Loss: 683491429.252557\n",
            "Training Loss: 494330099.60030043\n",
            "Validation Loss: 683530702.2172593\n",
            "Training Loss: 494326729.2192817\n",
            "Validation Loss: 683569907.2439725\n",
            "Training Loss: 494323371.90931\n",
            "Validation Loss: 683609044.432351\n",
            "Training Loss: 494320027.6196656\n",
            "Validation Loss: 683648113.8819863\n",
            "Training Loss: 494316696.2998195\n",
            "Validation Loss: 683687115.6923991\n",
            "Training Loss: 494313377.8994461\n",
            "Validation Loss: 683726049.9630368\n",
            "Training Loss: 494310072.36840624\n",
            "Validation Loss: 683764916.7932878\n",
            "Training Loss: 494306779.6567721\n",
            "Validation Loss: 683803716.282466\n",
            "Training Loss: 494303499.71478784\n",
            "Validation Loss: 683842448.5298144\n",
            "Training Loss: 494300232.4929043\n",
            "Validation Loss: 683881113.6345092\n",
            "Training Loss: 494296977.94177\n",
            "Validation Loss: 683919711.6956545\n",
            "Training Loss: 494293736.01220673\n",
            "Validation Loss: 683958242.812285\n",
            "Training Loss: 494290506.6552464\n",
            "Validation Loss: 683996707.0833626\n",
            "Training Loss: 494287289.8221017\n",
            "Validation Loss: 684035104.6077775\n",
            "Training Loss: 494284085.4641711\n",
            "Validation Loss: 684073435.4843509\n",
            "Training Loss: 494280893.5330488\n",
            "Validation Loss: 684111699.811831\n",
            "Training Loss: 494277713.9805215\n",
            "Validation Loss: 684149897.6888934\n",
            "Training Loss: 494274546.75854814\n",
            "Validation Loss: 684188029.2141423\n",
            "Training Loss: 494271391.81928384\n",
            "Validation Loss: 684226094.4861063\n",
            "Training Loss: 494268249.1150758\n",
            "Validation Loss: 684264093.6032436\n",
            "Training Loss: 494265118.5984409\n",
            "Validation Loss: 684302026.6639401\n",
            "Training Loss: 494262000.22209084\n",
            "Validation Loss: 684339893.7665\n",
            "Training Loss: 494258893.9389222\n",
            "Validation Loss: 684377695.0091602\n",
            "Training Loss: 494255799.70201015\n",
            "Validation Loss: 684415430.490084\n",
            "Training Loss: 494252717.46460867\n",
            "Validation Loss: 684453100.3073505\n",
            "Training Loss: 494249647.1801603\n",
            "Validation Loss: 684490704.5589826\n",
            "Training Loss: 494246588.8022824\n",
            "Validation Loss: 684528243.3429071\n",
            "Training Loss: 494243542.2847861\n",
            "Validation Loss: 684565716.7569823\n",
            "Training Loss: 494240507.5816376\n",
            "Validation Loss: 684603124.8989979\n",
            "Training Loss: 494237484.6470041\n",
            "Validation Loss: 684640467.8666567\n",
            "Training Loss: 494234473.4352253\n",
            "Validation Loss: 684677745.7575881\n",
            "Training Loss: 494231473.9008114\n",
            "Validation Loss: 684714958.66935\n",
            "Training Loss: 494228485.9984532\n",
            "Validation Loss: 684752106.6994166\n",
            "Training Loss: 494225509.6830188\n",
            "Validation Loss: 684789189.9451863\n",
            "Training Loss: 494222544.9095478\n",
            "Validation Loss: 684826208.5039835\n",
            "Training Loss: 494219591.63325995\n",
            "Validation Loss: 684863162.4730469\n",
            "Training Loss: 494216649.80954534\n",
            "Validation Loss: 684900051.9495419\n",
            "Training Loss: 494213719.39397144\n",
            "Validation Loss: 684936877.0305578\n",
            "Training Loss: 494210800.3422678\n",
            "Validation Loss: 684973637.8130952\n",
            "Training Loss: 494207892.6103462\n",
            "Validation Loss: 685010334.3940935\n",
            "Training Loss: 494204996.15428966\n",
            "Validation Loss: 685046966.8703887\n",
            "Training Loss: 494202110.93034625\n",
            "Validation Loss: 685083535.338756\n",
            "Training Loss: 494199236.89493597\n",
            "Validation Loss: 685120039.8958813\n",
            "Training Loss: 494196374.004646\n",
            "Validation Loss: 685156480.6383753\n",
            "Training Loss: 494193522.2162403\n",
            "Validation Loss: 685192857.6627637\n",
            "Training Loss: 494190681.48663485\n",
            "Validation Loss: 685229171.065496\n",
            "Training Loss: 494187851.77293557\n",
            "Validation Loss: 685265420.942936\n",
            "Training Loss: 494185033.03239554\n",
            "Validation Loss: 685301607.3913686\n",
            "Training Loss: 494182225.22244096\n",
            "Validation Loss: 685337730.5069948\n",
            "Training Loss: 494179428.30066\n",
            "Validation Loss: 685373790.3859441\n",
            "Training Loss: 494176642.2248128\n",
            "Validation Loss: 685409787.1242474\n",
            "Training Loss: 494173866.95282197\n",
            "Validation Loss: 685445720.817861\n",
            "Training Loss: 494171102.44276434\n",
            "Validation Loss: 685481591.5626636\n",
            "Training Loss: 494168348.65288717\n",
            "Validation Loss: 685517399.454448\n",
            "Training Loss: 494165605.5416099\n",
            "Validation Loss: 685553144.5889132\n",
            "Training Loss: 494162873.06748664\n",
            "Validation Loss: 685588827.061696\n",
            "Training Loss: 494160151.1892533\n",
            "Validation Loss: 685624446.9683303\n",
            "Training Loss: 494157439.86580384\n",
            "Validation Loss: 685660004.4042745\n",
            "Training Loss: 494154739.0561864\n",
            "Validation Loss: 685695499.4649045\n",
            "Training Loss: 494152048.71961296\n",
            "Validation Loss: 685730932.2455091\n",
            "Training Loss: 494149368.8154489\n",
            "Validation Loss: 685766302.8412939\n",
            "Training Loss: 494146699.3032192\n",
            "Validation Loss: 685801611.3473753\n",
            "Training Loss: 494144040.14260936\n",
            "Validation Loss: 685836857.8587923\n",
            "Training Loss: 494141391.293459\n",
            "Validation Loss: 685872042.4704939\n",
            "Training Loss: 494138752.7157593\n",
            "Validation Loss: 685907165.2773411\n",
            "Training Loss: 494136124.36966497\n",
            "Validation Loss: 685942226.3741215\n",
            "Training Loss: 494133506.21548647\n",
            "Validation Loss: 685977225.8555212\n",
            "Training Loss: 494130898.2136698\n",
            "Validation Loss: 686012163.8161477\n",
            "Training Loss: 494128300.3248401\n",
            "Validation Loss: 686047040.3505272\n",
            "Training Loss: 494125712.509756\n",
            "Validation Loss: 686081855.5530897\n",
            "Training Loss: 494123134.7293444\n",
            "Validation Loss: 686116609.5181822\n",
            "Training Loss: 494120566.94466496\n",
            "Validation Loss: 686151302.3400677\n",
            "Training Loss: 494118009.1169503\n",
            "Validation Loss: 686185934.1129208\n",
            "Training Loss: 494115461.2075577\n",
            "Validation Loss: 686220504.9308251\n",
            "Training Loss: 494112923.17802685\n",
            "Validation Loss: 686255014.8877811\n",
            "Training Loss: 494110394.99001557\n",
            "Validation Loss: 686289464.0777001\n",
            "Training Loss: 494107876.60534847\n",
            "Validation Loss: 686323852.5944054\n",
            "Training Loss: 494105367.985992\n",
            "Validation Loss: 686358180.531632\n",
            "Training Loss: 494102869.0940662\n",
            "Validation Loss: 686392447.9830296\n",
            "Training Loss: 494100379.89182997\n",
            "Validation Loss: 686426655.0421467\n",
            "Training Loss: 494097900.3416953\n",
            "Validation Loss: 686460801.8024658\n",
            "Training Loss: 494095430.4062174\n",
            "Validation Loss: 686494888.3573595\n",
            "Training Loss: 494092970.0480971\n",
            "Validation Loss: 686528914.8001213\n",
            "Training Loss: 494090519.23018014\n",
            "Validation Loss: 686562881.2239558\n",
            "Training Loss: 494088077.9154574\n",
            "Validation Loss: 686596787.7219741\n",
            "Training Loss: 494085646.0670633\n",
            "Validation Loss: 686630634.3871953\n",
            "Training Loss: 494083223.6482673\n",
            "Validation Loss: 686664421.3125623\n",
            "Training Loss: 494080810.6224992\n",
            "Validation Loss: 686698148.590914\n",
            "Training Loss: 494078406.9533143\n",
            "Validation Loss: 686731816.3150004\n",
            "Training Loss: 494076012.60441947\n",
            "Validation Loss: 686765424.5774887\n",
            "Training Loss: 494073627.5396545\n",
            "Validation Loss: 686798973.4709479\n",
            "Training Loss: 494071251.7230042\n",
            "Validation Loss: 686832463.0878634\n",
            "Training Loss: 494068885.1185983\n",
            "Validation Loss: 686865893.5206234\n",
            "Training Loss: 494066527.6906897\n",
            "Validation Loss: 686899264.8615296\n",
            "Training Loss: 494064179.40369046\n",
            "Validation Loss: 686932577.2027904\n",
            "Training Loss: 494061840.22213554\n",
            "Validation Loss: 686965830.6365211\n",
            "Training Loss: 494059510.1107074\n",
            "Validation Loss: 686999025.2547486\n",
            "Training Loss: 494057189.0342112\n",
            "Validation Loss: 687032161.1494048\n",
            "Training Loss: 494054876.9576104\n",
            "Validation Loss: 687065238.4123375\n",
            "Training Loss: 494052573.84598345\n",
            "Validation Loss: 687098257.1352909\n",
            "Training Loss: 494050279.6645635\n",
            "Validation Loss: 687131217.409925\n",
            "Training Loss: 494047994.37870085\n",
            "Validation Loss: 687164119.327805\n",
            "Training Loss: 494045717.9538865\n",
            "Validation Loss: 687196962.980405\n",
            "Training Loss: 494043450.3557579\n",
            "Validation Loss: 687229748.4591038\n",
            "Training Loss: 494041191.5500653\n",
            "Validation Loss: 687262475.8551902\n",
            "Training Loss: 494038941.5027048\n",
            "Validation Loss: 687295145.2598585\n",
            "Training Loss: 494036700.17970324\n",
            "Validation Loss: 687327756.7642101\n",
            "Training Loss: 494034467.547215\n",
            "Validation Loss: 687360310.4592549\n",
            "Training Loss: 494032243.57153106\n",
            "Validation Loss: 687392806.4359034\n",
            "Training Loss: 494030028.2190736\n",
            "Validation Loss: 687425244.7849791\n",
            "Training Loss: 494027821.4563866\n",
            "Validation Loss: 687457625.5972087\n",
            "Training Loss: 494025623.2501571\n",
            "Validation Loss: 687489948.9632322\n",
            "Training Loss: 494023433.5671883\n",
            "Validation Loss: 687522214.9735786\n",
            "Training Loss: 494021252.3744175\n",
            "Validation Loss: 687554423.7187024\n",
            "Training Loss: 494019079.6389186\n",
            "Validation Loss: 687586575.2889498\n",
            "Training Loss: 494016915.32787925\n",
            "Validation Loss: 687618669.774584\n",
            "Training Loss: 494014759.40862453\n",
            "Validation Loss: 687650707.2657616\n",
            "Training Loss: 494012611.84859514\n",
            "Validation Loss: 687682687.8525507\n",
            "Training Loss: 494010472.61538094\n",
            "Validation Loss: 687714611.6249274\n",
            "Training Loss: 494008341.6766704\n",
            "Validation Loss: 687746478.6727688\n",
            "Training Loss: 494006219.000289\n",
            "Validation Loss: 687778289.0858619\n",
            "Training Loss: 494004104.5541977\n",
            "Validation Loss: 687810042.9538857\n",
            "Training Loss: 494001998.3064603\n",
            "Validation Loss: 687841740.3664392\n",
            "Training Loss: 493999900.2252832\n",
            "Validation Loss: 687873381.4130208\n",
            "Training Loss: 493997810.2789865\n",
            "Validation Loss: 687904966.1830294\n",
            "Training Loss: 493995728.4360186\n",
            "Validation Loss: 687936494.765775\n",
            "Training Loss: 493993654.66494626\n",
            "Validation Loss: 687967967.2504606\n",
            "Training Loss: 493991588.934458\n",
            "Validation Loss: 687999383.7262126\n",
            "Training Loss: 493989531.2133677\n",
            "Validation Loss: 688030744.2820367\n",
            "Training Loss: 493987481.4706065\n",
            "Validation Loss: 688062049.0068626\n",
            "Training Loss: 493985439.67523277\n",
            "Validation Loss: 688093297.9895179\n",
            "Training Loss: 493983405.796411\n",
            "Validation Loss: 688124491.3187256\n",
            "Training Loss: 493981379.8034393\n",
            "Validation Loss: 688155629.0831267\n",
            "Training Loss: 493979361.6657308\n",
            "Validation Loss: 688186711.3712547\n",
            "Training Loss: 493977351.3528191\n",
            "Validation Loss: 688217738.2715489\n",
            "Training Loss: 493975348.8343444\n",
            "Validation Loss: 688248709.8723546\n",
            "Training Loss: 493973354.08007824\n",
            "Validation Loss: 688279626.2619171\n",
            "Training Loss: 493971367.05991\n",
            "Validation Loss: 688310487.5283883\n",
            "Training Loss: 493969387.74383855\n",
            "Validation Loss: 688341293.7598193\n",
            "Training Loss: 493967416.10198015\n",
            "Validation Loss: 688372045.0441672\n",
            "Training Loss: 493965452.1045664\n",
            "Validation Loss: 688402741.4692824\n",
            "Training Loss: 493963495.7219525\n",
            "Validation Loss: 688433383.122937\n",
            "Training Loss: 493961546.9245947\n",
            "Validation Loss: 688463970.0927869\n",
            "Training Loss: 493959605.68307906\n",
            "Validation Loss: 688494502.4664013\n",
            "Training Loss: 493957671.96810085\n",
            "Validation Loss: 688524980.3312446\n",
            "Training Loss: 493955745.7504608\n",
            "Validation Loss: 688555403.7746936\n",
            "Training Loss: 493953827.0010721\n",
            "Validation Loss: 688585772.8840108\n",
            "Training Loss: 493951915.69099295\n",
            "Validation Loss: 688616087.7463812\n",
            "Training Loss: 493950011.7913414\n",
            "Validation Loss: 688646348.4488753\n",
            "Training Loss: 493948115.2733913\n",
            "Validation Loss: 688676555.0784713\n",
            "Training Loss: 493946226.1085067\n",
            "Validation Loss: 688706707.7220508\n",
            "Training Loss: 493944344.2681676\n",
            "Validation Loss: 688736806.4663999\n",
            "Training Loss: 493942469.72396666\n",
            "Validation Loss: 688766851.3981946\n",
            "Training Loss: 493940602.4476064\n",
            "Validation Loss: 688796842.6040237\n",
            "Training Loss: 493938742.4108917\n",
            "Validation Loss: 688826780.1703727\n",
            "Training Loss: 493936889.5857509\n",
            "Validation Loss: 688856664.1836318\n",
            "Training Loss: 493935043.94420743\n",
            "Validation Loss: 688886494.7300881\n",
            "Training Loss: 493933205.45840156\n",
            "Validation Loss: 688916271.8959312\n",
            "Training Loss: 493931374.1005787\n",
            "Validation Loss: 688945995.7672576\n",
            "Training Loss: 493929549.8430954\n",
            "Validation Loss: 688975666.4300531\n",
            "Training Loss: 493927732.65840966\n",
            "Validation Loss: 689005283.9702163\n",
            "Training Loss: 493925922.51909345\n",
            "Validation Loss: 689034848.4735417\n",
            "Training Loss: 493924119.3978168\n",
            "Validation Loss: 689064360.0257221\n",
            "Training Loss: 493922323.26736134\n",
            "Validation Loss: 689093818.7123572\n",
            "Training Loss: 493920534.10061353\n",
            "Validation Loss: 689123224.6189393\n",
            "Training Loss: 493918751.8705665\n",
            "Validation Loss: 689152577.8308696\n",
            "Training Loss: 493916976.55031365\n",
            "Validation Loss: 689181878.433448\n",
            "Training Loss: 493915208.11305594\n",
            "Validation Loss: 689211126.511868\n",
            "Training Loss: 493913446.53210187\n",
            "Validation Loss: 689240322.1512325\n",
            "Training Loss: 493911691.7808554\n",
            "Validation Loss: 689269465.4365399\n",
            "Training Loss: 493909943.8328314\n",
            "Validation Loss: 689298556.4526912\n",
            "Training Loss: 493908202.6616399\n",
            "Validation Loss: 689327595.2844833\n",
            "Training Loss: 493906468.24100333\n",
            "Validation Loss: 689356582.0166242\n",
            "Training Loss: 493904740.5447375\n",
            "Validation Loss: 689385516.7337039\n",
            "Training Loss: 493903019.546757\n",
            "Validation Loss: 689414399.5202295\n",
            "Training Loss: 493901305.22109157\n",
            "Validation Loss: 689443230.4606041\n",
            "Training Loss: 493899597.54186124\n",
            "Validation Loss: 689472009.6391234\n",
            "Training Loss: 493897896.4832862\n",
            "Validation Loss: 689500737.1399872\n",
            "Training Loss: 493896202.0196941\n",
            "Validation Loss: 689529413.0473013\n",
            "Training Loss: 493894514.12550014\n",
            "Validation Loss: 689558037.4450608\n",
            "Training Loss: 493892832.775231\n",
            "Validation Loss: 689586610.4171687\n",
            "Training Loss: 493891157.94350636\n",
            "Validation Loss: 689615132.0474212\n",
            "Training Loss: 493889489.60504866\n",
            "Validation Loss: 689643602.4195235\n",
            "Training Loss: 493887827.7346693\n",
            "Validation Loss: 689672021.6170673\n",
            "Training Loss: 493886172.3072841\n",
            "Validation Loss: 689700389.7235577\n",
            "Training Loss: 493884523.29790884\n",
            "Validation Loss: 689728706.8223895\n",
            "Training Loss: 493882880.6816484\n",
            "Validation Loss: 689756972.9968575\n",
            "Training Loss: 493881244.43370616\n",
            "Validation Loss: 689785188.330166\n",
            "Training Loss: 493879614.5293978\n",
            "Validation Loss: 689813352.9054047\n",
            "Training Loss: 493877990.944107\n",
            "Validation Loss: 689841466.8055717\n",
            "Training Loss: 493876373.6533342\n",
            "Validation Loss: 689869530.1135614\n",
            "Training Loss: 493874762.6326627\n",
            "Validation Loss: 689897542.9121726\n",
            "Training Loss: 493873157.85777617\n",
            "Validation Loss: 689925505.2840937\n",
            "Training Loss: 493871559.3044594\n",
            "Validation Loss: 689953417.3119112\n",
            "Training Loss: 493869966.9485763\n",
            "Validation Loss: 689981279.0781308\n",
            "Training Loss: 493868380.7660953\n",
            "Validation Loss: 690009090.665134\n",
            "Training Loss: 493866800.7330749\n",
            "Validation Loss: 690036852.1552151\n",
            "Training Loss: 493865226.82566214\n",
            "Validation Loss: 690064563.6305583\n",
            "Training Loss: 493863659.0201072\n",
            "Validation Loss: 690092225.1732577\n",
            "Training Loss: 493862097.29274195\n",
            "Validation Loss: 690119836.8652934\n",
            "Training Loss: 493860541.6199945\n",
            "Validation Loss: 690147398.7885586\n",
            "Training Loss: 493858991.97838604\n",
            "Validation Loss: 690174911.0248343\n",
            "Training Loss: 493857448.3445281\n",
            "Validation Loss: 690202373.6558026\n",
            "Training Loss: 493855910.69511867\n",
            "Validation Loss: 690229786.7630501\n",
            "Training Loss: 493854379.0069488\n",
            "Validation Loss: 690257150.4280541\n",
            "Training Loss: 493852853.2569056\n",
            "Validation Loss: 690284464.7321966\n",
            "Training Loss: 493851333.42195266\n",
            "Validation Loss: 690311729.7567551\n",
            "Training Loss: 493849819.4791563\n",
            "Validation Loss: 690338945.5829134\n",
            "Training Loss: 493848311.4056593\n",
            "Validation Loss: 690366112.2917393\n",
            "Training Loss: 493846809.1787129\n",
            "Validation Loss: 690393229.964213\n",
            "Training Loss: 493845312.7756368\n",
            "Validation Loss: 690420298.6812112\n",
            "Training Loss: 493843822.1738373\n",
            "Validation Loss: 690447318.5234985\n",
            "Training Loss: 493842337.3508329\n",
            "Validation Loss: 690474289.5717499\n",
            "Training Loss: 493840858.2841982\n",
            "Validation Loss: 690501211.9065341\n",
            "Training Loss: 493839384.9516181\n",
            "Validation Loss: 690528085.6083257\n",
            "Training Loss: 493837917.33085763\n",
            "Validation Loss: 690554910.7574818\n",
            "Training Loss: 493836455.39975995\n",
            "Validation Loss: 690581687.4342734\n",
            "Training Loss: 493834999.13626397\n",
            "Validation Loss: 690608415.7188619\n",
            "Training Loss: 493833548.5183913\n",
            "Validation Loss: 690635095.6913157\n",
            "Training Loss: 493832103.5242493\n",
            "Validation Loss: 690661727.4315876\n",
            "Training Loss: 493830664.13202345\n",
            "Validation Loss: 690688311.0195439\n",
            "Training Loss: 493829230.3199933\n",
            "Validation Loss: 690714846.5349387\n",
            "Training Loss: 493827802.0665225\n",
            "Validation Loss: 690741334.0574296\n",
            "Training Loss: 493826379.35004556\n",
            "Validation Loss: 690767773.6665726\n",
            "Training Loss: 493824962.1490973\n",
            "Validation Loss: 690794165.441823\n",
            "Training Loss: 493823550.44229007\n",
            "Validation Loss: 690820509.4625249\n",
            "Training Loss: 493822144.2083183\n",
            "Validation Loss: 690846805.8079387\n",
            "Training Loss: 493820743.42595184\n",
            "Validation Loss: 690873054.5572075\n",
            "Training Loss: 493819348.07405907\n",
            "Validation Loss: 690899255.7893784\n",
            "Training Loss: 493817958.13157165\n",
            "Validation Loss: 690925409.5833982\n",
            "Training Loss: 493816573.5775219\n",
            "Validation Loss: 690951516.0181116\n",
            "Training Loss: 493815194.3910026\n",
            "Validation Loss: 690977575.1722602\n",
            "Training Loss: 493813820.55121106\n",
            "Validation Loss: 691003587.124485\n",
            "Training Loss: 493812452.0374056\n",
            "Validation Loss: 691029551.9533234\n",
            "Training Loss: 493811088.8289374\n",
            "Validation Loss: 691055469.737216\n",
            "Training Loss: 493809730.90522635\n",
            "Validation Loss: 691081340.5544988\n",
            "Training Loss: 493808378.24578726\n",
            "Validation Loss: 691107164.4834087\n",
            "Training Loss: 493807030.83020216\n",
            "Validation Loss: 691132941.6020718\n",
            "Training Loss: 493805688.6381374\n",
            "Validation Loss: 691158671.988523\n",
            "Training Loss: 493804351.6493306\n",
            "Validation Loss: 691184355.7206931\n",
            "Training Loss: 493803019.84361595\n",
            "Validation Loss: 691209992.8764077\n",
            "Training Loss: 493801693.20088464\n",
            "Validation Loss: 691235583.5333954\n",
            "Training Loss: 493800371.70111704\n",
            "Validation Loss: 691261127.7692815\n",
            "Training Loss: 493799055.3243756\n",
            "Validation Loss: 691286625.6615876\n",
            "Training Loss: 493797744.0507927\n",
            "Validation Loss: 691312077.2877393\n",
            "Training Loss: 493796437.8605702\n",
            "Validation Loss: 691337482.7250535\n",
            "Training Loss: 493795136.7340083\n",
            "Validation Loss: 691362842.0507455\n",
            "Training Loss: 493793840.65146464\n",
            "Validation Loss: 691388155.3419411\n",
            "Training Loss: 493792549.59337974\n",
            "Validation Loss: 691413422.675649\n",
            "Training Loss: 493791263.5402744\n",
            "Validation Loss: 691438644.1287856\n",
            "Training Loss: 493789982.4727355\n",
            "Validation Loss: 691463819.7781622\n",
            "Training Loss: 493788706.3714345\n",
            "Validation Loss: 691488949.700493\n",
            "Training Loss: 493787435.2171083\n",
            "Validation Loss: 691514033.9723836\n",
            "Training Loss: 493786168.99057823\n",
            "Validation Loss: 691539072.670346\n",
            "Training Loss: 493784907.6727359\n",
            "Validation Loss: 691564065.8707827\n",
            "Training Loss: 493783651.244546\n",
            "Validation Loss: 691589013.6500009\n",
            "Training Loss: 493782399.6870466\n",
            "Validation Loss: 691613916.0842057\n",
            "Training Loss: 493781152.9813485\n",
            "Validation Loss: 691638773.2494955\n",
            "Training Loss: 493779911.10864645\n",
            "Validation Loss: 691663585.2218773\n",
            "Training Loss: 493778674.0501923\n",
            "Validation Loss: 691688352.0772408\n",
            "Training Loss: 493777441.78731936\n",
            "Validation Loss: 691713073.8913921\n",
            "Training Loss: 493776214.3014337\n",
            "Validation Loss: 691737750.7400284\n",
            "Training Loss: 493774991.5740081\n",
            "Validation Loss: 691762382.6987379\n",
            "Training Loss: 493773773.5865926\n",
            "Validation Loss: 691786969.8430235\n",
            "Training Loss: 493772560.32081133\n",
            "Validation Loss: 691811512.24827\n",
            "Training Loss: 493771351.7583519\n",
            "Validation Loss: 691836009.9897738\n",
            "Training Loss: 493770147.8809745\n",
            "Validation Loss: 691860463.1427208\n",
            "Training Loss: 493768948.6705158\n",
            "Validation Loss: 691884871.7822036\n",
            "Training Loss: 493767754.10887766\n",
            "Validation Loss: 691909235.9832112\n",
            "Training Loss: 493766564.1780316\n",
            "Validation Loss: 691933555.820623\n",
            "Training Loss: 493765378.86003107\n",
            "Validation Loss: 691957831.3692285\n",
            "Training Loss: 493764198.136976\n",
            "Validation Loss: 691982062.7037157\n",
            "Training Loss: 493763021.99105597\n",
            "Validation Loss: 692006249.8986624\n",
            "Training Loss: 493761850.40452605\n",
            "Validation Loss: 692030393.0285527\n",
            "Training Loss: 493760683.35969806\n",
            "Validation Loss: 692054492.167764\n",
            "Training Loss: 493759520.8389629\n",
            "Validation Loss: 692078547.3905792\n",
            "Training Loss: 493758362.8247891\n",
            "Validation Loss: 692102558.7711775\n",
            "Training Loss: 493757209.2996886\n",
            "Validation Loss: 692126526.3836371\n",
            "Training Loss: 493756060.24626327\n",
            "Validation Loss: 692150450.3019327\n",
            "Training Loss: 493754915.6471678\n",
            "Validation Loss: 692174330.5999379\n",
            "Training Loss: 493753775.4851372\n",
            "Validation Loss: 692198167.3514322\n",
            "Training Loss: 493752639.74296427\n",
            "Validation Loss: 692221960.6300888\n",
            "Training Loss: 493751508.4035117\n",
            "Validation Loss: 692245710.50948\n",
            "Training Loss: 493750381.44970185\n",
            "Validation Loss: 692269417.0630761\n",
            "Training Loss: 493749258.86453223\n",
            "Validation Loss: 692293080.3642514\n",
            "Training Loss: 493748140.6310724\n",
            "Validation Loss: 692316700.4862764\n",
            "Training Loss: 493747026.7324352\n",
            "Validation Loss: 692340277.5023234\n",
            "Training Loss: 493745917.1518213\n",
            "Validation Loss: 692363811.4854552\n",
            "Training Loss: 493744811.87247956\n",
            "Validation Loss: 692387302.5086458\n",
            "Training Loss: 493743710.8777467\n",
            "Validation Loss: 692410750.6447631\n",
            "Training Loss: 493742614.1509906\n",
            "Validation Loss: 692434155.966573\n",
            "Training Loss: 493741521.6756811\n",
            "Validation Loss: 692457518.5467458\n",
            "Training Loss: 493740433.43531394\n",
            "Validation Loss: 692480838.457841\n",
            "Training Loss: 493739349.41348374\n",
            "Validation Loss: 692504115.7723347\n",
            "Training Loss: 493738269.59382886\n",
            "Validation Loss: 692527350.5625854\n",
            "Training Loss: 493737193.960053\n",
            "Validation Loss: 692550542.9008574\n",
            "Training Loss: 493736122.49593186\n",
            "Validation Loss: 692573692.8593194\n",
            "Training Loss: 493735055.18528926\n",
            "Validation Loss: 692596800.5100374\n",
            "Training Loss: 493733992.0120264\n",
            "Validation Loss: 692619865.9249686\n",
            "Training Loss: 493732932.9601004\n",
            "Validation Loss: 692642889.1759855\n",
            "Training Loss: 493731878.01352984\n",
            "Validation Loss: 692665870.3348478\n",
            "Training Loss: 493730827.15639645\n",
            "Validation Loss: 692688809.4732171\n",
            "Training Loss: 493729780.37284225\n",
            "Validation Loss: 692711706.6626611\n",
            "Training Loss: 493728737.647077\n",
            "Validation Loss: 692734561.9746386\n",
            "Training Loss: 493727698.9633686\n",
            "Validation Loss: 692757375.4805193\n",
            "Training Loss: 493726664.3060365\n",
            "Validation Loss: 692780147.2515632\n",
            "Training Loss: 493725633.6594747\n",
            "Validation Loss: 692802877.3589324\n",
            "Training Loss: 493724607.00812936\n",
            "Validation Loss: 692825565.8736924\n",
            "Training Loss: 493723584.3365132\n",
            "Validation Loss: 692848212.8668087\n",
            "Training Loss: 493722565.6291914\n",
            "Validation Loss: 692870818.4091425\n",
            "Training Loss: 493721550.87080073\n",
            "Validation Loss: 692893382.5714581\n",
            "Training Loss: 493720540.0460197\n",
            "Validation Loss: 692915905.4244211\n",
            "Training Loss: 493719533.13960075\n",
            "Validation Loss: 692938387.0385975\n",
            "Training Loss: 493718530.1363529\n",
            "Validation Loss: 692960827.4844496\n",
            "Training Loss: 493717531.0211385\n",
            "Validation Loss: 692983226.8323467\n",
            "Training Loss: 493716535.7788888\n",
            "Validation Loss: 693005585.1525517\n",
            "Training Loss: 493715544.3945813\n",
            "Validation Loss: 693027902.5152314\n",
            "Training Loss: 493714556.8532617\n",
            "Validation Loss: 693050178.9904554\n",
            "Training Loss: 493713573.14001966\n",
            "Validation Loss: 693072414.6481913\n",
            "Training Loss: 493712593.24002963\n",
            "Validation Loss: 693094609.5583049\n",
            "Training Loss: 493711617.13849384\n",
            "Validation Loss: 693116763.7905706\n",
            "Training Loss: 493710644.8206873\n",
            "Validation Loss: 693138877.4146518\n",
            "Training Loss: 493709676.271944\n",
            "Validation Loss: 693160950.5001255\n",
            "Training Loss: 493708711.4776417\n",
            "Validation Loss: 693182983.1164566\n",
            "Training Loss: 493707750.4232344\n",
            "Validation Loss: 693204975.3330275\n",
            "Training Loss: 493706793.09421295\n",
            "Validation Loss: 693226927.2191044\n",
            "Training Loss: 493705839.4761386\n",
            "Validation Loss: 693248838.8438625\n",
            "Training Loss: 493704889.5546138\n",
            "Validation Loss: 693270710.2763792\n",
            "Training Loss: 493703943.31532073\n",
            "Validation Loss: 693292541.5856314\n",
            "Training Loss: 493703000.74397427\n",
            "Validation Loss: 693314332.8404973\n",
            "Training Loss: 493702061.82635385\n",
            "Validation Loss: 693336084.1097561\n",
            "Training Loss: 493701126.5482905\n",
            "Validation Loss: 693357795.4620924\n",
            "Training Loss: 493700194.895678\n",
            "Validation Loss: 693379466.9660813\n",
            "Training Loss: 493699266.8544531\n",
            "Validation Loss: 693401098.6902083\n",
            "Training Loss: 493698342.410619\n",
            "Validation Loss: 693422690.702861\n",
            "Training Loss: 493697421.55023074\n",
            "Validation Loss: 693444243.0723261\n",
            "Training Loss: 493696504.25939244\n",
            "Validation Loss: 693465755.8667897\n",
            "Training Loss: 493695590.52425975\n",
            "Validation Loss: 693487229.1543398\n",
            "Training Loss: 493694680.33104974\n",
            "Validation Loss: 693508663.0029744\n",
            "Training Loss: 493693773.666031\n",
            "Validation Loss: 693530057.4805801\n",
            "Training Loss: 493692870.5155202\n",
            "Validation Loss: 693551412.6549555\n",
            "Training Loss: 493691970.86589634\n",
            "Validation Loss: 693572728.5937927\n",
            "Training Loss: 493691074.7035868\n",
            "Validation Loss: 693594005.3647007\n",
            "Training Loss: 493690182.0150625\n",
            "Validation Loss: 693615243.0351714\n",
            "Training Loss: 493689292.78685987\n",
            "Validation Loss: 693636441.6726103\n",
            "Training Loss: 493688407.0055698\n",
            "Validation Loss: 693657601.3443229\n",
            "Training Loss: 493687524.6578208\n",
            "Validation Loss: 693678722.1175193\n",
            "Training Loss: 493686645.73029494\n",
            "Validation Loss: 693699804.0593013\n",
            "Training Loss: 493685770.2097452\n",
            "Validation Loss: 693720847.2366949\n",
            "Training Loss: 493684898.0829468\n",
            "Validation Loss: 693741851.7165997\n",
            "Training Loss: 493684029.336758\n",
            "Validation Loss: 693762817.5658376\n",
            "Training Loss: 493683163.9580644\n",
            "Validation Loss: 693783744.8511312\n",
            "Training Loss: 493682301.93381083\n",
            "Validation Loss: 693804633.6390973\n",
            "Training Loss: 493681443.2509831\n",
            "Validation Loss: 693825483.9962656\n",
            "Training Loss: 493680587.8966416\n",
            "Validation Loss: 693846295.9890593\n",
            "Training Loss: 493679735.8578716\n",
            "Validation Loss: 693867069.6838081\n",
            "Training Loss: 493678887.1218212\n",
            "Validation Loss: 693887805.1467485\n",
            "Training Loss: 493678041.67568636\n",
            "Validation Loss: 693908502.4440128\n",
            "Training Loss: 493677199.5067078\n",
            "Validation Loss: 693929161.6416401\n",
            "Training Loss: 493676360.60218304\n",
            "Validation Loss: 693949782.8055735\n",
            "Training Loss: 493675524.9494587\n",
            "Validation Loss: 693970366.0016547\n",
            "Training Loss: 493674692.5359242\n",
            "Validation Loss: 693990911.2956369\n",
            "Training Loss: 493673863.3490203\n",
            "Validation Loss: 694011418.7531661\n",
            "Training Loss: 493673037.37623936\n",
            "Validation Loss: 694031888.4397987\n",
            "Training Loss: 493672214.6051173\n",
            "Validation Loss: 694052320.420994\n",
            "Training Loss: 493671395.02324384\n",
            "Validation Loss: 694072714.7621114\n",
            "Training Loss: 493670578.6182498\n",
            "Validation Loss: 694093071.5284152\n",
            "Training Loss: 493669765.3778281\n",
            "Validation Loss: 694113390.7850732\n",
            "Training Loss: 493668955.28970164\n",
            "Validation Loss: 694133672.597159\n",
            "Training Loss: 493668148.3416504\n",
            "Validation Loss: 694153917.0296469\n",
            "Training Loss: 493667344.52150345\n",
            "Validation Loss: 694174124.1474204\n",
            "Training Loss: 493666543.81712574\n",
            "Validation Loss: 694194294.0152576\n",
            "Training Loss: 493665746.2164461\n",
            "Validation Loss: 694214426.6978495\n",
            "Training Loss: 493664951.70743155\n",
            "Validation Loss: 694234522.2597818\n",
            "Training Loss: 493664160.27809083\n",
            "Validation Loss: 694254580.7655585\n",
            "Training Loss: 493663371.91648453\n",
            "Validation Loss: 694274602.2795734\n",
            "Training Loss: 493662586.61072505\n",
            "Validation Loss: 694294586.8661236\n",
            "Training Loss: 493661804.34896\n",
            "Validation Loss: 694314534.5894334\n",
            "Training Loss: 493661025.11938614\n",
            "Validation Loss: 694334445.5136011\n",
            "Training Loss: 493660248.9102537\n",
            "Validation Loss: 694354319.7026498\n",
            "Training Loss: 493659475.70985097\n",
            "Validation Loss: 694374157.2204952\n",
            "Training Loss: 493658705.5065066\n",
            "Validation Loss: 694393958.1309692\n",
            "Training Loss: 493657938.28861225\n",
            "Validation Loss: 694413722.4978006\n",
            "Training Loss: 493657174.04458547\n",
            "Validation Loss: 694433450.3846219\n",
            "Training Loss: 493656412.76290077\n",
            "Validation Loss: 694453141.854976\n",
            "Training Loss: 493655654.4320712\n",
            "Validation Loss: 694472796.972303\n",
            "Training Loss: 493654899.0406556\n",
            "Validation Loss: 694492415.7999598\n",
            "Training Loss: 493654146.5772619\n",
            "Validation Loss: 694511998.4011959\n",
            "Training Loss: 493653397.03053665\n",
            "Validation Loss: 694531544.8391714\n",
            "Training Loss: 493652650.3891716\n",
            "Validation Loss: 694551055.1769512\n",
            "Training Loss: 493651906.64190084\n",
            "Validation Loss: 694570529.4775069\n",
            "Training Loss: 493651165.77751464\n",
            "Validation Loss: 694589967.8037148\n",
            "Training Loss: 493650427.7848243\n",
            "Validation Loss: 694609370.2183564\n",
            "Training Loss: 493649692.652696\n",
            "Validation Loss: 694628736.7841148\n",
            "Training Loss: 493648960.3700558\n",
            "Validation Loss: 694648067.5635796\n",
            "Training Loss: 493648230.92584294\n",
            "Validation Loss: 694667362.619258\n",
            "Training Loss: 493647504.30905014\n",
            "Validation Loss: 694686622.013547\n",
            "Training Loss: 493646780.5087274\n",
            "Validation Loss: 694705845.8087527\n",
            "Training Loss: 493646059.51394886\n",
            "Validation Loss: 694725034.0670952\n",
            "Training Loss: 493645341.3138379\n",
            "Validation Loss: 694744186.8506938\n",
            "Training Loss: 493644625.8975596\n",
            "Validation Loss: 694763304.2215757\n",
            "Training Loss: 493643913.25432724\n",
            "Validation Loss: 694782386.2416722\n",
            "Training Loss: 493643203.3733845\n",
            "Validation Loss: 694801432.9728245\n",
            "Training Loss: 493642496.2440272\n",
            "Validation Loss: 694820444.4767791\n",
            "Training Loss: 493641791.85558087\n",
            "Validation Loss: 694839420.8151836\n",
            "Training Loss: 493641090.197423\n",
            "Validation Loss: 694858362.0496002\n",
            "Training Loss: 493640391.2589722\n",
            "Validation Loss: 694877268.2414931\n",
            "Training Loss: 493639695.0296808\n",
            "Validation Loss: 694896139.4522295\n",
            "Training Loss: 493639001.49905264\n",
            "Validation Loss: 694914975.7430922\n",
            "Training Loss: 493638310.6566139\n",
            "Validation Loss: 694933777.1752642\n",
            "Training Loss: 493637622.4919536\n",
            "Validation Loss: 694952543.8098347\n",
            "Training Loss: 493636936.9946819\n",
            "Validation Loss: 694971275.7078055\n",
            "Training Loss: 493636254.1544649\n",
            "Validation Loss: 694989972.9300816\n",
            "Training Loss: 493635573.96099836\n",
            "Validation Loss: 695008635.5374745\n",
            "Training Loss: 493634896.40402126\n",
            "Validation Loss: 695027263.5907006\n",
            "Training Loss: 493634221.47331715\n",
            "Validation Loss: 695045857.1503913\n",
            "Training Loss: 493633549.15869725\n",
            "Validation Loss: 695064416.277079\n",
            "Training Loss: 493632879.45002216\n",
            "Validation Loss: 695082941.0312067\n",
            "Training Loss: 493632212.3371933\n",
            "Validation Loss: 695101431.4731188\n",
            "Training Loss: 493631547.81014574\n",
            "Validation Loss: 695119887.6630768\n",
            "Training Loss: 493630885.8588536\n",
            "Validation Loss: 695138309.661243\n",
            "Training Loss: 493630226.473327\n",
            "Validation Loss: 695156697.5276887\n",
            "Training Loss: 493629569.6436318\n",
            "Validation Loss: 695175051.3223954\n",
            "Training Loss: 493628915.35984725\n",
            "Validation Loss: 695193371.1052504\n",
            "Training Loss: 493628263.6121138\n",
            "Validation Loss: 695211656.936044\n",
            "Training Loss: 493627614.39057976\n",
            "Validation Loss: 695229908.8744904\n",
            "Training Loss: 493626967.68547636\n",
            "Validation Loss: 695248126.980192\n",
            "Training Loss: 493626323.48704\n",
            "Validation Loss: 695266311.3126711\n",
            "Training Loss: 493625681.7855435\n",
            "Validation Loss: 695284461.931359\n",
            "b,w found by gradient descent: 34180.04,[-13702.89573317 -10410.01021381  74717.40332819] \n",
            "prediction: 20004.96, target value: 22000.0\n",
            "prediction: 62196.43, target value: 45000.0\n",
            "prediction: 25311.72, target value: 22000.0\n",
            "prediction: 41252.08, target value: 50000.0\n",
            "prediction: 48741.62, target value: 45000.0\n",
            "prediction: -5928.15, target value: 10000.0\n",
            "prediction: 14768.27, target value: 16000.0\n",
            "prediction: 33281.90, target value: 24000.0\n",
            "prediction: 12017.13, target value: 8500.0\n",
            "prediction: 302.18, target value: 10000.0\n",
            "prediction: 8112.14, target value: 12000.0\n",
            "prediction: 21566.95, target value: 25000.0\n",
            "prediction: 29376.92, target value: 35000.0\n",
            "prediction: 21566.95, target value: 40000.0\n",
            "prediction: 302.18, target value: 18000.0\n",
            "prediction: 113121.42, target value: 130000.0\n",
            "prediction: 21566.95, target value: 16000.0\n",
            "prediction: 55934.82, target value: 50000.0\n",
            "prediction: 7899.51, target value: 20000.0\n",
            "prediction: 115286.55, target value: 150000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,w,b,test):\n",
        "  n=X.shape[0]\n",
        "  for i in range(n):\n",
        "    y_pred.append(np.dot(X[i],w)+b)\n",
        "    print(str(i+1)+\":\",\"Predicted_value=\",y_pred[i])\n",
        "    print(\" Actual_Value:\",test[i])"
      ],
      "metadata": {
        "id": "3Xm29frTyt1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=[]\n",
        "predict(X_test,w_final,b_final,y_test)\n",
        "y_pred=np.array(y_pred)\n",
        "print(y_pred.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvsZYglywnh",
        "outputId": "f4280d15-0b64-470f-e7ff-b424bde4e0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 8200.0\n",
            "2: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 16000.0\n",
            "3: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 22000.0\n",
            "4: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 35000.0\n",
            "5: Predicted_value= 159981.2233046001\n",
            " Actual_Value: 180000.0\n",
            "6: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "7: Predicted_value= 57336.59728830197\n",
            " Actual_Value: 60000.0\n",
            "8: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 15000.0\n",
            "9: Predicted_value= 89691.52130253537\n",
            " Actual_Value: 35000.0\n",
            "10: Predicted_value= 56872.01161664842\n",
            " Actual_Value: 45000.0\n",
            "11: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 15000.0\n",
            "12: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 12000.0\n",
            "13: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "14: Predicted_value= 5626.591798951697\n",
            " Actual_Value: 45000.0\n",
            "15: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 50000.0\n",
            "16: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "17: Predicted_value= 144361.28952636351\n",
            " Actual_Value: 165000.0\n",
            "18: Predicted_value= 61557.9917501194\n",
            " Actual_Value: 40000.0\n",
            "19: Predicted_value= 302.177904454722\n",
            " Actual_Value: 13500.0\n",
            "20: Predicted_value= 14342.469553277602\n",
            " Actual_Value: 80000.0\n",
            "21: Predicted_value= 8272.356473048942\n",
            " Actual_Value: 30000.0\n",
            "22: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "23: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "24: Predicted_value= 58451.65374606215\n",
            " Actual_Value: 35000.0\n",
            "25: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 23000.0\n",
            "26: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "27: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 16000.0\n",
            "28: Predicted_value= 5769.154726837536\n",
            " Actual_Value: 13500.0\n",
            "29: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 19000.0\n",
            "30: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "31: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "32: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "33: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "34: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 21000.0\n",
            "35: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "36: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 28000.0\n",
            "37: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13500.0\n",
            "38: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 28000.0\n",
            "39: Predicted_value= 164046.4184286352\n",
            " Actual_Value: 40000.0\n",
            "40: Predicted_value= 3781.88552223401\n",
            " Actual_Value: 16000.0\n",
            "41: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "42: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "43: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "44: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "45: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 30000.0\n",
            "46: Predicted_value= 41412.28951788772\n",
            " Actual_Value: 40000.0\n",
            "47: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 15000.0\n",
            "48: Predicted_value= 58664.29165945209\n",
            " Actual_Value: 50000.0\n",
            "49: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 15000.0\n",
            "50: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "51: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 11000.0\n",
            "52: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "53: Predicted_value= 27211.786189588925\n",
            " Actual_Value: 32000.0\n",
            "54: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "55: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 22000.0\n",
            "56: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 14000.0\n",
            "57: Predicted_value= 50073.32808142195\n",
            " Actual_Value: 28000.0\n",
            "58: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "59: Predicted_value= 68426.75027084997\n",
            " Actual_Value: 50000.0\n",
            "60: Predicted_value= 38926.73652326639\n",
            " Actual_Value: 23000.0\n",
            "61: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 12500.0\n",
            "62: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "63: Predicted_value= 107476.58471644149\n",
            " Actual_Value: 150000.0\n",
            "64: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 27000.0\n",
            "65: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 21000.0\n",
            "66: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "67: Predicted_value= 35447.028905487095\n",
            " Actual_Value: 12000.0\n",
            "68: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 35000.0\n",
            "69: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 11000.0\n",
            "70: Predicted_value= 6728.011846815345\n",
            " Actual_Value: 36000.0\n",
            "71: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 12000.0\n",
            "72: Predicted_value= 48901.8330480542\n",
            " Actual_Value: 40000.0\n",
            "73: Predicted_value= 27797.273695844706\n",
            " Actual_Value: 11000.0\n",
            "74: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10500.0\n",
            "75: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 35000.0\n",
            "76: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 12000.0\n",
            "77: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 20000.0\n",
            "78: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 25000.0\n",
            "79: Predicted_value= 206841.02463930994\n",
            " Actual_Value: 260000.0\n",
            "80: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 17000.0\n",
            "81: Predicted_value= 99666.61782732318\n",
            " Actual_Value: 80000.0\n",
            "82: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 14000.0\n",
            "83: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 12500.0\n",
            "84: Predicted_value= 27033.925758522946\n",
            " Actual_Value: 23000.0\n",
            "85: Predicted_value= 130320.99787754062\n",
            " Actual_Value: 65000.0\n",
            "86: Predicted_value= 41248.06549671826\n",
            " Actual_Value: 28000.0\n",
            "87: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 21000.0\n",
            "88: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 10000.0\n",
            "89: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 13000.0\n",
            "90: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 16000.0\n",
            "91: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 14000.0\n",
            "92: Predicted_value= 302.177904454722\n",
            " Actual_Value: 18000.0\n",
            "93: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12500.0\n",
            "94: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 15000.0\n",
            "95: Predicted_value= 18282.750501016893\n",
            " Actual_Value: 15000.0\n",
            "96: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "97: Predicted_value= 78136.78264833384\n",
            " Actual_Value: 40000.0\n",
            "98: Predicted_value= 201196.18738586115\n",
            " Actual_Value: 80000.0\n",
            "99: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "100: Predicted_value= 206841.02463930994\n",
            " Actual_Value: 250000.0\n",
            "101: Predicted_value= 41517.14198571576\n",
            " Actual_Value: 10000.0\n",
            "102: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 12000.0\n",
            "103: Predicted_value= 2467.307540124246\n",
            " Actual_Value: 8000.0\n",
            "104: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 20000.0\n",
            "105: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 20000.0\n",
            "106: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "107: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "108: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "109: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 24000.0\n",
            "110: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 25000.0\n",
            "111: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 21000.0\n",
            "112: Predicted_value= -15317.755873781884\n",
            " Actual_Value: 10000.0\n",
            "113: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 13000.0\n",
            "114: Predicted_value= 58611.86542553806\n",
            " Actual_Value: 45000.0\n",
            "115: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "116: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 10000.0\n",
            "117: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "118: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 30000.0\n",
            "119: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "120: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "121: Predicted_value= 103731.81295135827\n",
            " Actual_Value: 80000.0\n",
            "122: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 12000.0\n",
            "123: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "124: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "125: Predicted_value= 172008.5723138423\n",
            " Actual_Value: 220000.00000000003\n",
            "126: Predicted_value= 302.177904454722\n",
            " Actual_Value: 20000.0\n",
            "127: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 25000.0\n",
            "128: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 20000.0\n",
            "129: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 22000.0\n",
            "130: Predicted_value= 302.177904454722\n",
            " Actual_Value: 18500.0\n",
            "131: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 22000.0\n",
            "132: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 26000.0\n",
            "133: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "134: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "135: Predicted_value= 29056.492466306612\n",
            " Actual_Value: 16000.0\n",
            "136: Predicted_value= 2467.307540124246\n",
            " Actual_Value: 14000.0\n",
            "137: Predicted_value= 29537.12750473435\n",
            " Actual_Value: 38000.0\n",
            "138: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "139: Predicted_value= 40931.65447945998\n",
            " Actual_Value: 22000.0\n",
            "140: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "141: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 70000.0\n",
            "142: Predicted_value= 391850.0361951757\n",
            " Actual_Value: 300000.0\n",
            "143: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 16000.0\n",
            "144: Predicted_value= 55930.80324826067\n",
            " Actual_Value: 45000.0\n",
            "145: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "146: Predicted_value= 166211.5480643047\n",
            " Actual_Value: 180000.0\n",
            "147: Predicted_value= -11412.772429222729\n",
            " Actual_Value: 16000.0\n",
            "148: Predicted_value= 15496.835855911471\n",
            " Actual_Value: 30000.0\n",
            "149: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 100000.0\n",
            "150: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 32000.0\n",
            "151: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 20000.0\n",
            "152: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 15000.0\n",
            "153: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "154: Predicted_value= 25294.0719496333\n",
            " Actual_Value: 15000.0\n",
            "155: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 15000.0\n",
            "156: Predicted_value= 6372.290984683386\n",
            " Actual_Value: 16000.0\n",
            "157: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 24000.0\n",
            "158: Predicted_value= 179666.3522068718\n",
            " Actual_Value: 85000.0\n",
            "159: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 11000.0\n",
            "160: Predicted_value= 2042.0317133443677\n",
            " Actual_Value: 15000.0\n",
            "161: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "162: Predicted_value= 9674.138171396691\n",
            " Actual_Value: 11000.0\n",
            "163: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 20000.0\n",
            "164: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 14500.0\n",
            "165: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 13500.0\n",
            "166: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 140000.0\n",
            "167: Predicted_value= 480.03833552070864\n",
            " Actual_Value: 20000.0\n",
            "168: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "169: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "170: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 14000.0\n",
            "171: Predicted_value= 55167.455310938916\n",
            " Actual_Value: 50000.0\n",
            "172: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 15000.0\n",
            "173: Predicted_value= 167791.19019371842\n",
            " Actual_Value: 180000.0\n",
            "174: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 14000.0\n",
            "175: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 26000.0\n",
            "176: Predicted_value= 116866.19373497352\n",
            " Actual_Value: 90000.0\n",
            "177: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 18000.0\n",
            "178: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "179: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 50000.0\n",
            "180: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 18000.0\n",
            "181: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 23000.0\n",
            "182: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 20000.0\n",
            "183: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 25000.0\n",
            "184: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 15000.0\n",
            "185: Predicted_value= 22504.14496283433\n",
            " Actual_Value: 16000.0\n",
            "186: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 23000.0\n",
            "187: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 16000.0\n",
            "188: Predicted_value= 124836.37230356774\n",
            " Actual_Value: 140000.0\n",
            "189: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "190: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 18000.0\n",
            "191: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 18000.0\n",
            "192: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "193: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 25000.0\n",
            "194: Predicted_value= 16880.96880266915\n",
            " Actual_Value: 27000.0\n",
            "195: Predicted_value= 19987.3068067264\n",
            " Actual_Value: 17000.0\n",
            "196: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 12500.0\n",
            "197: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 22000.0\n",
            "198: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "199: Predicted_value= 7951.933114097119\n",
            " Actual_Value: 10000.0\n",
            "200: Predicted_value= 58273.79331499617\n",
            " Actual_Value: 45000.0\n",
            "201: Predicted_value= 49214.231723618934\n",
            " Actual_Value: 15000.0\n",
            "202: Predicted_value= 115286.5516055598\n",
            " Actual_Value: 114999.99999999999\n",
            "203: Predicted_value= 39529.87278111224\n",
            " Actual_Value: 30000.0\n",
            "204: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 20000.0\n",
            "205: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15000.0\n",
            "206: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 30000.0\n",
            "207: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "208: Predicted_value= 40310.86947002407\n",
            " Actual_Value: 35000.0\n",
            "209: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 12000.0\n",
            "210: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 18000.0\n",
            "211: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "212: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 18000.0\n",
            "213: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 40000.0\n",
            "214: Predicted_value= 136551.32263724523\n",
            " Actual_Value: 160000.0\n",
            "215: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 11000.0\n",
            "216: Predicted_value= 38748.8760922004\n",
            " Actual_Value: 25000.0\n",
            "217: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "218: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "219: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "220: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "221: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 22000.0\n",
            "222: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "223: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 15000.0\n",
            "224: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 15000.0\n",
            "225: Predicted_value= 3582.3639978844076\n",
            " Actual_Value: 14000.0\n",
            "226: Predicted_value= -283.30960180105467\n",
            " Actual_Value: 18000.0\n",
            "227: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 20000.0\n",
            "228: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 30000.0\n",
            "229: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "230: Predicted_value= 45157.06128297096\n",
            " Actual_Value: 42000.0\n",
            "231: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 25000.0\n",
            "232: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 15000.0\n",
            "233: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 9500.0\n",
            "234: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 20000.0\n",
            "235: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 55000.0\n",
            "236: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 22000.0\n",
            "237: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "238: Predicted_value= 11023.493635830418\n",
            " Actual_Value: 22000.0\n",
            "239: Predicted_value= 19666.883447774577\n",
            " Actual_Value: 8000.0\n",
            "240: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 14000.0\n",
            "241: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 8500.0\n",
            "242: Predicted_value= 72331.73371540912\n",
            " Actual_Value: 150000.0\n",
            "243: Predicted_value= 462.3895839306351\n",
            " Actual_Value: 13000.0\n",
            "244: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 16000.0\n",
            "245: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "246: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 33000.0\n",
            "247: Predicted_value= 375909.67905798735\n",
            " Actual_Value: 700000.0\n",
            "248: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "249: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 12000.0\n",
            "250: Predicted_value= 47339.83967023054\n",
            " Actual_Value: 35000.0\n",
            "251: Predicted_value= 154336.38605115132\n",
            " Actual_Value: 120000.0\n",
            "252: Predicted_value= 15657.047535387384\n",
            " Actual_Value: 35000.0\n",
            "253: Predicted_value= 41252.0778384118\n",
            " Actual_Value: 30000.0\n",
            "254: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 17000.0\n",
            "255: Predicted_value= 222460.95841754653\n",
            " Actual_Value: 280000.0\n",
            "256: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 60000.0\n",
            "257: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 30000.0\n",
            "258: Predicted_value= 8059.718559659006\n",
            " Actual_Value: 24000.0\n",
            "259: Predicted_value= 15761.900003215414\n",
            " Actual_Value: 10500.0\n",
            "260: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "261: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "262: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "263: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 17500.0\n",
            "264: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 30000.0\n",
            "265: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13500.0\n",
            "266: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 20000.0\n",
            "267: Predicted_value= 154336.38605115132\n",
            " Actual_Value: 120000.0\n",
            "268: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 25000.0\n",
            "269: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "270: Predicted_value= 4367.3730284897865\n",
            " Actual_Value: 18000.0\n",
            "271: Predicted_value= 12798.12492704401\n",
            " Actual_Value: 14000.0\n",
            "272: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 22000.0\n",
            "273: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 8500.0\n",
            "274: Predicted_value= 78136.78264833384\n",
            " Actual_Value: 70000.0\n",
            "275: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 30000.0\n",
            "276: Predicted_value= 302.177904454722\n",
            " Actual_Value: 17000.0\n",
            "277: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 13000.0\n",
            "278: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "279: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 22000.0\n",
            "280: Predicted_value= 302.177904454722\n",
            " Actual_Value: 13000.0\n",
            "281: Predicted_value= 19614.457213860554\n",
            " Actual_Value: 27000.0\n",
            "282: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "283: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 18000.0\n",
            "284: Predicted_value= 175601.15708283673\n",
            " Actual_Value: 80000.0\n",
            "285: Predicted_value= 144361.28952636351\n",
            " Actual_Value: 140000.0\n",
            "286: Predicted_value= 43434.856225671385\n",
            " Actual_Value: 23000.0\n",
            "287: Predicted_value= 26647.439755760577\n",
            " Actual_Value: 45000.0\n",
            "288: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "289: Predicted_value= 18087.241318360844\n",
            " Actual_Value: 10000.0\n",
            "290: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 11000.0\n",
            "291: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 12000.0\n",
            "292: Predicted_value= 111541.77984047655\n",
            " Actual_Value: 180000.0\n",
            "293: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "294: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "295: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "296: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 13000.0\n",
            "297: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 80000.0\n",
            "298: Predicted_value= 42653.859536759555\n",
            " Actual_Value: 35000.0\n",
            "299: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "300: Predicted_value= 37967.87940328857\n",
            " Actual_Value: 15000.0\n",
            "301: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 16000.0\n",
            "302: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 20000.0\n",
            "303: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 18000.0\n",
            "304: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 23000.0\n",
            "305: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 16000.0\n",
            "306: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 13000.0\n",
            "307: Predicted_value= 12975.985358109996\n",
            " Actual_Value: 21000.0\n",
            "308: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 13000.0\n",
            "309: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 30000.0\n",
            "310: Predicted_value= 21332.64992946658\n",
            " Actual_Value: 23000.0\n",
            "311: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 14000.0\n",
            "312: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 50000.0\n",
            "313: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "314: Predicted_value= 91271.1634319491\n",
            " Actual_Value: 70000.0\n",
            "315: Predicted_value= 35021.75307870723\n",
            " Actual_Value: 80000.0\n",
            "316: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "317: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 10500.0\n",
            "318: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 17000.0\n",
            "319: Predicted_value= 27051.574510113016\n",
            " Actual_Value: 18000.0\n",
            "320: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 13000.0\n",
            "321: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 16000.0\n",
            "322: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "323: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "324: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "325: Predicted_value= 302.177904454722\n",
            " Actual_Value: 13000.0\n",
            "326: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "327: Predicted_value= 7526.657287317252\n",
            " Actual_Value: 22000.0\n",
            "328: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10700.0\n",
            "329: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "330: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 12000.0\n",
            "331: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "332: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "333: Predicted_value= 17822.17717105689\n",
            " Actual_Value: 27000.0\n",
            "334: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 21000.0\n",
            "335: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 13500.0\n",
            "336: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 11000.0\n",
            "337: Predicted_value= 49214.231723618934\n",
            " Actual_Value: 14000.0\n",
            "338: Predicted_value= 150591.6142860681\n",
            " Actual_Value: 70000.0\n",
            "339: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 17000.0\n",
            "340: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 20000.0\n",
            "341: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "342: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "343: Predicted_value= 111381.56816100064\n",
            " Actual_Value: 150000.0\n",
            "344: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "345: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 11000.0\n",
            "346: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "347: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 40000.0\n",
            "348: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "349: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 26000.0\n",
            "350: Predicted_value= 17974.36416714571\n",
            " Actual_Value: 23000.0\n",
            "351: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 17000.0\n",
            "352: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 35000.0\n",
            "353: Predicted_value= 7040.41052238008\n",
            " Actual_Value: 15000.0\n",
            "354: Predicted_value= 64681.97850576672\n",
            " Actual_Value: 100000.0\n",
            "355: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 17000.0\n",
            "356: Predicted_value= 302.177904454722\n",
            " Actual_Value: 14000.0\n",
            "357: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 23000.0\n",
            "358: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 18000.0\n",
            "359: Predicted_value= 23146.591065553865\n",
            " Actual_Value: 40000.0\n",
            "360: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 21000.0\n",
            "361: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 20000.0\n",
            "362: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 10000.0\n",
            "363: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 20000.0\n",
            "364: Predicted_value= 14360.118304867676\n",
            " Actual_Value: 18000.0\n",
            "365: Predicted_value= 13418.909936479933\n",
            " Actual_Value: 10000.0\n",
            "366: Predicted_value= 16099.972113757318\n",
            " Actual_Value: 16000.0\n",
            "367: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 17000.0\n",
            "368: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 11000.0\n",
            "369: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 20000.0\n",
            "370: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "371: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 18000.0\n",
            "372: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 13000.0\n",
            "373: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "374: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 11000.0\n",
            "375: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "376: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "377: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 22000.0\n",
            "378: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 28500.0\n",
            "379: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 12000.0\n",
            "380: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "381: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 10000.0\n",
            "382: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "383: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "384: Predicted_value= -5607.723496298022\n",
            " Actual_Value: 18000.0\n",
            "385: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 11000.0\n",
            "386: Predicted_value= 99666.61782732318\n",
            " Actual_Value: 90000.0\n",
            "387: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 10000.0\n",
            "388: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 18000.0\n",
            "389: Predicted_value= 89.53999106479023\n",
            " Actual_Value: 16000.0\n",
            "390: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 21000.0\n",
            "391: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11000.0\n",
            "392: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 24000.0\n",
            "393: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 18000.0\n",
            "394: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 20000.0\n",
            "395: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "396: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 14000.0\n",
            "397: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "398: Predicted_value= 14182.257873801696\n",
            " Actual_Value: 9000.0\n",
            "399: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 10500.0\n",
            "400: Predicted_value= -17482.885509451393\n",
            " Actual_Value: 12000.0\n",
            "401: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15500.0\n",
            "402: Predicted_value= 25684.570294089215\n",
            " Actual_Value: 16000.0\n",
            "403: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "404: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 12000.0\n",
            "405: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 16000.0\n",
            "406: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 24000.0\n",
            "407: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 18000.0\n",
            "408: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 20000.0\n",
            "409: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 10000.0\n",
            "410: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "411: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 16000.0\n",
            "412: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "413: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 13000.0\n",
            "414: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 12000.0\n",
            "415: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15000.0\n",
            "416: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11000.0\n",
            "417: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 24000.0\n",
            "418: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 16000.0\n",
            "419: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "420: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "421: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "422: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 28000.0\n",
            "423: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "424: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 100000.0\n",
            "425: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 16000.0\n",
            "426: Predicted_value= 72487.93305319149\n",
            " Actual_Value: 90000.0\n",
            "427: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 19000.0\n",
            "428: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 12000.0\n",
            "429: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "430: Predicted_value= 22951.08188289781\n",
            " Actual_Value: 14000.0\n",
            "431: Predicted_value= 107476.58471644149\n",
            " Actual_Value: 110000.00000000001\n",
            "432: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "433: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 30000.0\n",
            "434: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 13500.0\n",
            "435: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 13000.0\n",
            "436: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 12000.0\n",
            "437: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 12000.0\n",
            "438: Predicted_value= 49214.231723618934\n",
            " Actual_Value: 15000.0\n",
            "439: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 16000.0\n",
            "440: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 18000.0\n",
            "441: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "442: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 10000.0\n",
            "443: Predicted_value= 105311.455080772\n",
            " Actual_Value: 50000.0\n",
            "444: Predicted_value= 35021.75307870723\n",
            " Actual_Value: 65000.0\n",
            "445: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 13000.0\n",
            "446: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 15000.0\n",
            "447: Predicted_value= 10012.210281938584\n",
            " Actual_Value: 35000.0\n",
            "448: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 10000.0\n",
            "449: Predicted_value= 34843.89264764125\n",
            " Actual_Value: 30000.0\n",
            "450: Predicted_value= 117420.9161005989\n",
            " Actual_Value: 65000.0\n",
            "451: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "452: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "453: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 17000.0\n",
            "454: Predicted_value= 110370.28480710881\n",
            " Actual_Value: 110000.00000000001\n",
            "455: Predicted_value= 19666.883447774577\n",
            " Actual_Value: 13000.0\n",
            "456: Predicted_value= 25610.48296689158\n",
            " Actual_Value: 35000.0\n",
            "457: Predicted_value= 16490.470458213233\n",
            " Actual_Value: 20000.0\n",
            "458: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "459: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 12000.0\n",
            "460: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20500.0\n",
            "461: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 45000.0\n",
            "462: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 35000.0\n",
            "463: Predicted_value= 21723.148273922496\n",
            " Actual_Value: 17000.0\n",
            "464: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 24000.0\n",
            "465: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15500.0\n",
            "466: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 50000.0\n",
            "467: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 20000.0\n",
            "468: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "469: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "470: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 28000.0\n",
            "471: Predicted_value= 31329.407547538012\n",
            " Actual_Value: 28000.0\n",
            "472: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "473: Predicted_value= 6107.226837379429\n",
            " Actual_Value: 22000.0\n",
            "474: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 50000.0\n",
            "475: Predicted_value= 212485.86189275875\n",
            " Actual_Value: 280000.0\n",
            "476: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "477: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "478: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 21000.0\n",
            "479: Predicted_value= 144361.28952636351\n",
            " Actual_Value: 250000.0\n",
            "480: Predicted_value= 37347.094393852654\n",
            " Actual_Value: 16500.0\n",
            "481: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 14000.0\n",
            "482: Predicted_value= 24530.72401231154\n",
            " Actual_Value: 22000.0\n",
            "483: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 18000.0\n",
            "484: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 30000.0\n",
            "485: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "486: Predicted_value= 84046.68404908657\n",
            " Actual_Value: 40000.0\n",
            "487: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "488: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 28000.0\n",
            "489: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "490: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 20000.0\n",
            "491: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 24000.0\n",
            "492: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "493: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 8000.0\n",
            "494: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "495: Predicted_value= 75477.38156434006\n",
            " Actual_Value: 50000.0\n",
            "496: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 21000.0\n",
            "497: Predicted_value= 103731.81295135827\n",
            " Actual_Value: 55000.0\n",
            "498: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 14000.0\n",
            "499: Predicted_value= 1083.1745933665588\n",
            " Actual_Value: 15000.0\n",
            "500: Predicted_value= 76557.14051892009\n",
            " Actual_Value: 45000.0\n",
            "501: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 18000.0\n",
            "502: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 16000.0\n",
            "503: Predicted_value= 15318.975424845485\n",
            " Actual_Value: 13500.0\n",
            "504: Predicted_value= 4988.158037925707\n",
            " Actual_Value: 15000.0\n",
            "505: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 45000.0\n",
            "506: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "507: Predicted_value= 39529.87278111224\n",
            " Actual_Value: 30000.0\n",
            "508: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 11000.0\n",
            "509: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 12000.0\n",
            "510: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 40000.0\n",
            "511: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "512: Predicted_value= 4988.158037925707\n",
            " Actual_Value: 14000.0\n",
            "513: Predicted_value= 5769.154726837536\n",
            " Actual_Value: 10000.0\n",
            "514: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "515: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "516: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 32000.0\n",
            "517: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 18000.0\n",
            "518: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 14000.0\n",
            "519: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 8000.0\n",
            "520: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 10000.0\n",
            "521: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 20000.0\n",
            "522: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "523: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 38000.0\n",
            "524: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 17000.0\n",
            "525: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "526: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 23000.0\n",
            "527: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 15000.0\n",
            "528: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 22000.0\n",
            "529: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 13000.0\n",
            "530: Predicted_value= 19063.747189928727\n",
            " Actual_Value: 14000.0\n",
            "531: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "532: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "533: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "534: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "535: Predicted_value= -3655.2317740184517\n",
            " Actual_Value: 18000.0\n",
            "536: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 30000.0\n",
            "537: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "538: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 11000.0\n",
            "539: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "540: Predicted_value= 72509.59414647511\n",
            " Actual_Value: 70000.0\n",
            "541: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "542: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 22000.0\n",
            "543: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "544: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 80000.0\n",
            "545: Predicted_value= 34683.68096816534\n",
            " Actual_Value: 20000.0\n",
            "546: Predicted_value= 2467.307540124246\n",
            " Actual_Value: 13200.0\n",
            "547: Predicted_value= 83621.4082223067\n",
            " Actual_Value: 120000.0\n",
            "548: Predicted_value= 156076.23986004098\n",
            " Actual_Value: 100000.0\n",
            "549: Predicted_value= 64521.76682629081\n",
            " Actual_Value: 35000.0\n",
            "550: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10500.0\n",
            "551: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 18000.0\n",
            "552: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "553: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 24000.0\n",
            "554: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 30000.0\n",
            "555: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 21000.0\n",
            "556: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "557: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "558: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 14000.0\n",
            "559: Predicted_value= 56711.7999371725\n",
            " Actual_Value: 25000.0\n",
            "560: Predicted_value= 22347.94562505196\n",
            " Actual_Value: 28000.0\n",
            "561: Predicted_value= 28049.221454108323\n",
            " Actual_Value: 16000.0\n",
            "562: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 16000.0\n",
            "563: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 16000.0\n",
            "564: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 42000.0\n",
            "565: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 40000.0\n",
            "566: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 18500.0\n",
            "567: Predicted_value= 16082.323362167248\n",
            " Actual_Value: 12000.0\n",
            "568: Predicted_value= 199031.05775019163\n",
            " Actual_Value: 220000.00000000003\n",
            "569: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "570: Predicted_value= 19666.883447774577\n",
            " Actual_Value: 10000.0\n",
            "571: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "572: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 35000.0\n",
            "573: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "574: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 35000.0\n",
            "575: Predicted_value= 123096.51849467811\n",
            " Actual_Value: 140000.0\n",
            "576: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 21000.0\n",
            "577: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "578: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 16000.0\n",
            "579: Predicted_value= 95761.63438276402\n",
            " Actual_Value: 80000.0\n",
            "580: Predicted_value= 26252.929069611113\n",
            " Actual_Value: 17000.0\n",
            "581: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 15000.0\n",
            "582: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 25000.0\n",
            "583: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 15000.0\n",
            "584: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 18000.0\n",
            "585: Predicted_value= 14182.257873801696\n",
            " Actual_Value: 9000.0\n",
            "586: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 35000.0\n",
            "587: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 40000.0\n",
            "588: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 16000.0\n",
            "589: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 16000.0\n",
            "590: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "591: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 30000.0\n",
            "592: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 30000.0\n",
            "593: Predicted_value= 4385.021780079856\n",
            " Actual_Value: 22000.0\n",
            "594: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "595: Predicted_value= 7509.008535727182\n",
            " Actual_Value: 20000.0\n",
            "596: Predicted_value= 6107.226837379429\n",
            " Actual_Value: 22000.0\n",
            "597: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "598: Predicted_value= 17822.17717105689\n",
            " Actual_Value: 30000.0\n",
            "599: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 30000.0\n",
            "600: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "601: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "602: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "603: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15000.0\n",
            "604: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 20000.0\n",
            "605: Predicted_value= 107476.58471644149\n",
            " Actual_Value: 80000.0\n",
            "606: Predicted_value= 39529.87278111224\n",
            " Actual_Value: 30000.0\n",
            "607: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "608: Predicted_value= 19302.058538295823\n",
            " Actual_Value: 19000.0\n",
            "609: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "610: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 14000.0\n",
            "611: Predicted_value= 4988.158037925707\n",
            " Actual_Value: 10000.0\n",
            "612: Predicted_value= 37347.094393852654\n",
            " Actual_Value: 28000.0\n",
            "613: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "614: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 21000.0\n",
            "615: Predicted_value= 45157.06128297096\n",
            " Actual_Value: 27000.0\n",
            "616: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "617: Predicted_value= 141.96622497881617\n",
            " Actual_Value: 10000.0\n",
            "618: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 50000.0\n",
            "619: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15000.0\n",
            "620: Predicted_value= 37026.67103490083\n",
            " Actual_Value: 50000.0\n",
            "621: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "622: Predicted_value= 222460.95841754653\n",
            " Actual_Value: 180000.0\n",
            "623: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 10500.0\n",
            "624: Predicted_value= 35447.028905487095\n",
            " Actual_Value: 18000.0\n",
            "625: Predicted_value= 32110.404236449845\n",
            " Actual_Value: 20000.0\n",
            "626: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 24000.0\n",
            "627: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 10000.0\n",
            "628: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "629: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 14000.0\n",
            "630: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 25000.0\n",
            "631: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10500.0\n",
            "632: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 24000.0\n",
            "633: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "634: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "635: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 18000.0\n",
            "636: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16500.0\n",
            "637: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 31000.0\n",
            "638: Predicted_value= 64681.97850576672\n",
            " Actual_Value: 45000.0\n",
            "639: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 16000.0\n",
            "640: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 35000.0\n",
            "641: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "642: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 21000.0\n",
            "643: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "644: Predicted_value= 142781.6473969498\n",
            " Actual_Value: 60000.0\n",
            "645: Predicted_value= 91271.1634319491\n",
            " Actual_Value: 70000.0\n",
            "646: Predicted_value= 128741.3557481269\n",
            " Actual_Value: 150000.0\n",
            "647: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "648: Predicted_value= 12177.339917608093\n",
            " Actual_Value: 23000.0\n",
            "649: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 25000.0\n",
            "650: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "651: Predicted_value= 74071.58752429875\n",
            " Actual_Value: 45000.0\n",
            "652: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "653: Predicted_value= 46554.830639625165\n",
            " Actual_Value: 40000.0\n",
            "654: Predicted_value= 302.177904454722\n",
            " Actual_Value: 16000.0\n",
            "655: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 22000.0\n",
            "656: Predicted_value= 18052.46383603689\n",
            " Actual_Value: 30000.0\n",
            "657: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "658: Predicted_value= 205261.3825098962\n",
            " Actual_Value: 300000.0\n",
            "659: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 14000.0\n",
            "660: Predicted_value= 24513.075260721474\n",
            " Actual_Value: 12000.0\n",
            "661: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "662: Predicted_value= 43200.55721899784\n",
            " Actual_Value: 35000.0\n",
            "663: Predicted_value= 6728.011846815345\n",
            " Actual_Value: 20000.0\n",
            "664: Predicted_value= 56711.7999371725\n",
            " Actual_Value: 35000.0\n",
            "665: Predicted_value= 17484.105060514994\n",
            " Actual_Value: 10000.0\n",
            "666: Predicted_value= 48741.621368578286\n",
            " Actual_Value: 16000.0\n",
            "667: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 10000.0\n",
            "668: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 60000.0\n",
            "669: Predicted_value= 23046.830303379065\n",
            " Actual_Value: 15000.0\n",
            "670: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "671: Predicted_value= 14182.257873801696\n",
            " Actual_Value: 9500.0\n",
            "672: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 20000.0\n",
            "673: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "674: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 15000.0\n",
            "675: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "676: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 26000.0\n",
            "677: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11000.0\n",
            "678: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 18000.0\n",
            "679: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "680: Predicted_value= 48901.8330480542\n",
            " Actual_Value: 16500.0\n",
            "681: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "682: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "683: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "684: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "685: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 20000.0\n",
            "686: Predicted_value= 27797.273695844706\n",
            " Actual_Value: 24000.0\n",
            "687: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 14000.0\n",
            "688: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 11500.0\n",
            "689: Predicted_value= 37347.094393852654\n",
            " Actual_Value: 30000.0\n",
            "690: Predicted_value= 23128.94231396379\n",
            " Actual_Value: 17000.0\n",
            "691: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 16000.0\n",
            "692: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 15000.0\n",
            "693: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 22000.0\n",
            "694: Predicted_value= 14182.257873801696\n",
            " Actual_Value: 9500.0\n",
            "695: Predicted_value= 7951.933114097119\n",
            " Actual_Value: 10000.0\n",
            "696: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 28000.0\n",
            "697: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "698: Predicted_value= 1421.2467039084513\n",
            " Actual_Value: 16000.0\n",
            "699: Predicted_value= 101566.68331568874\n",
            " Actual_Value: 80000.0\n",
            "700: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 27000.0\n",
            "701: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 28000.0\n",
            "702: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 17000.0\n",
            "703: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "704: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 17000.0\n",
            "705: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "706: Predicted_value= 4224.810100603943\n",
            " Actual_Value: 14000.0\n",
            "707: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 20000.0\n",
            "708: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 15500.0\n",
            "709: Predicted_value= 105311.455080772\n",
            " Actual_Value: 70000.0\n",
            "710: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 15000.0\n",
            "711: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 15500.0\n",
            "712: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 35000.0\n",
            "713: Predicted_value= 10632.9952913745\n",
            " Actual_Value: 18000.0\n",
            "714: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "715: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 15000.0\n",
            "716: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 14000.0\n",
            "717: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 10000.0\n",
            "718: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "719: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 55000.0\n",
            "720: Predicted_value= 123096.51849467811\n",
            " Actual_Value: 100000.0\n",
            "721: Predicted_value= 109376.65020480705\n",
            " Actual_Value: 50000.0\n",
            "722: Predicted_value= 11804.490324742248\n",
            " Actual_Value: 20000.0\n",
            "723: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 10000.0\n",
            "724: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 28000.0\n",
            "725: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "726: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "727: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 12000.0\n",
            "728: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 20000.0\n",
            "729: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 13000.0\n",
            "730: Predicted_value= 16330.25877873732\n",
            " Actual_Value: 15000.0\n",
            "731: Predicted_value= 29980.052083104285\n",
            " Actual_Value: 13000.0\n",
            "732: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15000.0\n",
            "733: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 25000.0\n",
            "734: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "735: Predicted_value= 35021.75307870723\n",
            " Actual_Value: 45000.0\n",
            "736: Predicted_value= 68604.61070191595\n",
            " Actual_Value: 36000.0\n",
            "737: Predicted_value= 81721.34273394116\n",
            " Actual_Value: 45000.0\n",
            "738: Predicted_value= 167791.19019371842\n",
            " Actual_Value: 260000.0\n",
            "739: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 21000.0\n",
            "740: Predicted_value= 115286.5516055598\n",
            " Actual_Value: 150000.0\n",
            "741: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "742: Predicted_value= 302.177904454722\n",
            " Actual_Value: 14000.0\n",
            "743: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 18000.0\n",
            "744: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 10000.0\n",
            "745: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 22000.0\n",
            "746: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 16000.0\n",
            "747: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "748: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "749: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 12000.0\n",
            "750: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "751: Predicted_value= 64521.76682629081\n",
            " Actual_Value: 40000.0\n",
            "752: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 29500.0\n",
            "753: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 14500.0\n",
            "754: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "755: Predicted_value= 36093.48734990018\n",
            " Actual_Value: 20000.0\n",
            "756: Predicted_value= 27424.42410297886\n",
            " Actual_Value: 25000.0\n",
            "757: Predicted_value= 58273.79331499617\n",
            " Actual_Value: 45000.0\n",
            "758: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "759: Predicted_value= 107476.58471644149\n",
            " Actual_Value: 45000.0\n",
            "760: Predicted_value= 23571.866892333725\n",
            " Actual_Value: 10000.0\n",
            "761: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "762: Predicted_value= 6728.011846815345\n",
            " Actual_Value: 16000.0\n",
            "763: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "764: Predicted_value= 58451.65374606215\n",
            " Actual_Value: 50000.0\n",
            "765: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "766: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 45000.0\n",
            "767: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 12000.0\n",
            "768: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 50000.0\n",
            "769: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 35000.0\n",
            "770: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 17000.0\n",
            "771: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "772: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "773: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "774: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 11000.0\n",
            "775: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 13000.0\n",
            "776: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "777: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "778: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 70000.0\n",
            "779: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 18000.0\n",
            "780: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "781: Predicted_value= 245890.8590849015\n",
            " Actual_Value: 360000.0\n",
            "782: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 14500.0\n",
            "783: Predicted_value= 56711.7999371725\n",
            " Actual_Value: 27000.0\n",
            "784: Predicted_value= 52967.02817208927\n",
            " Actual_Value: 23000.0\n",
            "785: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 18000.0\n",
            "786: Predicted_value= 9674.138171396691\n",
            " Actual_Value: 18000.0\n",
            "787: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 15000.0\n",
            "788: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "789: Predicted_value= 109056.22684585521\n",
            " Actual_Value: 80000.0\n",
            "790: Predicted_value= 35447.028905487095\n",
            " Actual_Value: 15000.0\n",
            "791: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "792: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "793: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 25000.0\n",
            "794: Predicted_value= 302.177904454722\n",
            " Actual_Value: 14500.0\n",
            "795: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "796: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 12000.0\n",
            "797: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 20000.0\n",
            "798: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "799: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "800: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 19000.0\n",
            "801: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 25000.0\n",
            "802: Predicted_value= 7018.749429096468\n",
            " Actual_Value: 15000.0\n",
            "803: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 23000.0\n",
            "804: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 15500.0\n",
            "805: Predicted_value= 18442.96218049281\n",
            " Actual_Value: 35000.0\n",
            "806: Predicted_value= 15318.975424845485\n",
            " Actual_Value: 30000.0\n",
            "807: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 31000.0\n",
            "808: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 17000.0\n",
            "809: Predicted_value= 14182.257873801696\n",
            " Actual_Value: 9000.0\n",
            "810: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "811: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 28000.0\n",
            "812: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 18000.0\n",
            "813: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11500.0\n",
            "814: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "815: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "816: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 100000.0\n",
            "817: Predicted_value= 29537.12750473435\n",
            " Actual_Value: 30000.0\n",
            "818: Predicted_value= 26643.42741406703\n",
            " Actual_Value: 20000.0\n",
            "819: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 8500.0\n",
            "820: Predicted_value= 95761.63438276402\n",
            " Actual_Value: 35000.0\n",
            "821: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 17000.0\n",
            "822: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "823: Predicted_value= 48741.621368578286\n",
            " Actual_Value: 19000.0\n",
            "824: Predicted_value= 64521.76682629081\n",
            " Actual_Value: 35000.0\n",
            "825: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 12000.0\n",
            "826: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 13500.0\n",
            "827: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 15000.0\n",
            "828: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 23000.0\n",
            "829: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "830: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 65000.0\n",
            "831: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 20000.0\n",
            "832: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "833: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "834: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 35000.0\n",
            "835: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "836: Predicted_value= 62569.27510401124\n",
            " Actual_Value: 45000.0\n",
            "837: Predicted_value= 31407.507216429196\n",
            " Actual_Value: 20000.0\n",
            "838: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 50000.0\n",
            "839: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "840: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 21000.0\n",
            "841: Predicted_value= 191221.09086107332\n",
            " Actual_Value: 450000.0\n",
            "842: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 13000.0\n",
            "843: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "844: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "845: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "846: Predicted_value= 117186.61709392536\n",
            " Actual_Value: 60000.0\n",
            "847: Predicted_value= 142781.6473969498\n",
            " Actual_Value: 114999.99999999999\n",
            "848: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 20000.0\n",
            "849: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "850: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 18000.0\n",
            "851: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 15000.0\n",
            "852: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 14000.0\n",
            "853: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 19000.0\n",
            "854: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 35000.0\n",
            "855: Predicted_value= 40931.65447945998\n",
            " Actual_Value: 14000.0\n",
            "856: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 80000.0\n",
            "857: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "858: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 17000.0\n",
            "859: Predicted_value= 88111.87917312165\n",
            " Actual_Value: 60000.0\n",
            "860: Predicted_value= 140296.09440232845\n",
            " Actual_Value: 70000.0\n",
            "861: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "862: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 12500.0\n",
            "863: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 24000.0\n",
            "864: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10500.0\n",
            "865: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 12000.0\n",
            "866: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 20000.0\n",
            "867: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 11000.0\n",
            "868: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 14000.0\n",
            "869: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "870: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 12000.0\n",
            "871: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 11000.0\n",
            "872: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 30000.0\n",
            "873: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "874: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 22000.0\n",
            "875: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 15000.0\n",
            "876: Predicted_value= 39920.37112556815\n",
            " Actual_Value: 35000.0\n",
            "877: Predicted_value= 74987.12245770934\n",
            " Actual_Value: 50000.0\n",
            "878: Predicted_value= 27732.810436850046\n",
            " Actual_Value: 15500.0\n",
            "879: Predicted_value= 95761.63438276402\n",
            " Actual_Value: 45000.0\n",
            "880: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 35000.0\n",
            "881: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "882: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 20000.0\n",
            "883: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10000.0\n",
            "884: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "885: Predicted_value= 64681.97850576672\n",
            " Actual_Value: 60000.0\n",
            "886: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 14000.0\n",
            "887: Predicted_value= 28808.557049736537\n",
            " Actual_Value: 16000.0\n",
            "888: Predicted_value= 63059.534210641956\n",
            " Actual_Value: 50000.0\n",
            "889: Predicted_value= 70166.6040797396\n",
            " Actual_Value: 35000.0\n",
            "890: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 20000.0\n",
            "891: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 15000.0\n",
            "892: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 22000.0\n",
            "893: Predicted_value= 105367.89365637957\n",
            " Actual_Value: 90000.0\n",
            "894: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 30000.0\n",
            "895: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "896: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "897: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 12000.0\n",
            "898: Predicted_value= 128741.3557481269\n",
            " Actual_Value: 229999.99999999997\n",
            "899: Predicted_value= 1098438.5488526833\n",
            " Actual_Value: 1660000.0000000002\n",
            "900: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "901: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 11500.0\n",
            "902: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 18000.0\n",
            "903: Predicted_value= 27211.786189588925\n",
            " Actual_Value: 35000.0\n",
            "904: Predicted_value= 154336.38605115132\n",
            " Actual_Value: 250000.0\n",
            "905: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 9500.0\n",
            "906: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 16000.0\n",
            "907: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 11000.0\n",
            "908: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 17000.0\n",
            "909: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "910: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 14000.0\n",
            "911: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 30000.0\n",
            "912: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 15000.0\n",
            "913: Predicted_value= 26413.140749087026\n",
            " Actual_Value: 25000.0\n",
            "914: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 14000.0\n",
            "915: Predicted_value= 26252.929069611113\n",
            " Actual_Value: 20000.0\n",
            "916: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 12500.0\n",
            "917: Predicted_value= 4224.810100603943\n",
            " Actual_Value: 11300.0\n",
            "918: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 20000.0\n",
            "919: Predicted_value= 113121.4219698903\n",
            " Actual_Value: 130000.0\n",
            "920: Predicted_value= 64521.76682629081\n",
            " Actual_Value: 50000.0\n",
            "921: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 10000.0\n",
            "922: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 20000.0\n",
            "923: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 19000.0\n",
            "924: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 32000.0\n",
            "925: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "926: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "927: Predicted_value= 19401.819300470623\n",
            " Actual_Value: 25000.0\n",
            "928: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 25000.0\n",
            "929: Predicted_value= 101566.68331568874\n",
            " Actual_Value: 100000.0\n",
            "930: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 32000.0\n",
            "931: Predicted_value= 111541.77984047655\n",
            " Actual_Value: 50000.0\n",
            "932: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 15000.0\n",
            "933: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "934: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "935: Predicted_value= -5477.19759349282\n",
            " Actual_Value: 12000.0\n",
            "936: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 12000.0\n",
            "937: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 11000.0\n",
            "938: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 25000.0\n",
            "939: Predicted_value= 99666.61782732318\n",
            " Actual_Value: 42000.0\n",
            "940: Predicted_value= 50641.68685694384\n",
            " Actual_Value: 35000.0\n",
            "941: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 14000.0\n",
            "942: Predicted_value= 47161.97923916456\n",
            " Actual_Value: 11500.0\n",
            "943: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "944: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 15000.0\n",
            "945: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11500.0\n",
            "946: Predicted_value= 84046.68404908657\n",
            " Actual_Value: 35000.0\n",
            "947: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "948: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 16000.0\n",
            "949: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 14000.0\n",
            "950: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 60000.0\n",
            "951: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 17000.0\n",
            "952: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "953: Predicted_value= 15761.900003215414\n",
            " Actual_Value: 10000.0\n",
            "954: Predicted_value= 41872.862847847726\n",
            " Actual_Value: 36000.0\n",
            "955: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 35000.0\n",
            "956: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 40000.0\n",
            "957: Predicted_value= 88111.87917312165\n",
            " Actual_Value: 40000.0\n",
            "958: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 16000.0\n",
            "959: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 16000.0\n",
            "960: Predicted_value= 38926.73652326639\n",
            " Actual_Value: 50000.0\n",
            "961: Predicted_value= 74071.58752429875\n",
            " Actual_Value: 30000.0\n",
            "962: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "963: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 32000.0\n",
            "964: Predicted_value= 84046.68404908657\n",
            " Actual_Value: 24000.0\n",
            "965: Predicted_value= 2467.307540124246\n",
            " Actual_Value: 15000.0\n",
            "966: Predicted_value= 54706.88198097891\n",
            " Actual_Value: 110000.00000000001\n",
            "967: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "968: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 25000.0\n",
            "969: Predicted_value= 7951.933114097119\n",
            " Actual_Value: 12000.0\n",
            "970: Predicted_value= 65693.26185965855\n",
            " Actual_Value: 80000.0\n",
            "971: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "972: Predicted_value= 6550.151415749366\n",
            " Actual_Value: 18000.0\n",
            "973: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 90000.0\n",
            "974: Predicted_value= 84046.68404908657\n",
            " Actual_Value: 50000.0\n",
            "975: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "976: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 30000.0\n",
            "977: Predicted_value= 52967.02817208927\n",
            " Actual_Value: 23000.0\n",
            "978: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 30000.0\n",
            "979: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 17000.0\n",
            "980: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 9000.0\n",
            "981: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 28000.0\n",
            "982: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "983: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "984: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 140000.0\n",
            "985: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "986: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 13000.0\n",
            "987: Predicted_value= 19223.95886940464\n",
            " Actual_Value: 14000.0\n",
            "988: Predicted_value= 107476.58471644149\n",
            " Actual_Value: 70000.0\n",
            "989: Predicted_value= 30318.124193646177\n",
            " Actual_Value: 50000.0\n",
            "990: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "991: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "992: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11500.0\n",
            "993: Predicted_value= 31719.905891993927\n",
            " Actual_Value: 25000.0\n",
            "994: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "995: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 42000.0\n",
            "996: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "997: Predicted_value= 302.177904454722\n",
            " Actual_Value: 16000.0\n",
            "998: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 20000.0\n",
            "999: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 20000.0\n",
            "1000: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 18000.0\n",
            "1001: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 31000.0\n",
            "1002: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 14000.0\n",
            "1003: Predicted_value= 105367.89365637957\n",
            " Actual_Value: 90000.0\n",
            "1004: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 17000.0\n",
            "1005: Predicted_value= 345487.1457353185\n",
            " Actual_Value: 350000.0\n",
            "1006: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 15000.0\n",
            "1007: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 27000.0\n",
            "1008: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 11000.0\n",
            "1009: Predicted_value= 14772.277742607206\n",
            " Actual_Value: 20000.0\n",
            "1010: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 15000.0\n",
            "1011: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13500.0\n",
            "1012: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 12000.0\n",
            "1013: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 20000.0\n",
            "1014: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 20000.0\n",
            "1015: Predicted_value= 26252.929069611113\n",
            " Actual_Value: 17000.0\n",
            "1016: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1017: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 22000.0\n",
            "1018: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "1019: Predicted_value= 4029.3009179478904\n",
            " Actual_Value: 10000.0\n",
            "1020: Predicted_value= -9672.91862033309\n",
            " Actual_Value: 14000.0\n",
            "1021: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1022: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1023: Predicted_value= 141762.33935967085\n",
            " Actual_Value: 100000.0\n",
            "1024: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 16000.0\n",
            "1025: Predicted_value= 10012.210281938584\n",
            " Actual_Value: 30000.0\n",
            "1026: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 20000.0\n",
            "1027: Predicted_value= 40931.65447945998\n",
            " Actual_Value: 21000.0\n",
            "1028: Predicted_value= 39352.01235004625\n",
            " Actual_Value: 18000.0\n",
            "1029: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 15500.0\n",
            "1030: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "1031: Predicted_value= 209006.15427497946\n",
            " Actual_Value: 280000.0\n",
            "1032: Predicted_value= 12194.988669198166\n",
            " Actual_Value: 20000.0\n",
            "1033: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 14000.0\n",
            "1034: Predicted_value= 41252.0778384118\n",
            " Actual_Value: 30000.0\n",
            "1035: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 16000.0\n",
            "1036: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 15000.0\n",
            "1037: Predicted_value= 70006.39240026369\n",
            " Actual_Value: 45000.0\n",
            "1038: Predicted_value= 18087.241318360844\n",
            " Actual_Value: 13500.0\n",
            "1039: Predicted_value= 34062.89595872942\n",
            " Actual_Value: 25000.0\n",
            "1040: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 40000.0\n",
            "1041: Predicted_value= 302.177904454722\n",
            " Actual_Value: 18000.0\n",
            "1042: Predicted_value= 31798.00556088511\n",
            " Actual_Value: 25500.0\n",
            "1043: Predicted_value= 52967.02817208927\n",
            " Actual_Value: 35000.0\n",
            "1044: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "1045: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "1046: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 24000.0\n",
            "1047: Predicted_value= 15318.975424845485\n",
            " Actual_Value: 21000.0\n",
            "1048: Predicted_value= 86958.03289134396\n",
            " Actual_Value: 75000.0\n",
            "1049: Predicted_value= 222460.95841754653\n",
            " Actual_Value: 280000.0\n",
            "1050: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18600.0\n",
            "1051: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 10000.0\n",
            "1052: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 22000.0\n",
            "1053: Predicted_value= 18087.241318360844\n",
            " Actual_Value: 9000.0\n",
            "1054: Predicted_value= 18655.600093882742\n",
            " Actual_Value: 21000.0\n",
            "1055: Predicted_value= 622.6012634065482\n",
            " Actual_Value: 18000.0\n",
            "1056: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "1057: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 35000.0\n",
            "1058: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "1059: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 21000.0\n",
            "1060: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 23000.0\n",
            "1061: Predicted_value= 138113.31601506888\n",
            " Actual_Value: 135000.0\n",
            "1062: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 10000.0\n",
            "1063: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 22000.0\n",
            "1064: Predicted_value= 64521.76682629081\n",
            " Actual_Value: 50000.0\n",
            "1065: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "1066: Predicted_value= 81881.55441341706\n",
            " Actual_Value: 45000.0\n",
            "1067: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 24000.0\n",
            "1068: Predicted_value= 87951.66749364573\n",
            " Actual_Value: 45000.0\n",
            "1069: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "1070: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 22000.0\n",
            "1071: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "1072: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 15000.0\n",
            "1073: Predicted_value= 89513.66087146939\n",
            " Actual_Value: 60000.0\n",
            "1074: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 35000.0\n",
            "1075: Predicted_value= 43096.78411512949\n",
            " Actual_Value: 12000.0\n",
            "1076: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 100000.0\n",
            "1077: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 120000.0\n",
            "1078: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "1079: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "1080: Predicted_value= 6159.653071293455\n",
            " Actual_Value: 12000.0\n",
            "1081: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 18000.0\n",
            "1082: Predicted_value= 48901.8330480542\n",
            " Actual_Value: 55000.0\n",
            "1083: Predicted_value= 64681.97850576672\n",
            " Actual_Value: 35000.0\n",
            "1084: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 15000.0\n",
            "1085: Predicted_value= 7951.933114097119\n",
            " Actual_Value: 15000.0\n",
            "1086: Predicted_value= 120931.38885900861\n",
            " Actual_Value: 75000.0\n",
            "1087: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 11000.0\n",
            "1088: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 16000.0\n",
            "1089: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1090: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11500.0\n",
            "1091: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1092: Predicted_value= 90472.51799144721\n",
            " Actual_Value: 80000.0\n",
            "1093: Predicted_value= 41091.866158935896\n",
            " Actual_Value: 50000.0\n",
            "1094: Predicted_value= 103571.60127188233\n",
            " Actual_Value: 75000.0\n",
            "1095: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "1096: Predicted_value= 22187.73394557605\n",
            " Actual_Value: 12000.0\n",
            "1097: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "1098: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1099: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 12000.0\n",
            "1100: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 20000.0\n",
            "1101: Predicted_value= 48901.8330480542\n",
            " Actual_Value: 18000.0\n",
            "1102: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 20000.0\n",
            "1103: Predicted_value= 39299.58611613223\n",
            " Actual_Value: 32000.0\n",
            "1104: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 18000.0\n",
            "1105: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 25000.0\n",
            "1106: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "1107: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 20000.0\n",
            "1108: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 14000.0\n",
            "1109: Predicted_value= 45617.63461293097\n",
            " Actual_Value: 65000.0\n",
            "1110: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 70000.0\n",
            "1111: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "1112: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 13000.0\n",
            "1113: Predicted_value= 47105.54066355699\n",
            " Actual_Value: 30000.0\n",
            "1114: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 60000.0\n",
            "1115: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1116: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "1117: Predicted_value= 6550.151415749366\n",
            " Actual_Value: 18000.0\n",
            "1118: Predicted_value= 75473.36922264652\n",
            " Actual_Value: 45000.0\n",
            "1119: Predicted_value= 131140.78439046995\n",
            " Actual_Value: 100000.0\n",
            "1120: Predicted_value= 35021.75307870723\n",
            " Actual_Value: 35000.0\n",
            "1121: Predicted_value= 15088.688759865483\n",
            " Actual_Value: 30000.0\n",
            "1122: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "1123: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 13000.0\n",
            "1124: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 14000.0\n",
            "1125: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 17000.0\n",
            "1126: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 11000.0\n",
            "1127: Predicted_value= 89105.5137754234\n",
            " Actual_Value: 60000.0\n",
            "1128: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 30000.0\n",
            "1129: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 35000.0\n",
            "1130: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 20000.0\n",
            "1131: Predicted_value= 45777.84629240688\n",
            " Actual_Value: 40000.0\n",
            "1132: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "1133: Predicted_value= 35624.88933655308\n",
            " Actual_Value: 25000.0\n",
            "1134: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "1135: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 26000.0\n",
            "1136: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 9000.0\n",
            "1137: Predicted_value= 15496.835855911471\n",
            " Actual_Value: 20000.0\n",
            "1138: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 14000.0\n",
            "1139: Predicted_value= 19046.09843833866\n",
            " Actual_Value: 10000.0\n",
            "1140: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 15500.0\n",
            "1141: Predicted_value= 14963.254562713519\n",
            " Actual_Value: 7000.0\n",
            "1142: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "1143: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 12000.0\n",
            "1144: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 12000.0\n",
            "1145: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "1146: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 11000.0\n",
            "1147: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 13000.0\n",
            "1148: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "1149: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 23000.0\n",
            "1150: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 15000.0\n",
            "1151: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 45000.0\n",
            "1152: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 9000.0\n",
            "1153: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "1154: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 25000.0\n",
            "1155: Predicted_value= 31542.045460927948\n",
            " Actual_Value: 17000.0\n",
            "1156: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 30000.0\n",
            "1157: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "1158: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 20000.0\n",
            "1159: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 21000.0\n",
            "1160: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 13000.0\n",
            "1161: Predicted_value= 10012.210281938584\n",
            " Actual_Value: 30000.0\n",
            "1162: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 40000.0\n",
            "1163: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 14000.0\n",
            "1164: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 11000.0\n",
            "1165: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 14000.0\n",
            "1166: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 28000.0\n",
            "1167: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 40000.0\n",
            "1168: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1169: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 17000.0\n",
            "1170: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 13000.0\n",
            "1171: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1172: Predicted_value= 6728.011846815345\n",
            " Actual_Value: 15000.0\n",
            "1173: Predicted_value= 127161.71361871317\n",
            " Actual_Value: 110000.00000000001\n",
            "1174: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 60000.0\n",
            "1175: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 40000.0\n",
            "1176: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 30000.0\n",
            "1177: Predicted_value= 11023.493635830418\n",
            " Actual_Value: 22000.0\n",
            "1178: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "1179: Predicted_value= 68426.75027084997\n",
            " Actual_Value: 50000.0\n",
            "1180: Predicted_value= 19666.883447774577\n",
            " Actual_Value: 8000.0\n",
            "1181: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 15000.0\n",
            "1182: Predicted_value= 84046.68404908657\n",
            " Actual_Value: 60000.0\n",
            "1183: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 13000.0\n",
            "1184: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1185: Predicted_value= 89691.52130253537\n",
            " Actual_Value: 40000.0\n",
            "1186: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 20000.0\n",
            "1187: Predicted_value= 17822.17717105689\n",
            " Actual_Value: 22000.0\n",
            "1188: Predicted_value= 17896.264498254524\n",
            " Actual_Value: 30000.0\n",
            "1189: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 35000.0\n",
            "1190: Predicted_value= 9513.926491920778\n",
            " Actual_Value: 11000.0\n",
            "1191: Predicted_value= 49608.7424097684\n",
            " Actual_Value: 47000.0\n",
            "1192: Predicted_value= 126255.80275350556\n",
            " Actual_Value: 120000.0\n",
            "1193: Predicted_value= 33707.17509659746\n",
            " Actual_Value: 5500.0\n",
            "1194: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 19000.0\n",
            "1195: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 13000.0\n",
            "1196: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "1197: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 10000.0\n",
            "1198: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "1199: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 25000.0\n",
            "1200: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 17000.0\n",
            "1201: Predicted_value= 7526.657287317252\n",
            " Actual_Value: 22000.0\n",
            "1202: Predicted_value= 20004.95555831647\n",
            " Actual_Value: 20000.0\n",
            "1203: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 14000.0\n",
            "1204: Predicted_value= 74071.58752429875\n",
            " Actual_Value: 45000.0\n",
            "1205: Predicted_value= 23571.866892333725\n",
            " Actual_Value: 15000.0\n",
            "1206: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "1207: Predicted_value= 302.177904454722\n",
            " Actual_Value: 22000.0\n",
            "1208: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 20000.0\n",
            "1209: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "1210: Predicted_value= 111168.93024761073\n",
            " Actual_Value: 160000.0\n",
            "1211: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 70000.0\n",
            "1212: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 19000.0\n",
            "1213: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 16000.0\n",
            "1214: Predicted_value= 103731.81295135827\n",
            " Actual_Value: 150000.0\n",
            "1215: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "1216: Predicted_value= 43200.55721899784\n",
            " Actual_Value: 35000.0\n",
            "1217: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "1218: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 19000.0\n",
            "1219: Predicted_value= 17822.17717105689\n",
            " Actual_Value: 30000.0\n",
            "1220: Predicted_value= 50641.68685694384\n",
            " Actual_Value: 100000.0\n",
            "1221: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 20000.0\n",
            "1222: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1223: Predicted_value= 17484.105060514994\n",
            " Actual_Value: 16500.0\n",
            "1224: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 25000.0\n",
            "1225: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 15000.0\n",
            "1226: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11800.0\n",
            "1227: Predicted_value= 17822.17717105689\n",
            " Actual_Value: 30000.0\n",
            "1228: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 15000.0\n",
            "1229: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "1230: Predicted_value= 138113.31601506888\n",
            " Actual_Value: 250000.0\n",
            "1231: Predicted_value= 14199.906625391763\n",
            " Actual_Value: 10500.0\n",
            "1232: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 18000.0\n",
            "1233: Predicted_value= 319.8266560447955\n",
            " Actual_Value: 17000.0\n",
            "1234: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 14000.0\n",
            "1235: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 19000.0\n",
            "1236: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "1237: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 50000.0\n",
            "1238: Predicted_value= 27814.922447434776\n",
            " Actual_Value: 18000.0\n",
            "1239: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1240: Predicted_value= 68426.75027084997\n",
            " Actual_Value: 40000.0\n",
            "1241: Predicted_value= 101566.68331568874\n",
            " Actual_Value: 80000.0\n",
            "1242: Predicted_value= 56711.7999371725\n",
            " Actual_Value: 27000.0\n",
            "1243: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 85000.0\n",
            "1244: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 28000.0\n",
            "1245: Predicted_value= 74674.7237821446\n",
            " Actual_Value: 59000.0\n",
            "1246: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 40000.0\n",
            "1247: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 15500.0\n",
            "1248: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 14500.0\n",
            "1249: Predicted_value= 212485.86189275875\n",
            " Actual_Value: 250000.0\n",
            "1250: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 14000.0\n",
            "1251: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 40000.0\n",
            "1252: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 16000.0\n",
            "1253: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10000.0\n",
            "1254: Predicted_value= 29216.704145782525\n",
            " Actual_Value: 26000.0\n",
            "1255: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 17000.0\n",
            "1256: Predicted_value= 29537.12750473435\n",
            " Actual_Value: 20000.0\n",
            "1257: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "1258: Predicted_value= 41252.0778384118\n",
            " Actual_Value: 50000.0\n",
            "1259: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 60000.0\n",
            "1260: Predicted_value= 1881.8200338684546\n",
            " Actual_Value: 19000.0\n",
            "1261: Predicted_value= 39529.87278111224\n",
            " Actual_Value: 30000.0\n",
            "1262: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 27000.0\n",
            "1263: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 12000.0\n",
            "1264: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 12000.0\n",
            "1265: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 80000.0\n",
            "1266: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 20000.0\n",
            "1267: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 18000.0\n",
            "1268: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "1269: Predicted_value= 64681.97850576672\n",
            " Actual_Value: 88000.0\n",
            "1270: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "1271: Predicted_value= 24690.935691787454\n",
            " Actual_Value: 22000.0\n",
            "1272: Predicted_value= 102560.31791799053\n",
            " Actual_Value: 90000.0\n",
            "1273: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1274: Predicted_value= 23909.939002875624\n",
            " Actual_Value: 20000.0\n",
            "1275: Predicted_value= 11804.490324742248\n",
            " Actual_Value: 20000.0\n",
            "1276: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 80000.0\n",
            "1277: Predicted_value= 46949.34132577462\n",
            " Actual_Value: 26000.0\n",
            "1278: Predicted_value= 4597.659693469788\n",
            " Actual_Value: 13000.0\n",
            "1279: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "1280: Predicted_value= -15317.755873781884\n",
            " Actual_Value: 11000.0\n",
            "1281: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10000.0\n",
            "1282: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 35000.0\n",
            "1283: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 40000.0\n",
            "1284: Predicted_value= 16880.96880266915\n",
            " Actual_Value: 20000.0\n",
            "1285: Predicted_value= 91856.65093820488\n",
            " Actual_Value: 75000.0\n",
            "1286: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 45000.0\n",
            "1287: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 16000.0\n",
            "1288: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 21000.0\n",
            "1289: Predicted_value= 68426.75027084997\n",
            " Actual_Value: 40000.0\n",
            "1290: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "1291: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "1292: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 10000.0\n",
            "1293: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 15000.0\n",
            "1294: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 26000.0\n",
            "1295: Predicted_value= 27637.062016368796\n",
            " Actual_Value: 15000.0\n",
            "1296: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "1297: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 32000.0\n",
            "1298: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 18000.0\n",
            "1299: Predicted_value= -1862.9517312147873\n",
            " Actual_Value: 14000.0\n",
            "1300: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 19000.0\n",
            "1301: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 16000.0\n",
            "1302: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 27000.0\n",
            "1303: Predicted_value= 20629.752909445935\n",
            " Actual_Value: 18000.0\n",
            "1304: Predicted_value= 72491.94539488503\n",
            " Actual_Value: 45000.0\n",
            "1305: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 15500.0\n",
            "1306: Predicted_value= 142196.15989069402\n",
            " Actual_Value: 130000.0\n",
            "1307: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 35000.0\n",
            "1308: Predicted_value= 95921.84606223996\n",
            " Actual_Value: 150000.0\n",
            "1309: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 37000.0\n",
            "1310: Predicted_value= 46715.04231910108\n",
            " Actual_Value: 19000.0\n",
            "1311: Predicted_value= 13596.770367545916\n",
            " Actual_Value: 14000.0\n",
            "1312: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 20000.0\n",
            "1313: Predicted_value= 46558.84298131871\n",
            " Actual_Value: 30000.0\n",
            "1314: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 17000.0\n",
            "1315: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 12000.0\n",
            "1316: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 21000.0\n",
            "1317: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 25000.0\n",
            "1318: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 11000.0\n",
            "1319: Predicted_value= 75065.22212660054\n",
            " Actual_Value: 20000.0\n",
            "1320: Predicted_value= 54386.45862202709\n",
            " Actual_Value: 18000.0\n",
            "1321: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 17000.0\n",
            "1322: Predicted_value= 19063.747189928727\n",
            " Actual_Value: 13000.0\n",
            "1323: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "1324: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 12000.0\n",
            "1325: Predicted_value= 1864.1712822783884\n",
            " Actual_Value: 16000.0\n",
            "1326: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "1327: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 13000.0\n",
            "1328: Predicted_value= 22191.746287269598\n",
            " Actual_Value: 20000.0\n",
            "1329: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 14000.0\n",
            "1330: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 9500.0\n",
            "1331: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 16000.0\n",
            "1332: Predicted_value= 7509.008535727182\n",
            " Actual_Value: 16000.0\n",
            "1333: Predicted_value= 302.177904454722\n",
            " Actual_Value: 12000.0\n",
            "1334: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 35000.0\n",
            "1335: Predicted_value= 52221.32898635758\n",
            " Actual_Value: 65000.0\n",
            "1336: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 35000.0\n",
            "1337: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 25000.0\n",
            "1338: Predicted_value= 28418.058705280626\n",
            " Actual_Value: 10000.0\n",
            "1339: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1340: Predicted_value= 41482.36450339181\n",
            " Actual_Value: 33000.0\n",
            "1341: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "1342: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "1343: Predicted_value= 76396.92883944418\n",
            " Actual_Value: 50000.0\n",
            "1344: Predicted_value= 302.177904454722\n",
            " Actual_Value: 16000.0\n",
            "1345: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "1346: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 20000.0\n",
            "1347: Predicted_value= 103731.81295135827\n",
            " Actual_Value: 110000.00000000001\n",
            "1348: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 22000.0\n",
            "1349: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 19300.0\n",
            "1350: Predicted_value= 302.177904454722\n",
            " Actual_Value: 14000.0\n",
            "1351: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 28000.0\n",
            "1352: Predicted_value= 89513.66087146939\n",
            " Actual_Value: 60000.0\n",
            "1353: Predicted_value= 76236.71715996828\n",
            " Actual_Value: 70000.0\n",
            "1354: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 38000.0\n",
            "1355: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 15000.0\n",
            "1356: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1357: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 30000.0\n",
            "1358: Predicted_value= 81881.55441341706\n",
            " Actual_Value: 75000.0\n",
            "1359: Predicted_value= 27211.786189588925\n",
            " Actual_Value: 32000.0\n",
            "1360: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 25000.0\n",
            "1361: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 15000.0\n",
            "1362: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "1363: Predicted_value= 11804.490324742248\n",
            " Actual_Value: 22000.0\n",
            "1364: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 15000.0\n",
            "1365: Predicted_value= 128741.3557481269\n",
            " Actual_Value: 60000.0\n",
            "1366: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "1367: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 22000.0\n",
            "1368: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "1369: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 17000.0\n",
            "1370: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 20000.0\n",
            "1371: Predicted_value= 10277.274429242541\n",
            " Actual_Value: 10000.0\n",
            "1372: Predicted_value= 134386.19300157568\n",
            " Actual_Value: 110000.00000000001\n",
            "1373: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 13000.0\n",
            "1374: Predicted_value= 31116.769634148077\n",
            " Actual_Value: 30000.0\n",
            "1375: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 16000.0\n",
            "1376: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10000.0\n",
            "1377: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "1378: Predicted_value= 31116.769634148077\n",
            " Actual_Value: 30000.0\n",
            "1379: Predicted_value= 72491.94539488503\n",
            " Actual_Value: 27000.0\n",
            "1380: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 13000.0\n",
            "1381: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 18000.0\n",
            "1382: Predicted_value= 272800.46737003565\n",
            " Actual_Value: 550000.0\n",
            "1383: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "1384: Predicted_value= 27654.710767958866\n",
            " Actual_Value: 18600.0\n",
            "1385: Predicted_value= 202527.89409870483\n",
            " Actual_Value: 300000.0\n",
            "1386: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 11500.0\n",
            "1387: Predicted_value= 290320.46663663787\n",
            " Actual_Value: 500000.0\n",
            "1388: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 35000.0\n",
            "1389: Predicted_value= -5928.146855249848\n",
            " Actual_Value: 10000.0\n",
            "1390: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 13500.0\n",
            "1391: Predicted_value= 80301.91228400334\n",
            " Actual_Value: 35000.0\n",
            "1392: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "1393: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 26000.0\n",
            "1394: Predicted_value= 60616.78338173166\n",
            " Actual_Value: 30000.0\n",
            "1395: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 20000.0\n",
            "1396: Predicted_value= 302.177904454722\n",
            " Actual_Value: 10000.0\n",
            "1397: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "1398: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 19000.0\n",
            "1399: Predicted_value= 6550.151415749366\n",
            " Actual_Value: 11000.0\n",
            "1400: Predicted_value= 52967.02817208927\n",
            " Actual_Value: 23000.0\n",
            "1401: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 22000.0\n",
            "1402: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 20000.0\n",
            "1403: Predicted_value= -9672.91862033309\n",
            " Actual_Value: 14000.0\n",
            "1404: Predicted_value= -2023.1634106907004\n",
            " Actual_Value: 30000.0\n",
            "1405: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 17000.0\n",
            "1406: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 40000.0\n",
            "1407: Predicted_value= 27814.922447434776\n",
            " Actual_Value: 18000.0\n",
            "1408: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 22000.0\n",
            "1409: Predicted_value= 72491.94539488503\n",
            " Actual_Value: 50000.0\n",
            "1410: Predicted_value= 18087.241318360844\n",
            " Actual_Value: 9000.0\n",
            "1411: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 18000.0\n",
            "1412: Predicted_value= 37612.158541156605\n",
            " Actual_Value: 11000.0\n",
            "1413: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 12000.0\n",
            "1414: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "1415: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 30000.0\n",
            "1416: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 16000.0\n",
            "1417: Predicted_value= 25632.144060175197\n",
            " Actual_Value: 22000.0\n",
            "1418: Predicted_value= 11856.916558656267\n",
            " Actual_Value: 10000.0\n",
            "1419: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 18000.0\n",
            "1420: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 10000.0\n",
            "1421: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 25000.0\n",
            "1422: Predicted_value= 37659.49306941739\n",
            " Actual_Value: 15000.0\n",
            "1423: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "1424: Predicted_value= 302.177904454722\n",
            " Actual_Value: 15000.0\n",
            "1425: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "1426: Predicted_value= 49062.04472753011\n",
            " Actual_Value: 25000.0\n",
            "1427: Predicted_value= 302.177904454722\n",
            " Actual_Value: 13000.0\n",
            "1428: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 11000.0\n",
            "1429: Predicted_value= 15761.900003215414\n",
            " Actual_Value: 10000.0\n",
            "1430: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 18000.0\n",
            "1431: Predicted_value= 27476.85033689288\n",
            " Actual_Value: 11000.0\n",
            "1432: Predicted_value= 123096.51849467811\n",
            " Actual_Value: 80000.0\n",
            "1433: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 10000.0\n",
            "1434: Predicted_value= 33281.899269817586\n",
            " Actual_Value: 16000.0\n",
            "1435: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 9000.0\n",
            "1436: Predicted_value= 87951.66749364573\n",
            " Actual_Value: 35000.0\n",
            "1437: Predicted_value= 6107.226837379429\n",
            " Actual_Value: 18000.0\n",
            "1438: Predicted_value= 197026.13979399804\n",
            " Actual_Value: 110000.00000000001\n",
            "1439: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 15500.0\n",
            "1440: Predicted_value= 5947.015157903523\n",
            " Actual_Value: 24000.0\n",
            "1441: Predicted_value= -3763.017219580339\n",
            " Actual_Value: 8000.0\n",
            "1442: Predicted_value= 66261.62063518046\n",
            " Actual_Value: 42000.0\n",
            "1443: Predicted_value= 25471.932380699283\n",
            " Actual_Value: 20000.0\n",
            "1444: Predicted_value= 20608.09181616232\n",
            " Actual_Value: 14000.0\n",
            "1445: Predicted_value= -7507.788984663581\n",
            " Actual_Value: 11000.0\n",
            "1446: Predicted_value= 302.177904454722\n",
            " Actual_Value: 18000.0\n",
            "1447: Predicted_value= 44996.84960349505\n",
            " Actual_Value: 45000.0\n",
            "1448: Predicted_value= 20004.95555831647\n",
            " Actual_Value: 20000.0\n",
            "1449: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 7000.0\n",
            "1450: Predicted_value= 302.177904454722\n",
            " Actual_Value: 11000.0\n",
            "1451: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 22000.0\n",
            "1452: Predicted_value= 4207.16134901387\n",
            " Actual_Value: 14000.0\n",
            "1453: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 40000.0\n",
            "1454: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 22000.0\n",
            "1455: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 18000.0\n",
            "1456: Predicted_value= 19666.883447774577\n",
            " Actual_Value: 10000.0\n",
            "1457: Predicted_value= 23732.07857180964\n",
            " Actual_Value: 35000.0\n",
            "1458: Predicted_value= 843833.6282674266\n",
            " Actual_Value: 1000000.0\n",
            "1459: Predicted_value= 9851.99860246267\n",
            " Actual_Value: 15000.0\n",
            "1460: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 26000.0\n",
            "1461: Predicted_value= 17501.753812105067\n",
            " Actual_Value: 15000.0\n",
            "1462: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 10000.0\n",
            "1463: Predicted_value= 37186.88271437674\n",
            " Actual_Value: 19000.0\n",
            "1464: Predicted_value= 52806.816492613354\n",
            " Actual_Value: 27000.0\n",
            "1465: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 17000.0\n",
            "1466: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 20000.0\n",
            "1467: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 20000.0\n",
            "1468: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 20000.0\n",
            "1469: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 14000.0\n",
            "1470: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11500.0\n",
            "1471: Predicted_value= 30938.909203082098\n",
            " Actual_Value: 25000.0\n",
            "1472: Predicted_value= 50624.03810535377\n",
            " Actual_Value: 50000.0\n",
            "1473: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 18000.0\n",
            "1474: Predicted_value= 12017.12823813218\n",
            " Actual_Value: 11500.0\n",
            "1475: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 16000.0\n",
            "1476: Predicted_value= 74071.58752429875\n",
            " Actual_Value: 50000.0\n",
            "1477: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "1478: Predicted_value= 302.177904454722\n",
            " Actual_Value: 17000.0\n",
            "1479: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 20000.0\n",
            "1480: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 18000.0\n",
            "1481: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 11000.0\n",
            "1482: Predicted_value= 6550.151415749366\n",
            " Actual_Value: 18000.0\n",
            "1483: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 13000.0\n",
            "1484: Predicted_value= 33121.68759034168\n",
            " Actual_Value: 13000.0\n",
            "1485: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 21000.0\n",
            "1486: Predicted_value= 23306.802745029774\n",
            " Actual_Value: 22000.0\n",
            "1487: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 12000.0\n",
            "1488: Predicted_value= 15709.473769301403\n",
            " Actual_Value: 22000.0\n",
            "1489: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 18000.0\n",
            "1490: Predicted_value= 9691.786922986761\n",
            " Actual_Value: 15000.0\n",
            "1491: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 32000.0\n",
            "1492: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 20000.0\n",
            "1493: Predicted_value= 26856.065327456963\n",
            " Actual_Value: 18000.0\n",
            "1494: Predicted_value= 5786.80347842761\n",
            " Actual_Value: 20000.0\n",
            "1495: Predicted_value= 33442.11094929351\n",
            " Actual_Value: 20000.0\n",
            "1496: Predicted_value= -3602.8055401044257\n",
            " Actual_Value: 11000.0\n",
            "1497: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 23000.0\n",
            "1498: Predicted_value= 174351.5623805778\n",
            " Actual_Value: 150000.0\n",
            "1499: Predicted_value= 29376.91582525844\n",
            " Actual_Value: 35000.0\n",
            "1500: Predicted_value= 17661.965491580977\n",
            " Actual_Value: 23000.0\n",
            "1501: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 22000.0\n",
            "1502: Predicted_value= 132806.55087216196\n",
            " Actual_Value: 250000.0\n",
            "1503: Predicted_value= 25311.72070122337\n",
            " Actual_Value: 18000.0\n",
            "1504: Predicted_value= 4046.949669537964\n",
            " Actual_Value: 10500.0\n",
            "1505: Predicted_value= 19827.09512725049\n",
            " Actual_Value: 16500.0\n",
            "1506: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "1507: Predicted_value= 13756.982047021826\n",
            " Actual_Value: 16000.0\n",
            "1508: Predicted_value= 21406.73725666422\n",
            " Actual_Value: 15000.0\n",
            "1509: Predicted_value= 8112.144793573032\n",
            " Actual_Value: 16000.0\n",
            "1510: Predicted_value= 206841.02463930994\n",
            " Actual_Value: 250000.0\n",
            "1511: Predicted_value= 15922.111682691335\n",
            " Actual_Value: 12000.0\n",
            "1512: Predicted_value= 21566.948936140132\n",
            " Actual_Value: 25000.0\n",
            "(1512,)\n",
            "(1512, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of Actual vs. Predicted values\n",
        "plt.scatter(y_test, y_pred, color='blue', label='Actual vs. Predicted')\n",
        "\n",
        "# Plotting the perfect correlation line\n",
        "plt.plot(y_test, y_test, color='red', label='Perfect Correlation')\n",
        "\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Multivariate Linear Regression')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xWmkclYhy_uZ",
        "outputId": "2ea31208-240c-47e6-b9bd-9d3997b5424a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2=r2_score(y_test,y_pred)\n",
        "print(\"Accuracy:\",r2*100,\"%\")"
      ],
      "metadata": {
        "id": "OqM7yT0vzICq",
        "outputId": "ea7e0fa6-b34d-4737-fd9a-89d1dc59158b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 82.00662684958023 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REcFbj-N5rx6",
        "outputId": "3ed8b3b3-1533-4260-a769-a32b4d4ca44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 815499057.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmYeQ8mu9UOW",
        "outputId": "98ee14c9-3d4c-47c8-f789-6c0ca992833c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: 0.82\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}